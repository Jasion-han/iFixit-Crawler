#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
from crawler import IFixitCrawler

def crawl_batch(search_term, debug=False):
    """æ‰¹é‡çˆ¬å–æœç´¢ç»“æœä¸­çš„æ‰€æœ‰äº§å“ä¿¡æ¯"""
    crawler = IFixitCrawler()
    crawler.debug = debug  # è®¾ç½®debugå±æ€§
    # é¦–å…ˆè·å–æœç´¢ç»“æœé¡µé¢
    search_url = f"https://www.ifixit.com/Search?query={search_term}"
    soup = crawler.get_soup(search_url)
    if not soup:
        print(f"æ— æ³•è·å–æœç´¢é¡µé¢: {search_url}")
        return None
    
    # è·å–æœç´¢ç»“æœä¸­çš„æ‰€æœ‰äº§å“é“¾æ¥
    product_links = []
    search_results = soup.find_all("div", class_="search-results")
    if search_results:
        for result in search_results:
            links = result.find_all("a")
            for link in links:
                href = link.get("href")
                if href and "/Device/" in href:
                    # æ„å»ºå®Œæ•´URL
                    if not href.startswith("http"):
                        href = "https://www.ifixit.com" + href
                    product_links.append(href)
    
    if not product_links:
        print(f"æ²¡æœ‰æ‰¾åˆ°äº§å“é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–ç±»å‹çš„é“¾æ¥")
        # å°è¯•å…¶ä»–å¯èƒ½çš„æœç´¢ç»“æœç»“æ„
        items = soup.select(".search-results-items a, .searchResults a")
        for item in items:
            href = item.get("href")
            if href and ("/Device/" in href or "/Guide/" in href or "/Teardown/" in href):
                # æ„å»ºå®Œæ•´URL
                if not href.startswith("http"):
                    href = "https://www.ifixit.com" + href
                product_links.append(href)
    
    if not product_links:
        print(f"æœªæ‰¾åˆ°ä»»ä½•äº§å“é“¾æ¥ï¼š{search_url}")
        return None
    
    # å»é‡
    product_links = list(set(product_links))
    print(f"ğŸ“‹ æ‰¾åˆ° {len(product_links)} ä¸ªäº§å“é“¾æ¥")

    # çˆ¬å–æ¯ä¸ªäº§å“çš„è¯¦ç»†ä¿¡æ¯
    results = []
    for i, link in enumerate(product_links, 1):
        print(f"[{i}/{len(product_links)}] æ­£åœ¨çˆ¬å–...")
        product_info = crawler.extract_product_info_from_url(link)
        if product_info:
            results.append(product_info)
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(1)

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    filename = f"results/batch_{search_term.replace(' ', '_')}.json"
    os.makedirs("results", exist_ok=True)
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆï¼Œå·²ä¿å­˜ {len(results)} ä¸ªäº§å“ä¿¡æ¯åˆ° {filename}")
    return results

def test_url(url, debug=False):
    """æµ‹è¯•ç›´æ¥çˆ¬å–ç‰¹å®šURL"""
    crawler = IFixitCrawler()
    crawler.debug = debug
    
    print(f"çˆ¬å–URL: {url}")
    product_info = crawler.extract_product_info_from_url(url)
    
    if product_info:
        print("\nçˆ¬å–ç»“æœ:")
        print(f"äº§å“åç§°: {product_info.get('product_name', 'N/A')}")
        print(f"äº§å“URL: {product_info.get('product_url', 'N/A')}")
        print(f"è¯´æ˜ä¹¦é“¾æ¥: {product_info.get('instruction_url', 'N/A')}")
        
        # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        # ä»URLä¸­æå–æœ€åä¸€ä¸ªéƒ¨åˆ†ä½œä¸ºæ–‡ä»¶å
        url_parts = product_info.get('product_url', '').split('/')
        file_name = url_parts[-1] if url_parts and url_parts[-1] else "unknown"
        filename = f"results/batch_{file_name}.json"
        os.makedirs("results", exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            # å°†å•ä¸ªäº§å“ä¿¡æ¯åŒ…è£…åœ¨æ•°ç»„ä¸­
            json.dump([product_info], f, ensure_ascii=False, indent=2)
        
        print(f"\nå·²ä¿å­˜äº§å“ä¿¡æ¯åˆ° {filename}")
        return product_info
    else:
        print("æœªèƒ½æå–äº§å“ä¿¡æ¯")
        return None

def test_multiple_urls(debug=False):
    """æµ‹è¯•å¤šä¸ªé¢„å®šä¹‰çš„URL"""
    # å®šä¹‰æµ‹è¯•URLåˆ—è¡¨ï¼ŒåŒ…å«ä¸åŒç±»å‹çš„äº§å“
    urls = [
        "https://www.ifixit.com/Device/Golf_Cart",  # æœ‰è§†é¢‘æŒ‡å—çš„äº§å“
        "https://www.ifixit.com/Device/iPad_3G",    # å¯èƒ½æœ‰PDFæ–‡æ¡£çš„äº§å“
        "https://www.ifixit.com/Device/LG_37LG10-UM"  # æœ‰é”šç‚¹é“¾æ¥çš„äº§å“
    ]
    
    crawler = IFixitCrawler()
    crawler.debug = debug
    
    results = []
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] æµ‹è¯•URL: {url}")
        product_info = crawler.extract_product_info_from_url(url)
        
        if product_info:
            results.append(product_info)
            print(f"äº§å“: {product_info.get('product_name', 'N/A')}")
            print(f"è¯´æ˜ä¹¦: {product_info.get('instruction_url', 'N/A')}")
        else:
            print(f"æœªèƒ½æå–äº§å“ä¿¡æ¯: {url}")
            
        # æ·»åŠ å»¶è¿Ÿ
        if i < len(urls):
            time.sleep(2)
    
    # å¦‚æœæœ‰ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªURLçš„æœ€åéƒ¨åˆ†ä½œä¸ºæ–‡ä»¶å
    if results:
        url_parts = results[0].get("product_url", "").split("/")
        file_name = url_parts[-1] if url_parts and url_parts[-1] else "multiple_urls"
        filename = f"results/batch_{file_name}.json"
    else:
        filename = "results/batch_multiple_urls.json"
    
    os.makedirs("results", exist_ok=True)
    with open(filename, "w", encoding="utf-8") as f:
        # resultså·²ç»æ˜¯æ•°ç»„ï¼Œç›´æ¥ä¿å­˜
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\næµ‹è¯•å®Œæˆï¼Œå·²ä¿å­˜ {len(results)} ä¸ªäº§å“ä¿¡æ¯åˆ° {filename}")
    return results

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–äº§å“å"""
    if not input_text:
        return None
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        return input_text
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text
        return input_text
    
    # å¦åˆ™è§†ä¸ºäº§å“åï¼Œè¿›è¡Œæœç´¢æˆ–æ„å»ºURL
    return input_text

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python batch_crawler.py <æœç´¢è¯/URL/äº§å“å> [debug]")
        print("ä¾‹å¦‚: python batch_crawler.py iPad")
        print("      python batch_crawler.py https://www.ifixit.com/Device/iPad_3G")
        print("      python batch_crawler.py iPad_3G")
        print("æˆ–è€…: python batch_crawler.py test [debug] - æµ‹è¯•å¤šä¸ªé¢„å®šä¹‰URL")
        sys.exit(1)
    
    input_text = sys.argv[1]
    debug = len(sys.argv) > 2 and sys.argv[2].lower() in ('debug', 'true', '1')
    
    # åˆ¤æ–­è¾“å…¥ç±»å‹
    if input_text.lower() == "test":
        test_multiple_urls(debug)
    elif input_text.startswith("http"):
        # ç›´æ¥ä½¿ç”¨å®Œæ•´URL
        test_url(input_text, debug)
    elif "/Device/" in input_text:
        # å¤„ç†éƒ¨åˆ†URL
        url = process_input(input_text)
        test_url(url, debug)
    else:
        # å°è¯•ç›´æ¥ä½œä¸ºäº§å“åæ„å»ºURL
        product_url = f"https://www.ifixit.com/Device/{input_text}"
        print(f"å°è¯•ç›´æ¥è®¿é—®äº§å“é¡µé¢: {product_url}")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶æµ‹è¯•URL
        crawler = IFixitCrawler()
        crawler.debug = debug
        
        # æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®
        soup = crawler.get_soup(product_url)
        if soup and soup.title and "Page Not Found" not in soup.title.text:
            print("æ‰¾åˆ°äº§å“é¡µé¢ï¼Œç›´æ¥çˆ¬å–...")
            test_url(product_url, debug)
        else:
            print(f"æœªæ‰¾åˆ°äº§å“é¡µé¢ï¼Œå°è¯•æœç´¢: {input_text}")
            crawl_batch(input_text, debug)
