import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import json
import time
import os
import random
import re

class IFixitCrawler:
    def __init__(self, base_url="https://www.ifixit.com"):
        self.base_url = base_url
        self.results = []
        self.visited_urls = set()
        self.debug = False  # é»˜è®¤å…³é—­è°ƒè¯•æ¨¡å¼

        # åˆå§‹åŒ–Sessionå’Œé‡è¯•ç­–ç•¥
        self.session = requests.Session()

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # è®¾ç½®HTTPå¤´éƒ¨
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0"
        })
        
    def get_soup(self, url):
        """è·å–é¡µé¢å†…å®¹å¹¶è§£æä¸ºBeautifulSoupå¯¹è±¡ï¼Œå¼ºåˆ¶ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬"""
        try:
            # é¢„å¤„ç†URLï¼Œç¡®ä¿ä½¿ç”¨è‹±æ–‡åŸŸå
            if "zh.ifixit.com" in url:
                url = url.replace("zh.ifixit.com", "www.ifixit.com")
                self.print_debug(f"ğŸ”„ å°†ä¸­æ–‡URLè½¬æ¢ä¸ºè‹±æ–‡URL: {url}")

            # ç¡®ä¿URLåŒ…å«è‹±æ–‡è¯­è¨€å‚æ•°
            if "?lang=en" not in url and "&lang=en" not in url:
                url += "?lang=en" if "?" not in url else "&lang=en"
                self.print_debug(f"ğŸŒ æ·»åŠ è‹±æ–‡è¯­è¨€å‚æ•°: {url}")

            # é¦–æ¬¡å°è¯•ï¼šç¦æ­¢é‡å®šå‘
            self.print_debug(f"ğŸ“¡ æ­£åœ¨è®¿é—®: {url}")
            response = self.session.get(url, allow_redirects=False)

            # å¦‚æœæ˜¯é‡å®šå‘å“åº”ï¼Œæ£€æŸ¥é‡å®šå‘ç›®æ ‡
            if response.status_code in [301, 302, 303, 307, 308]:
                redirect_url = response.headers.get('Location', '')
                self.print_debug(f"ğŸ”€ æ£€æµ‹åˆ°é‡å®šå‘: {response.status_code} -> {redirect_url}")

                if "zh.ifixit.com" in redirect_url:
                    # å¼ºåˆ¶è½¬æ¢é‡å®šå‘ç›®æ ‡ä¸ºè‹±æ–‡ç‰ˆæœ¬
                    english_redirect = redirect_url.replace("zh.ifixit.com", "www.ifixit.com")
                    if "?lang=en" not in english_redirect and "&lang=en" not in english_redirect:
                        english_redirect += "?lang=en" if "?" not in english_redirect else "&lang=en"

                    self.print_debug(f"ğŸ”„ å¼ºåˆ¶é‡å®šå‘åˆ°è‹±æ–‡ç‰ˆæœ¬: {english_redirect}")
                    response = self.session.get(english_redirect, allow_redirects=False)
                else:
                    # å¦‚æœé‡å®šå‘ç›®æ ‡å·²ç»æ˜¯è‹±æ–‡ç‰ˆæœ¬ï¼Œè·Ÿéšé‡å®šå‘
                    response = self.session.get(redirect_url, allow_redirects=True)

            # æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœè¿˜æ˜¯è¢«é‡å®šå‘åˆ°ä¸­æ–‡ç‰ˆæœ¬ï¼Œå¼ºåˆ¶é‡æ–°è¯·æ±‚
            final_url = getattr(response, 'url', url)
            if "zh.ifixit.com" in final_url:
                english_final = final_url.replace("zh.ifixit.com", "www.ifixit.com")
                if "?lang=en" not in english_final:
                    english_final += "?lang=en" if "?" not in english_final else "&lang=en"

                self.print_debug(f"âš ï¸  æœ€ç»ˆURLä»ä¸ºä¸­æ–‡ç‰ˆæœ¬ï¼Œå¼ºåˆ¶è®¿é—®: {english_final}")
                response = self.session.get(english_final, allow_redirects=False)

            response.raise_for_status()
            self.print_debug(f"âœ… æˆåŠŸè·å–é¡µé¢ï¼Œæœ€ç»ˆURL: {getattr(response, 'url', url)}")
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"è·å–é¡µé¢æ—¶å‘ç”Ÿé”™è¯¯: {url}, é”™è¯¯: {str(e)}")
            return None
    
    def print_debug(self, message):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        if self.debug:
            print(f"[DEBUG] {message}")
            
    def extract_breadcrumbs(self, soup):
        """æå–é¡µé¢ä¸­çš„é¢åŒ…å±‘å¯¼èˆª"""
        breadcrumbs = []
        try:
            # æŸ¥æ‰¾é¢åŒ…å±‘å¯¼èˆªå…ƒç´ 
            breadcrumb_elements = soup.select("nav.breadcrumb li, .breadcrumbs a, .breadcrumb a")
            for element in breadcrumb_elements:
                text = element.get_text().strip()
                if text:
                    breadcrumbs.append(text)
                    
            if not breadcrumbs:
                # ç¬¬äºŒç§æ–¹æ³•ï¼šæ£€æŸ¥é¡µé¢é¡¶éƒ¨çš„å¯¼èˆªè·¯å¾„
                nav_path = soup.select_one("div.container > div.row:first-child")
                if nav_path:
                    path_items = nav_path.get_text().strip().split(">")
                    breadcrumbs = [item.strip() for item in path_items if item.strip()]
                    
            if breadcrumbs:
                self.print_debug(f"é¢åŒ…å±‘å¯¼èˆª: {' > '.join(breadcrumbs)}")
        except Exception as e:
            print(f"æå–é¢åŒ…å±‘å¯¼èˆªæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
        return breadcrumbs
            
    def extract_categories(self, soup, url):
        """æå–é¡µé¢ä¸­çš„è®¾å¤‡ç±»åˆ«é“¾æ¥"""
        categories = []
        try:
            # æ‰“å°é¡µé¢æ ‡é¢˜ï¼Œå¸®åŠ©è°ƒè¯•
            page_title = soup.title.text if soup.title else "æ— æ ‡é¢˜"
            self.print_debug(f"é¡µé¢æ ‡é¢˜: {page_title}")
            
            # ä»é¡µé¢æå–é¢åŒ…å±‘å¯¼èˆª
            breadcrumbs = self.extract_breadcrumbs(soup)
            
            # æŸ¥æ‰¾"ä¸ªç±»åˆ«"æ ‡è®°
            category_sections = soup.find_all(string=re.compile(r"\d+\s*ä¸ªç±»åˆ«"))
            if category_sections:
                self.print_debug(f"æ‰¾åˆ°ç±»åˆ«æ ‡é¢˜: {[section for section in category_sections]}")
                
                for section in category_sections:
                    # æŸ¥æ‰¾åŒ…å«ç±»åˆ«çš„åŒºåŸŸ
                    parent = section.parent
                    if parent:
                        # æŸ¥æ‰¾ä¹‹åçš„divï¼Œé€šå¸¸åŒ…å«ç±»åˆ«åˆ—è¡¨
                        next_section = parent.find_next_sibling()
                        if next_section:
                            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                            links = next_section.find_all("a", href=True)
                            for link in links:
                                href = link.get("href")
                                text = link.text.strip()
                                if href and text and "/Device/" in href:
                                    full_url = self.base_url + href if href.startswith("/") else href
                                    categories.append({
                                        "name": text,
                                        "url": full_url
                                    })
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»åˆ«æ ‡é¢˜ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not categories:
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«ç±»åˆ«çš„åŒºåŸŸ
                category_containers = soup.select("div.categories, div.category-grid, ul.browse-devices")
                for container in category_containers:
                    links = container.find_all("a", href=True)
                    for link in links:
                        href = link.get("href")
                        text = link.text.strip()
                        if href and text and "/Device/" in href:
                            full_url = self.base_url + href if href.startswith("/") else href
                            categories.append({
                                "name": text,
                                "url": full_url
                            })
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è®¾å¤‡é“¾æ¥
            if not categories:
                # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                all_links = soup.find_all("a", href=True)
                for link in all_links:
                    href = link.get("href")
                    text = link.text.strip()
                    if href and text and "/Device/" in href and not any(x in href for x in ["/Edit/", "/History/", "?revision", "/Answers/"]):
                        full_url = self.base_url + href if href.startswith("/") else href
                        categories.append({
                            "name": text,
                            "url": full_url
                        })
            
            # å»é‡
            unique_categories = []
            urls = set()
            for category in categories:
                if category["url"] not in urls and not any(x in category["url"] for x in ["/Edit/", "/History/", "?revision", "/Answers/"]):
                    urls.add(category["url"])
                    unique_categories.append(category)
            
            categories = unique_categories
            self.print_debug(f"æ‰¾åˆ° {len(categories)} ä¸ªç±»åˆ«")
            for category in categories:
                self.print_debug(f"  - {category['name']}: {category['url']}")
            
        except Exception as e:
            print(f"æå–ç±»åˆ«æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        return categories
        
    def is_final_product_page(self, soup, url):
        """åˆ¤æ–­å½“å‰é¡µé¢æ˜¯å¦ä¸ºæœ€ç»ˆäº§å“é¡µé¢ï¼ˆæ²¡æœ‰å­ç±»åˆ«çš„é¡µé¢ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å­ç±»åˆ«
        sub_categories = self.extract_categories(soup, url)
        
        # è¿‡æ»¤æ‰"åˆ›å»ºæŒ‡å—"ç­‰éå®é™…ç±»åˆ«
        real_categories = [c for c in sub_categories if not any(x in c["name"] or x in c["url"] for x in ["åˆ›å»ºæŒ‡å—", "Guide/new"])]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰äº§å“æ ‡é¢˜
        product_title = soup.select_one("div.device-title h1, h1.device-title, h1.title, h1")
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if product_title:
            self.print_debug(f"æ‰¾åˆ°äº§å“æ ‡é¢˜: {product_title.text}")
        
        # å¦‚æœæœ‰äº§å“æ ‡é¢˜ä½†æ²¡æœ‰å®é™…å­ç±»åˆ«ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ€ç»ˆäº§å“é¡µé¢
        if product_title and len(real_categories) == 0:
            return True
            
        return False
        
    def extract_product_info(self, soup, url, breadcrumb):
        """æå–äº§å“åç§°å’Œè¯´æ˜ä¹¦ä¿¡æ¯"""
        product_info = {
            "product_name": "",
            "product_url": url,    # äº§å“URLæ”¾åœ¨ç¬¬äºŒä¸ªä½ç½®
            "instruction_url": ""  # è¯´æ˜ä¹¦URLæ”¾åœ¨ç¬¬ä¸‰ä¸ªä½ç½®
        }
        
        try:
            # æå–äº§å“åç§°ï¼ˆé¡µé¢æ ‡é¢˜ï¼‰
            title_selectors = [
                "div.device-title h1", 
                "h1.device-title", 
                "h1.title",
                "h1"
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    # æ¸…ç†äº§å“åç§°ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å¹¶å¤„ç†å¼•å·
                    product_name = title_elem.text.strip()
                    # æ¸…ç†äº§å“åç§°ï¼Œä¿æŒè‹±æ–‡æ ¼å¼
                    product_name = product_name.replace('\\"', '"').replace('\\"', '"')
                    product_name = product_name.replace('"', '"').replace('"', '"')
                    product_name = re.sub(r'\s+', ' ', product_name)  # å¤„ç†å¤šä½™ç©ºæ ¼
                    product_info["product_name"] = product_name
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äº§å“åç§°ï¼Œå°è¯•ä»URLä¸­æå–
            if not product_info["product_name"]:
                url_path = url.split("/")[-1]
                product_name = url_path.replace("_", " ").replace("-", " ")
                product_name = product_name.replace('%22', '"').replace('%E2%80%9D', '"')  # å°†URLç¼–ç çš„å¼•å·æ›¿æ¢ä¸ºè‹±æ–‡å¼•å·
                product_info["product_name"] = product_name
            
            # æŸ¥æ‰¾"æ–‡æ¡£"æ ç›®ä¸‹çš„é“¾æ¥
            pdf_link = None
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾"æ–‡æ¡£"æ ‡é¢˜
            doc_headings = soup.find_all(string=lambda text: text and (("æ–‡æ¡£" in text or "Documents" in text)))
            for heading in doc_headings:
                self.print_debug(f"æ‰¾åˆ°æ–‡æ¡£æ ‡é¢˜: {heading}")
                # å‘ä¸Šæ‰¾åˆ°åŒ…å«æ–‡æ¡£çš„åŒºåŸŸ
                section = None
                parent = heading.parent
                for _ in range(5):  # å‘ä¸ŠæŸ¥æ‰¾æœ€å¤š5å±‚
                    if parent and parent.name in ['div', 'section']:
                        section = parent
                        break
                    parent = parent.parent if parent else None
                
                if section:
                    # åœ¨åŒºåŸŸå†…æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    links = section.find_all('a', href=True)
                    best_link = None
                    best_priority = 0
                    
                    for link in links:
                        href = link.get('href')
                        text = link.text.strip()
                        if href and text:
                            # å¤„ç†é”šç‚¹é“¾æ¥ä»¥ä¾¿äºæ˜¾ç¤º
                            link_url = href
                            if link_url.startswith('#'):
                                link_url = url + link_url
                            self.print_debug(f"æ–‡æ¡£åŒºåŸŸå†…é“¾æ¥: {text} -> {link_url}")
                            
                            # ç»™ä¸åŒç±»å‹çš„æ–‡æ¡£é“¾æ¥åˆ†é…ä¼˜å…ˆçº§
                            text_lower = text.lower()
                            priority = 0
                            
                            # æœåŠ¡æ–‡æ¡£ã€ä½¿ç”¨æ‰‹å†Œç­‰æœ‰æœ€é«˜ä¼˜å…ˆçº§
                            if any(kw in text_lower for kw in ['service document', 'service manual', 'æœåŠ¡æ–‡æ¡£', 'æœåŠ¡æ‰‹å†Œ', 'ä½¿ç”¨æ‰‹å†Œ', 'è¯´æ˜ä¹¦']):
                                priority = 3
                            # ä¸€èˆ¬æ–‡æ¡£å…¶æ¬¡
                            elif any(kw in text_lower for kw in ['document', 'manual', 'æ–‡æ¡£', 'æ‰‹å†Œ']):
                                priority = 2
                            # æŠ€æœ¯è§„æ ¼æœ€å
                            elif any(kw in text_lower for kw in ['spec', 'technical', 'è§„æ ¼', 'æŠ€æœ¯å‚æ•°']):
                                priority = 1
                            
                            if priority > best_priority:
                                best_link = href
                                best_priority = priority
                    
                    # ä½¿ç”¨æœ€ä½³é“¾æ¥
                    if best_link:
                        pdf_link = best_link
                        # å¤„ç†é”šç‚¹é“¾æ¥
                        if pdf_link.startswith('#'):
                            pdf_link = url + pdf_link
                        product_info["instruction_url"] = pdf_link
                        self.print_debug(f"æ‰¾åˆ°æ–‡æ¡£é“¾æ¥: {product_info['instruction_url']} (ä¼˜å…ˆçº§: {best_priority})")
                        
                        # å¦‚æœæ˜¯é”šç‚¹é“¾æ¥ï¼Œå°è¯•æ‰¾åˆ°å®é™…çš„PDFé“¾æ¥
                        if best_link.startswith('#'):
                            # é¦–å…ˆæ‰¾åˆ°é”šç‚¹æ‰€åœ¨çš„åŒºåŸŸ
                            anchor_id = best_link[1:]  # å»æ‰#å·
                            anchor_section = soup.find(id=anchor_id)
                            
                            if anchor_section:
                                # æŸ¥æ‰¾è¯¥åŒºåŸŸä¸‹çš„æ‰€æœ‰é“¾æ¥
                                section_parent = anchor_section.parent
                                if section_parent:
                                    # æŸ¥æ‰¾è¯¥åŒºåŸŸä¸‹çš„å®é™…PDFé“¾æ¥
                                    pdf_links = []
                                    # å‘ä¸‹æŸ¥æ‰¾æœ€å¤š5ä¸ªå…„å¼Ÿå…ƒç´ 
                                    next_elem = section_parent.next_sibling
                                    for _ in range(5):
                                        if not next_elem:
                                            break
                                        # æŸ¥æ‰¾å…ƒç´ ä¸­çš„é“¾æ¥
                                        if hasattr(next_elem, 'find_all'):
                                            for link in next_elem.find_all('a', href=True):
                                                href = link.get('href')
                                                if href and (href.endswith('.pdf') or '/Document/' in href):
                                                    pdf_links.append(href)
                                        next_elem = next_elem.next_sibling
                                    
                                    # å¦‚æœæ‰¾åˆ°PDFé“¾æ¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
                                    if pdf_links:
                                        pdf_link = pdf_links[0]
                                        if pdf_link.startswith('/'):
                                            pdf_link = self.base_url + pdf_link
                                        product_info["instruction_url"] = pdf_link
                                        self.print_debug(f"æ‰¾åˆ°å®é™…PDFæ–‡æ¡£é“¾æ¥: {product_info['instruction_url']}")
                        break
            
            # æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾DocumentsåŒºå—
            if not pdf_link:
                # å¤„ç†Reactç»„ä»¶ä¸­çš„æ–‡æ¡£
                doc_sections = soup.find_all('div', class_=lambda c: c and 'component-DocumentsSection' in c)
                for section in doc_sections:
                    self.print_debug(f"æ‰¾åˆ°Reactæ–‡æ¡£ç»„ä»¶: {section}")
                    # ä»data-propså±æ€§ä¸­æå–JSONæ•°æ®
                    props_data = section.get('data-props')
                    if props_data:
                        try:
                            # ä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é—®é¢˜
                            props_data = props_data.replace('&quot;', '"')
                            import json
                            data = json.loads(props_data)
                            if 'documents' in data and len(data['documents']) > 0:
                                for doc in data['documents']:
                                    # ä¼˜å…ˆä½¿ç”¨ç½‘é¡µURLè€Œä¸æ˜¯ä¸‹è½½é“¾æ¥
                                    if 'url' in doc:
                                        pdf_link = doc['url']
                                        # å¦‚æœæ˜¯ç›¸å¯¹URLï¼Œè½¬ä¸ºç»å¯¹URL
                                        full_url = self.base_url + pdf_link if pdf_link.startswith("/") else pdf_link
                                        # ç§»é™¤URLä¸­çš„referrerå‚æ•°ï¼Œè·å–æ›´å¹²å‡€çš„é“¾æ¥
                                        if '?' in full_url:
                                            full_url = full_url.split('?')[0]
                                        product_info["instruction_url"] = full_url
                                        self.print_debug(f"ä»Reactç»„ä»¶ä¸­æ‰¾åˆ°æ–‡æ¡£é¡µé¢é“¾æ¥: {product_info['instruction_url']}")
                                        break
                                    elif 'downloadUrl' in doc and not pdf_link:
                                        pdf_link = doc['downloadUrl']
                                        product_info["instruction_url"] = pdf_link
                                        self.print_debug(f"ä»Reactç»„ä»¶ä¸­æ‰¾åˆ°æ–‡æ¡£ä¸‹è½½é“¾æ¥: {pdf_link}")
                                        break
                        except Exception as e:
                            self.print_debug(f"è§£æReactç»„ä»¶æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                
                # æŸ¥æ‰¾å¸¸è§„æ–‡æ¡£åŒºå—
                if not pdf_link:
                    doc_sections = soup.find_all(['section', 'div'], string=lambda text: text and "Documents" in text)
                    if not doc_sections:
                        doc_sections = soup.find_all(['h2', 'h3', 'h4'], string=lambda text: text and "Documents" in text)
                    
                    for section in doc_sections:
                        self.print_debug(f"æ‰¾åˆ°DocumentsåŒºå—: {section}")
                        # æ‰¾åˆ°æ–‡æ¡£åŒºå—åï¼ŒæŸ¥æ‰¾å…¶åçš„æ‰€æœ‰é“¾æ¥
                        parent_section = section.parent
                        if parent_section:
                            # æŸ¥æ‰¾æ–‡æ¡£åŒºå—ä¸­çš„æ‰€æœ‰é“¾æ¥
                            links = parent_section.find_all('a', href=True)
                            if not links:
                                # å¦‚æœåœ¨çˆ¶å…ƒç´ ä¸­æ²¡æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾å…„å¼Ÿå…ƒç´ 
                                next_elem = section.find_next_sibling()
                                if next_elem:
                                    links = next_elem.find_all('a', href=True)
                            
                            for link in links:
                                href = link.get('href')
                                text = link.text.strip()
                                self.print_debug(f"DocumentsåŒºå—å†…é“¾æ¥: {text} -> {href}")
                                if href and text and not text.lower() in ['create a guide', 'repair guides']:
                                    pdf_link = href
                                    product_info["instruction_url"] = self.base_url + pdf_link if pdf_link.startswith("/") else pdf_link
                                    self.print_debug(f"æ‰¾åˆ°DocumentsåŒºå—ä¸­çš„é“¾æ¥: {product_info['instruction_url']}")
                                    break
                        
                        if pdf_link:
                            break
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾å«æœ‰"PDF"å›¾æ ‡çš„é“¾æ¥
            if not pdf_link:
                pdf_icons = soup.select('img[src*="pdf"], .pdf-icon, .icon-pdf')
                for icon in pdf_icons:
                    self.print_debug(f"æ‰¾åˆ°PDFå›¾æ ‡: {icon}")
                    parent_link = icon.find_parent('a')
                    if parent_link and parent_link.get('href'):
                        pdf_link = parent_link.get('href')
                        product_info["instruction_url"] = self.base_url + pdf_link if pdf_link.startswith("/") else pdf_link
                        self.print_debug(f"æ‰¾åˆ°PDFé“¾æ¥: {product_info['instruction_url']}")
                        break
            
            # æ–¹æ³•4ï¼šæ£€æŸ¥é¡µé¢å†…æ‰€æœ‰å¯èƒ½çš„æ–‡æ¡£é“¾æ¥
            if not pdf_link:
                internal_links = []  # å­˜å‚¨ifixitå†…éƒ¨ç½‘é¡µé“¾æ¥
                external_web_links = []  # å­˜å‚¨å¤–éƒ¨ç½‘ç«™çš„ç½‘é¡µé“¾æ¥
                pdf_links = []  # å­˜å‚¨PDFä¸‹è½½é“¾æ¥
                
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link.get('href')
                    text = link.text.strip()
                    if not href or not text:
                        continue
                        
                    # æ£€æŸ¥é“¾æ¥URLå’Œæ–‡æœ¬æ˜¯å¦åŒ…å«æ–‡æ¡£ç›¸å…³å…³é”®è¯
                    href_lower = href.lower()
                    text_lower = text.lower()
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯æ–‡æ¡£é“¾æ¥
                    is_doc_link = False
                    
                    # æ£€æŸ¥é“¾æ¥æ˜¯å¦æ˜¯ifixitå†…éƒ¨é“¾æ¥
                    is_internal = href.startswith('/') or self.base_url in href
                    
                    # æ’é™¤ä¸ç›¸å…³çš„é“¾æ¥
                    if 'wikipedia.org' in href_lower:
                        continue  # æ’é™¤ç»´åŸºç™¾ç§‘é“¾æ¥
                    
                    # æ’é™¤é€šç”¨é“¾æ¥å’Œåˆ›å»ºæŒ‡å—é“¾æ¥
                    if href == '/Guide' or href == '/Answers' or href == '/Teardown' or href == '/News' or 'Guide/new' in href:
                        continue  # æ’é™¤ç½‘ç«™é€šç”¨å¯¼èˆªé“¾æ¥å’Œåˆ›å»ºæŒ‡å—é“¾æ¥
                    
                    # æ£€æŸ¥é“¾æ¥æ˜¯å¦æ˜¯äº§å“ç›¸å…³çš„
                    product_name_lower = product_info["product_name"].lower()
                    # å°è¯•æå–äº§å“å‹å·
                    product_model = re.search(r'(\w+[-\s]?\w+\d+\w*[-\w]*)', product_name_lower)
                    if product_model:
                        product_model = product_model.group(1).replace(' ', '').lower()
                        # æ£€æŸ¥é“¾æ¥æˆ–æ–‡æœ¬æ˜¯å¦åŒ…å«äº§å“å‹å·
                        contains_model = product_model in href_lower or product_model in text_lower
                        
                        # å¤–éƒ¨é“¾æ¥å¿…é¡»ä¸äº§å“å‹å·ç›¸å…³
                        if not is_internal and not contains_model:
                            continue
                    
                    # URLä¸­åŒ…å«æ–‡æ¡£ç›¸å…³å…³é”®è¯
                    if any(kw in href_lower for kw in ['manual', 'document', 'pdf', 'guide', 'instruction', 'service', 'æ‰‹å†Œ', 'æ–‡æ¡£', 'è¯´æ˜ä¹¦']):
                        # å¢åŠ ç›¸å…³æ€§åˆ¤æ–­
                        relevance_score = 0
                        
                        # åŸºäºé“¾æ¥ç±»å‹çš„ç›¸å…³æ€§
                        if href_lower.endswith('.pdf'):
                            relevance_score += 5  # PDFé“¾æ¥é«˜ç›¸å…³æ€§
                        if '/document/' in href_lower:
                            relevance_score += 4  # æ–‡æ¡£è·¯å¾„é«˜ç›¸å…³æ€§
                        if 'service' in href_lower or 'manual' in href_lower:
                            relevance_score += 3  # åŒ…å«æœåŠ¡æ‰‹å†Œå…³é”®è¯
                            
                        # åŸºäºæ–‡æœ¬çš„ç›¸å…³æ€§
                        if product_model and product_model in text_lower:
                            relevance_score += 5  # æ–‡æœ¬åŒ…å«äº§å“å‹å·
                        if product_model and product_model in href_lower:
                            relevance_score += 4  # URLåŒ…å«äº§å“å‹å·
                            
                        # åªæœ‰ç›¸å…³æ€§è¶³å¤Ÿé«˜çš„é“¾æ¥æ‰è¢«è€ƒè™‘
                        if is_internal and relevance_score >= 2:
                            internal_links.append((href, text, relevance_score))
                            is_doc_link = True
                        elif relevance_score >= 4:  # å¤–éƒ¨é“¾æ¥éœ€è¦æ›´é«˜çš„ç›¸å…³æ€§
                            if href_lower.endswith('.pdf'):
                                pdf_links.append((href, text, relevance_score))
                            else:
                                external_web_links.append((href, text, relevance_score))
                            is_doc_link = True
                    
                    # é“¾æ¥æ–‡æœ¬åŒ…å«æ–‡æ¡£ç›¸å…³å…³é”®è¯
                    elif any(kw in text_lower for kw in ['manual', 'service manual', 'guide', 'technician', 'æ‰‹å†Œ', 'ç»´ä¿®æ‰‹å†Œ']):
                        # å¢åŠ ç›¸å…³æ€§åˆ¤æ–­
                        relevance_score = 0
                        
                        # åŸºäºæ–‡æœ¬çš„ç›¸å…³æ€§
                        if 'service manual' in text_lower or 'ç»´ä¿®æ‰‹å†Œ' in text_lower:
                            relevance_score += 4  # æœåŠ¡æ‰‹å†Œé«˜ç›¸å…³æ€§
                        elif 'manual' in text_lower or 'æ‰‹å†Œ' in text_lower:
                            relevance_score += 2  # æ‰‹å†Œä¸€èˆ¬ç›¸å…³æ€§
                            
                        if product_model and product_model in text_lower:
                            relevance_score += 5  # æ–‡æœ¬åŒ…å«äº§å“å‹å·
                        if product_model and product_model in href_lower:
                            relevance_score += 4  # URLåŒ…å«äº§å“å‹å·
                            
                        # åªæœ‰ç›¸å…³æ€§è¶³å¤Ÿé«˜çš„é“¾æ¥æ‰è¢«è€ƒè™‘
                        if is_internal and relevance_score >= 3:
                            internal_links.append((href, text, relevance_score))
                            is_doc_link = True
                        elif relevance_score >= 5:  # å¤–éƒ¨é“¾æ¥éœ€è¦æ›´é«˜çš„ç›¸å…³æ€§
                            external_web_links.append((href, text, relevance_score))
                            is_doc_link = True
                
                # ç›¸å…³æ€§åˆ†æ•°ä¸å¤Ÿæ—¶ï¼Œè®¾ç½®ä¸ºç©º
                if (internal_links and internal_links[0][2] < 2) or \
                   (external_web_links and external_web_links[0][2] < 4) or \
                   (pdf_links and pdf_links[0][2] < 3):
                    self.print_debug("æ‰¾åˆ°çš„é“¾æ¥ç›¸å…³æ€§ä¸å¤Ÿï¼Œä¸è®¾ç½®æ–‡æ¡£é“¾æ¥")
                    product_info["instruction_url"] = ""
                    
                    # å°è¯•æŸ¥æ‰¾è§†é¢‘é“¾æ¥
                    videos = self.extract_youtube_videos(soup, url)
                    if videos:
                        video = videos[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘
                        product_info["instruction_url"] = video["url"]
                        self.print_debug(f"ä½¿ç”¨è§†é¢‘é“¾æ¥ä½œä¸ºè¯´æ˜ä¹¦: {video['title']} -> {video['url']}")
                    
                    return product_info
                
                # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©é“¾æ¥
                if internal_links:
                    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
                    internal_links.sort(key=lambda x: x[2], reverse=True)
                    # ä¼˜å…ˆä½¿ç”¨ifixitå†…éƒ¨é“¾æ¥
                    href, text, relevance = internal_links[0]
                    self.print_debug(f"é€‰æ‹©å†…éƒ¨é“¾æ¥: {text} -> {href} (ç›¸å…³æ€§åˆ†æ•°: {relevance})")
                    pdf_link = href
                    # å¤„ç†é”šç‚¹é“¾æ¥ï¼ˆä»¥#å¼€å¤´çš„é“¾æ¥ï¼‰
                    if pdf_link.startswith('#'):
                        pdf_link = url + pdf_link  # å°†é”šç‚¹ä¸å½“å‰é¡µé¢URLåˆå¹¶
                    product_info["instruction_url"] = self.base_url + pdf_link if pdf_link.startswith("/") else pdf_link
                    self.print_debug(f"æ‰¾åˆ°ifixitå†…éƒ¨æ–‡æ¡£é“¾æ¥: {product_info['instruction_url']}")
                elif external_web_links:
                    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
                    external_web_links.sort(key=lambda x: x[2], reverse=True)
                    # å…¶æ¬¡ä½¿ç”¨å¤–éƒ¨ç½‘ç«™çš„ç½‘é¡µé“¾æ¥
                    href, text, relevance = external_web_links[0]
                    self.print_debug(f"é€‰æ‹©å¤–éƒ¨é“¾æ¥: {text} -> {href} (ç›¸å…³æ€§åˆ†æ•°: {relevance})")
                    pdf_link = href
                    # å¤„ç†é”šç‚¹é“¾æ¥ï¼ˆä»¥#å¼€å¤´çš„é“¾æ¥ï¼‰
                    if pdf_link.startswith('#'):
                        pdf_link = url + pdf_link  # å°†é”šç‚¹ä¸å½“å‰é¡µé¢URLåˆå¹¶
                    product_info["instruction_url"] = self.base_url + pdf_link if pdf_link.startswith("/") else pdf_link
                    self.print_debug(f"æ‰¾åˆ°å¤–éƒ¨ç½‘ç«™æ–‡æ¡£é“¾æ¥: {product_info['instruction_url']}")
                elif pdf_links:
                    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
                    pdf_links.sort(key=lambda x: x[2], reverse=True)
                    # æœ€åæ‰ä½¿ç”¨PDFä¸‹è½½é“¾æ¥
                    href, text, relevance = pdf_links[0]
                    self.print_debug(f"é€‰æ‹©PDFé“¾æ¥: {text} -> {href} (ç›¸å…³æ€§åˆ†æ•°: {relevance})")
                    pdf_link = href
                    # å¤„ç†é”šç‚¹é“¾æ¥ï¼ˆä»¥#å¼€å¤´çš„é“¾æ¥ï¼‰
                    if pdf_link.startswith('#'):
                        pdf_link = url + pdf_link  # å°†é”šç‚¹ä¸å½“å‰é¡µé¢URLåˆå¹¶
                    product_info["instruction_url"] = self.base_url + pdf_link if pdf_link.startswith("/") else pdf_link
                    self.print_debug(f"æ‰¾åˆ°PDFæ–‡æ¡£ä¸‹è½½é“¾æ¥: {product_info['instruction_url']}")
            
            # å¦‚æœæœ€ç»ˆé“¾æ¥æ˜¯é”šç‚¹ï¼Œåˆå¹¶ä¸ºå®Œæ•´URL
            if product_info.get("instruction_url", "").startswith('#'):
                product_info["instruction_url"] = url + product_info["instruction_url"]
                self.print_debug(f"åˆå¹¶é”šç‚¹é“¾æ¥ä¸ºå®Œæ•´URL: {product_info['instruction_url']}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾è§†é¢‘é“¾æ¥
            if not product_info.get("instruction_url"):
                videos = self.extract_youtube_videos(soup, url)
                if videos:
                    video = videos[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘
                    product_info["instruction_url"] = video["url"]
                    self.print_debug(f"ä½¿ç”¨è§†é¢‘é“¾æ¥ä½œä¸ºè¯´æ˜ä¹¦: {video['title']} -> {video['url']}")
        
        except Exception as e:
            print(f"æå–äº§å“ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        return product_info
        
    def crawl_recursive(self, url, breadcrumb=None, parent_url=None):
        """é€’å½’çˆ¬å–ç±»åˆ«å’Œäº§å“"""
        # é¿å…é‡å¤è®¿é—®
        if url in self.visited_urls:
            return
            
        # è·³è¿‡"åˆ›å»ºæŒ‡å—"é¡µé¢
        if "åˆ›å»ºæŒ‡å—" in url or "Guide/new" in url:
            return
            
        self.visited_urls.add(url)
        
        if breadcrumb is None:
            breadcrumb = []
        
        # é˜²æ­¢è¿‡å¿«è¯·æ±‚ï¼Œæ·»åŠ å»¶è¿Ÿ
        time.sleep(random.uniform(2, 4))
        
        print(f"çˆ¬å–: {url}")
        soup = self.get_soup(url)
        if not soup:
            return
            
        # æå–é¢åŒ…å±‘å¯¼èˆª
        page_breadcrumbs = self.extract_breadcrumbs(soup)
        if page_breadcrumbs and len(page_breadcrumbs) > len(breadcrumb):
            breadcrumb = page_breadcrumbs
        
        # æå–å­ç±»åˆ«
        categories = self.extract_categories(soup, url)
        
        # è¿‡æ»¤æ‰"åˆ›å»ºæŒ‡å—"ç­‰éå®é™…ç±»åˆ«
        real_categories = [c for c in categories if not any(x in c["name"] or x in c["url"] for x in ["åˆ›å»ºæŒ‡å—", "Guide/new"])]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ç»ˆäº§å“é¡µé¢
        is_final_page = self.is_final_product_page(soup, url)
        
        # å¦‚æœæ˜¯æœ€ç»ˆäº§å“é¡µé¢ï¼ˆæ²¡æœ‰å­ç±»åˆ«ï¼‰ï¼Œåˆ™æå–äº§å“ä¿¡æ¯
        if is_final_page:
            product_info = self.extract_product_info(soup, url, breadcrumb)
            
            # åªæœ‰å½“äº§å“åç§°ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­ï¼Œå¹¶ç¡®ä¿æ²¡æœ‰é‡å¤
            if product_info["product_name"] and not any(p["product_url"] == product_info["product_url"] for p in self.results):
                self.results.append(product_info)
                print(f"å·²æ‰¾åˆ°äº§å“: {product_info['product_name']}")
                print(f"äº§å“URL: {product_info['product_url']}")
                print(f"è¯´æ˜ä¹¦é“¾æ¥: {product_info['instruction_url'] or 'æ— '}")
                print(f"çŠ¶æ€: æœ€ç»ˆäº§å“é¡µé¢ (æ— å­ç±»åˆ«)")
        else:
            # å¦‚æœæœ‰å­ç±»åˆ«ï¼Œåˆ™æ‰“å°ä¿¡æ¯
            if real_categories:
                print(f"ç±»åˆ«é¡µé¢: {url}")
                print(f"æ‰¾åˆ° {len(real_categories)} ä¸ªå­ç±»åˆ«")
        
        # å¦‚æœæœ‰å­ç±»åˆ«ï¼Œåˆ™ç»§ç»­é€’å½’çˆ¬å–
        if categories:
            for category in categories:
                # è·³è¿‡å·²è®¿é—®çš„é“¾æ¥
                if category["url"] in self.visited_urls:
                    continue
                    
                # è·³è¿‡ç¼–è¾‘ã€å†å²ã€åˆ›å»ºæŒ‡å—ç­‰éè®¾å¤‡é¡µé¢
                if any(x in category["url"] or x in category["name"] for x in ["/Edit/", "/History/", "?revision", "/Answers/", "åˆ›å»ºæŒ‡å—", "Guide/new"]):
                    continue
                
                # æ›´æ–°é¢åŒ…å±‘ - æ³¨æ„ï¼šè¿™é‡Œä¸æ˜¯è¿½åŠ è·¯å¾„ï¼Œè€Œæ˜¯æ–°çš„ç‹¬ç«‹è·¯å¾„
                new_breadcrumb = breadcrumb.copy()
                if category["name"] not in new_breadcrumb:
                    new_breadcrumb.append(category["name"])
                
                print(f"çˆ¬å–ç±»åˆ«: {' > '.join(new_breadcrumb)}")
                self.crawl_recursive(category["url"], new_breadcrumb, url)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å­ç±»åˆ«ä½†ä¹Ÿä¸ç¬¦åˆæœ€ç»ˆäº§å“é¡µé¢çš„å®šä¹‰
            product_info = self.extract_product_info(soup, url, breadcrumb)
            if product_info["product_name"]:
                self.results.append(product_info)
                print(f"å·²æ‰¾åˆ°äº§å“: {product_info['product_name']}")
                print(f"äº§å“URL: {product_info['product_url']}")
                print(f"è¯´æ˜ä¹¦é“¾æ¥: {product_info['instruction_url'] or 'æ— '}")
                print(f"çŠ¶æ€: æœ€ç»ˆäº§å“é¡µé¢ (æ— å­ç±»åˆ«)")
            
    def start_crawl(self, start_url=None):
        """å¼€å§‹çˆ¬å–"""
        if not start_url:
            # ä»è®¾å¤‡é¡µé¢å¼€å§‹
            start_url = self.base_url + "/Device"
        
        print(f"å¼€å§‹çˆ¬å–: {start_url}")
        # ä»èµ·å§‹é¡µé¢å¼€å§‹çˆ¬å–
        soup = self.get_soup(start_url)
        if not soup:
            print("æ— æ³•è·å–èµ·å§‹é¡µé¢")
            return
            
        # ä»è®¾å¤‡é¦–é¡µæå–æ‰€æœ‰ç±»åˆ«
        categories = self.extract_categories(soup, start_url)
        if not categories:
            print("æœªåœ¨é¦–é¡µæ‰¾åˆ°è®¾å¤‡ç±»åˆ«ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„è®¾å¤‡ç±»åˆ«")
            categories = [
                {"name": "Apple iPhone", "url": "https://www.ifixit.com/Device/iPhone"},
                {"name": "iPad", "url": "https://www.ifixit.com/Device/iPad"},
                {"name": "å¹³æ¿ç”µè„‘", "url": "https://www.ifixit.com/Device/Tablet"},
                {"name": "æ‰‹æœº", "url": "https://www.ifixit.com/Device/Phone"},
                {"name": "ç”µè„‘ç¡¬ä»¶", "url": "https://www.ifixit.com/Device/Computer_Hardware"}
            ]
            
        for category in categories:
            # è·³è¿‡å·²è®¿é—®çš„é“¾æ¥
            if category["url"] in self.visited_urls:
                continue
                
            # è·³è¿‡ç¼–è¾‘ã€å†å²ç­‰éè®¾å¤‡é¡µé¢
            if any(x in category["url"] for x in ["/Edit/", "/History/", "?revision", "/Answers/"]):
                continue
                
            print(f"çˆ¬å–ä¸»ç±»åˆ«: {category['name']}")
            self.crawl_recursive(category["url"], [category["name"]])
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
    def save_results(self):
        """ä¿å­˜çˆ¬å–ç»“æœåˆ°JSONæ–‡ä»¶"""
        if not self.results:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•äº§å“ä¿¡æ¯")
            return
            
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)
        
        # å¤„ç†äº§å“åç§°ä¸­çš„å¼•å·é—®é¢˜ï¼Œä¿æŒè‹±æ–‡æ ¼å¼
        for item in self.results:
            if "product_name" in item:
                # æ ‡å‡†åŒ–å¼•å·ä¸ºè‹±æ–‡æ ¼å¼
                item["product_name"] = item["product_name"].replace('\\"', '"').replace('\\"', '"')
                item["product_name"] = item["product_name"].replace('"', '"').replace('"', '"')
            
            # å…¼å®¹æ—§æ•°æ®ï¼Œå°†manual_urlè½¬ä¸ºinstruction_url
            if "manual_url" in item and "instruction_url" not in item:
                item["instruction_url"] = item["manual_url"]
                del item["manual_url"]
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open("results/ifixit_products.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"å·²ä¿å­˜ {len(self.results)} æ¡äº§å“ä¿¡æ¯åˆ° results/ifixit_products.json")
    
    def save_result(self, product_info, filename=None):
        """ä¿å­˜å•ä¸ªäº§å“ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)
        
        if not filename:
            # ä»URLä¸­æå–æ–‡ä»¶å
            if "product_url" in product_info:
                url_parts = product_info["product_url"].split('/')
                filename = f"results/{url_parts[-1]}.json"
            else:
                filename = "results/product_info.json"
        
        # å¤„ç†äº§å“åç§°ä¸­çš„å¼•å·é—®é¢˜ï¼Œä¿æŒè‹±æ–‡æ ¼å¼
        if "product_name" in product_info:
            # æ ‡å‡†åŒ–å¼•å·ä¸ºè‹±æ–‡æ ¼å¼
            product_info["product_name"] = product_info["product_name"].replace('\\"', '"').replace('\\"', '"')
            product_info["product_name"] = product_info["product_name"].replace('"', '"').replace('"', '"')
            
        # å…¼å®¹æ—§æ•°æ®ï¼Œå°†manual_urlè½¬ä¸ºinstruction_url
        if "manual_url" in product_info and "instruction_url" not in product_info:
            product_info["instruction_url"] = product_info["manual_url"]
            del product_info["manual_url"]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(product_info, f, ensure_ascii=False, indent=2)
        
        print(f"\nå·²ä¿å­˜äº§å“ä¿¡æ¯åˆ° {filename}")

    def extract_youtube_videos(self, soup, url):
        """æå–é¡µé¢ä¸­çš„YouTubeè§†é¢‘é“¾æ¥"""
        videos = []
        try:
            # æŸ¥æ‰¾è§†é¢‘æŒ‡å—åŒºåŸŸ
            video_guides_section = None
            video_sections = soup.find_all(id=lambda x: x and "Video_Guides" in x)
            if video_sections:
                video_guides_section = video_sections[0]
                self.print_debug(f"æ‰¾åˆ°è§†é¢‘æŒ‡å—åŒºåŸŸ: {video_guides_section.get_text().strip()}")
                
            # å¦‚æœæ‰¾ä¸åˆ°ä¸“é—¨çš„è§†é¢‘åŒºåŸŸï¼Œåˆ™å°è¯•æŸ¥æ‰¾é¡µé¢ä¸­æ‰€æœ‰è§†é¢‘
            if video_guides_section:
                # ä»è§†é¢‘æŒ‡å—åŒºåŸŸå¼€å§‹æŸ¥æ‰¾
                parent = video_guides_section.parent
                if parent:
                    # åœ¨åŒä¸€ä¸ªåŒºåŸŸå†…æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ¡†
                    video_boxes = []
                    # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¤§æ ‡é¢˜ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
                    next_elem = video_guides_section.find_next()
                    while next_elem and not (next_elem.name == 'h2' or next_elem.name == 'h1'):
                        if 'videoBox' in next_elem.get('class', []) or next_elem.find(class_='videoBox'):
                            video_boxes.append(next_elem)
                        elif next_elem.find(attrs={'videoid': True}):
                            video_boxes.append(next_elem)
                        next_elem = next_elem.find_next()
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å­å…ƒç´ 
                    if not video_boxes:
                        video_boxes = parent.find_all(class_='videoBox') or parent.find_all(attrs={'videoid': True})
                        
                    for box in video_boxes:
                        # å°è¯•ä»å…ƒç´ ä¸­æå–è§†é¢‘ID
                        video_elem = box.find(attrs={'videoid': True})
                        if not video_elem and 'videoid' in box.attrs:
                            video_elem = box
                            
                        if video_elem and 'videoid' in video_elem.attrs:
                            video_id = video_elem['videoid']
                            # æ„å»ºYouTubeé“¾æ¥
                            video_url = f"https://www.youtube.com/watch?v={video_id}"

                            # å°è¯•æå–è§†é¢‘æ ‡é¢˜
                            video_title = ""
                            # æŸ¥æ‰¾è§†é¢‘æ ‡é¢˜ï¼Œé€šå¸¸åœ¨å‰é¢çš„h3æ ‡ç­¾ä¸­
                            prev_header = box.find_previous(['h3', 'h4'])
                            if prev_header:
                                video_title = prev_header.get_text().strip()
                                # ç§»é™¤ä¸­æ–‡å­—ç¬¦ï¼Œä¿æŒè‹±æ–‡å†…å®¹
                                video_title = re.sub(r'[\u4e00-\u9fff]+', '', video_title)
                                video_title = re.sub(r'\s+', ' ', video_title).strip()

                            if not video_title:
                                # å¦‚æœæ‰¾ä¸åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨è‹±æ–‡æ ‡é¢˜
                                video_title = "Video Guide"

                            videos.append({
                                "url": video_url,
                                "title": video_title,
                                "video_id": video_id
                            })
                            self.print_debug(f"æ‰¾åˆ°è§†é¢‘: {video_title} -> {video_url}")
            
            # å¦‚æœæ²¡æœ‰ä»è§†é¢‘åŒºåŸŸæ‰¾åˆ°ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢æŸ¥æ‰¾
            if not videos:
                video_elems = soup.find_all(attrs={'videoid': True})
                for video_elem in video_elems:
                    video_id = video_elem['videoid']
                    video_url = f"https://www.youtube.com/watch?v={video_id}"
                    
                    # å°è¯•æå–è§†é¢‘æ ‡é¢˜
                    video_title = "Video Guide"
                    # æŸ¥æ‰¾æœ€è¿‘çš„æ ‡é¢˜æ ‡ç­¾
                    prev_header = video_elem.find_previous(['h1', 'h2', 'h3', 'h4', 'h5'])
                    if prev_header:
                        video_title = prev_header.get_text().strip()
                        # ç§»é™¤ä¸­æ–‡å­—ç¬¦ï¼Œä¿æŒè‹±æ–‡å†…å®¹
                        video_title = re.sub(r'[\u4e00-\u9fff]+', '', video_title)
                        video_title = re.sub(r'\s+', ' ', video_title).strip()

                    videos.append({
                        "url": video_url,
                        "title": video_title,
                        "video_id": video_id
                    })
                    self.print_debug(f"æ‰¾åˆ°è§†é¢‘: {video_title} -> {video_url}")
                    
        except Exception as e:
            self.print_debug(f"æå–è§†é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
        return videos

    def extract_product_info_from_url(self, url):
        """ä»URLç›´æ¥æå–äº§å“ä¿¡æ¯ï¼Œé€‚ç”¨äºç›´æ¥è°ƒç”¨çš„åœºæ™¯"""
        soup = self.get_soup(url)
        if not soup:
            self.print_debug(f"æ— æ³•è·å–é¡µé¢å†…å®¹: {url}")
            return None
            
        return self.extract_product_info(soup, url, [])

if __name__ == "__main__":
    crawler = IFixitCrawler()
    # ä»è®¾å¤‡é¡µé¢å¼€å§‹çˆ¬å–
    crawler.start_crawl("https://www.ifixit.com/Device")
 