#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•´åˆçˆ¬è™«å·¥å…· - ç»“åˆæ ‘å½¢ç»“æ„å’Œè¯¦ç»†å†…å®¹æå– + ä»£ç†æ±  + æœ¬åœ°æ–‡ä»¶å­˜å‚¨
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
æ”¯æŒHTTPéš§é“ä»£ç†æ± ã€æœ¬åœ°æ–‡ä»¶å¤¹å­˜å‚¨ã€åª’ä½“æ–‡ä»¶ä¸‹è½½ç­‰åŠŸèƒ½
ç”¨æ³•: python auto_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python auto_crawler.py https://www.ifixit.com/Device/Television
      python auto_crawler.py Television
"""

import os
import sys
import json
import time
import random
import re
import requests
import httpx
import asyncio
import aiofiles
from urllib.parse import urlparse, urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
import hashlib
import mimetypes
import threading
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import queue

# å¯¼å…¥ä¸¤ä¸ªåŸºç¡€çˆ¬è™«
from enhanced_crawler import EnhancedIFixitCrawler
from tree_crawler import TreeCrawler


class TunnelProxyManager:
    """éš§é“ä»£ç†ç®¡ç†å™¨ - ä½¿ç”¨HTTPéš§é“ä»£ç†æ± ï¼Œæ”¯æŒå¤šä»£ç†å¹¶å‘"""

    def __init__(self, tunnel_host="b353.kdltpspro.com", tunnel_port=15818,
                 username="t15237111788411", password="htjqu9dr", max_reset_cycles=3,
                 pool_size=10):
        """
        åˆå§‹åŒ–éš§é“ä»£ç†ç®¡ç†å™¨

        Args:
            tunnel_host: éš§é“åŸŸå
            tunnel_port: éš§é“ç«¯å£
            username: ç”¨æˆ·å
            password: å¯†ç 
            max_reset_cycles: æœ€å¤§é‡ç½®å‘¨æœŸæ•°
            pool_size: ä»£ç†æ± å¤§å°ï¼ˆå¹¶å‘è¿æ¥æ•°ï¼‰
        """
        self.tunnel_host = tunnel_host
        self.tunnel_port = tunnel_port
        self.username = username
        self.password = password
        self.pool_size = pool_size

        self.proxy_pool = []  # ä»£ç†æ± 
        self.current_index = 0  # å½“å‰ä»£ç†ç´¢å¼•
        self.proxy_switch_count = 0  # è¯·æ±‚è®¡æ•°å™¨
        self.proxy_lock = threading.Lock()

        # æ·»åŠ é‡ç½®å¾ªç¯é™åˆ¶
        self.max_reset_cycles = max_reset_cycles
        self.reset_cycle_count = 0
        self.failed_count = 0  # å¤±è´¥è®¡æ•°

        # åˆå§‹åŒ–ä»£ç†æ± 
        self._init_proxy_pool()

    def _init_proxy_pool(self):
        """åˆå§‹åŒ–ä»£ç†æ±  - åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„ä»£ç†è¿æ¥"""
        try:
            # æ„å»ºåŸºç¡€ä»£ç†URL
            base_proxy_url = f"http://{self.username}:{self.password}@{self.tunnel_host}:{self.tunnel_port}"

            # åˆ›å»ºä»£ç†æ±  - æ¯ä¸ªä»£ç†éƒ½æ˜¯ç‹¬ç«‹çš„è¿æ¥
            for i in range(self.pool_size):
                proxy_config = {
                    'http': base_proxy_url,
                    'https': base_proxy_url,
                    'id': i,
                    'active': True,
                    'failed_count': 0
                }
                self.proxy_pool.append(proxy_config)

            print(f"ğŸ“‹ å¤šä»£ç†æ± åˆå§‹åŒ–æˆåŠŸ")
            print(f"ğŸŒ éš§é“åœ°å€: {self.tunnel_host}:{self.tunnel_port}")
            print(f"ğŸ‘¤ ç”¨æˆ·å: {self.username}")
            print(f"ğŸ”„ ä»£ç†æ± å¤§å°: {self.pool_size} ä¸ªå¹¶å‘è¿æ¥")
            print("ğŸ’¡ æ”¯æŒçœŸæ­£çš„å¤šä»£ç†å¹¶å‘ï¼Œæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹ä»£ç†")

        except Exception as e:
            print(f"âŒ ä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
            self.proxy_pool = []

    def get_proxy(self, thread_id=None):
        """è·å–ä»£ç†é…ç½® - æ”¯æŒçº¿ç¨‹ç‹¬ç«‹ä»£ç†åˆ†é…"""
        with self.proxy_lock:
            if not self.proxy_pool:
                print("âŒ ä»£ç†æ± æœªåˆå§‹åŒ–")
                return None

            # å¦‚æœæŒ‡å®šäº†çº¿ç¨‹IDï¼Œå°è¯•ä¸ºè¯¥çº¿ç¨‹åˆ†é…å›ºå®šä»£ç†
            if thread_id is not None:
                proxy_index = thread_id % len(self.proxy_pool)
                proxy = self.proxy_pool[proxy_index]
                if proxy['active']:
                    return {
                        'http': proxy['http'],
                        'https': proxy['https'],
                        'proxy_id': proxy['id']
                    }

            # è½®è¯¢åˆ†é…ä»£ç†
            active_proxies = [p for p in self.proxy_pool if p['active']]
            if not active_proxies:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„ä»£ç†")
                return None

            # ä½¿ç”¨è½®è¯¢æ–¹å¼åˆ†é…
            proxy = active_proxies[self.current_index % len(active_proxies)]
            self.current_index = (self.current_index + 1) % len(active_proxies)

            return {
                'http': proxy['http'],
                'https': proxy['https'],
                'proxy_id': proxy['id']
            }

    def mark_proxy_failed(self, proxy, reason=""):
        """æ ‡è®°ä»£ç†å¤±è´¥"""
        with self.proxy_lock:
            if proxy and 'proxy_id' in proxy:
                proxy_id = proxy['proxy_id']
                if proxy_id < len(self.proxy_pool):
                    self.proxy_pool[proxy_id]['failed_count'] += 1
                    failed_count = self.proxy_pool[proxy_id]['failed_count']

                    print(f"âš ï¸  ä»£ç† #{proxy_id} è¯·æ±‚å¤±è´¥ (ç¬¬{failed_count}æ¬¡): {reason}")

                    # å¦‚æœæŸä¸ªä»£ç†å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæš‚æ—¶ç¦ç”¨
                    if failed_count >= 5:
                        self.proxy_pool[proxy_id]['active'] = False
                        print(f"ğŸš« ä»£ç† #{proxy_id} å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæš‚æ—¶ç¦ç”¨")

                        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¯ç”¨ä»£ç†
                        active_count = sum(1 for p in self.proxy_pool if p['active'])
                        if active_count == 0:
                            print("âš ï¸  æ‰€æœ‰ä»£ç†éƒ½è¢«ç¦ç”¨ï¼Œé‡æ–°æ¿€æ´»æ‰€æœ‰ä»£ç†")
                            for p in self.proxy_pool:
                                p['active'] = True
                                p['failed_count'] = 0
            else:
                self.failed_count += 1
                print(f"âš ï¸  ä»£ç†è¯·æ±‚å¤±è´¥ (ç¬¬{self.failed_count}æ¬¡): {reason}")

    def get_stats(self):
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        active_count = sum(1 for p in self.proxy_pool if p['active'])
        total_failed = sum(p['failed_count'] for p in self.proxy_pool)

        return {
            'tunnel_host': self.tunnel_host,
            'tunnel_port': self.tunnel_port,
            'username': self.username,
            'pool_size': self.pool_size,
            'active_proxies': active_count,
            'total_failed': total_failed,
            'proxy_details': [
                {
                    'id': p['id'],
                    'active': p['active'],
                    'failed_count': p['failed_count']
                } for p in self.proxy_pool
            ]
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        with self.proxy_lock:
            for proxy in self.proxy_pool:
                proxy['failed_count'] = 0
                proxy['active'] = True
            self.failed_count = 0
            print("ğŸ“Š ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®ï¼Œæ‰€æœ‰ä»£ç†é‡æ–°æ¿€æ´»")


class AsyncHttpClientManager:
    """å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨ - åŸºäºhttpxçš„é«˜æ€§èƒ½å¼‚æ­¥è¯·æ±‚"""

    def __init__(self, proxy_manager=None, max_connections=500, max_keepalive_connections=100,
                 timeout=3.0, max_retries=2):
        """
        åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ›´é«˜å¹¶å‘ï¼‰

        Args:
            proxy_manager: ä»£ç†ç®¡ç†å™¨å®ä¾‹
            max_connections: æœ€å¤§è¿æ¥æ•°ï¼ˆæå‡åˆ°200ä»¥æ”¯æŒæ›´é«˜å¹¶å‘ï¼‰
            max_keepalive_connections: æœ€å¤§ä¿æŒè¿æ¥æ•°ï¼ˆæå‡åˆ°50ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.proxy_manager = proxy_manager
        self.max_connections = max_connections
        self.max_keepalive_connections = max_keepalive_connections
        self.timeout = timeout
        self.max_retries = max_retries

        # è¯·æ±‚å¤´é…ç½®
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1"
        }

        # åª’ä½“æ–‡ä»¶ä¸“ç”¨è¯·æ±‚å¤´
        self.media_headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://www.ifixit.com/",
            "Origin": "https://www.ifixit.com",
            "Sec-Fetch-Dest": "image",
            "Sec-Fetch-Mode": "no-cors",
            "Sec-Fetch-Site": "same-site",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }

        self._client = None
        self._semaphore = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._init_client()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self._close_client()

    async def _init_client(self):
        """åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
        if not self._client:
            self._semaphore = asyncio.Semaphore(self.max_connections)
            self._client = httpx.AsyncClient(
                timeout=self.timeout,
                follow_redirects=True,
                limits=httpx.Limits(
                    max_connections=self.max_connections,
                    max_keepalive_connections=self.max_keepalive_connections
                ),
                headers=self.headers
            )

    async def _close_client(self):
        """å…³é—­å¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
        if self._client:
            await self._client.aclose()
            self._client = None

    async def get(self, url, headers=None, **kwargs):
        """å¼‚æ­¥GETè¯·æ±‚"""
        if not self._client:
            await self._init_client()

        async with self._semaphore:
            request_headers = self.headers.copy()
            if headers:
                request_headers.update(headers)

            return await self._client.get(url, headers=request_headers, **kwargs)

    async def get_with_retry(self, url, headers=None, max_retries=None, **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„å¼‚æ­¥GETè¯·æ±‚"""
        if max_retries is None:
            max_retries = self.max_retries

        last_error = None

        for attempt in range(max_retries + 1):
            try:
                response = await self.get(url, headers=headers, **kwargs)
                
                # ç‰¹æ®Šå¤„ç†404é”™è¯¯ - ä¸é‡è¯•ï¼Œç›´æ¥è¿”å›å“åº”
                if response.status_code == 404:
                    print(f"âš ï¸ é¡µé¢ä¸å­˜åœ¨ (404): {url}")
                    return response
                    
                response.raise_for_status()
                return response

            except (httpx.ProxyError, httpx.ConnectTimeout, httpx.ConnectError, httpx.ReadTimeout, httpx.TimeoutException) as e:
                # ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†å¹¶é‡è¯•
                if self.proxy_manager and attempt < max_retries:
                    print(f"ğŸ”„ ç½‘ç»œé”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç† (ç¬¬{attempt+1}æ¬¡): {str(e)[:100]}")
                    self.proxy_manager.mark_proxy_failed(None, f"ç½‘ç»œé”™è¯¯: {str(e)[:50]}")

                    # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯ä»¥ä½¿ç”¨æ–°ä»£ç†
                    await self._close_client()
                    await self._init_client()

                    # æ·»åŠ æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                    delay = min(2 ** attempt, 10)  # æœ€å¤§å»¶è¿Ÿ10ç§’
                    print(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)

                last_error = e

            except Exception as e:
                last_error = e

                # ç‰¹æ®Šå¤„ç†äº‹ä»¶å¾ªç¯å…³é—­çš„æƒ…å†µ
                if "Event loop is closed" in str(e):
                    return None

                if attempt < max_retries:
                    # æé€Ÿé‡è¯•ç­–ç•¥ï¼Œæœ€å°åŒ–ç­‰å¾…æ—¶é—´
                    base_delay = 0.2  # åŸºç¡€å»¶è¿Ÿ0.2ç§’
                    linear_increment = 0.5  # æ¯æ¬¡é‡è¯•å¢åŠ 0.5ç§’
                    max_delay = 3  # æœ€å¤§å»¶è¿Ÿ3ç§’
                    delay = min(max_delay, base_delay + (attempt * linear_increment) + random.uniform(0, 0.3))
                    await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œä½†ä¸è¦æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯è¿”å›Noneè®©è°ƒç”¨è€…å¤„ç†
        if last_error:
            print(f"âš ï¸ è¯·æ±‚æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡æ­¤URL: {url} - {str(last_error)[:100]}")
            return None

    async def download_file(self, url, local_path, headers=None):
        """å¼‚æ­¥ä¸‹è½½æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæå‡ä¸‹è½½é€Ÿåº¦ï¼‰"""
        if not headers:
            headers = self.media_headers

        try:
            response = await self.get_with_retry(url, headers=headers)
            if response is None:
                return None

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # å¼‚æ­¥å†™å…¥æ–‡ä»¶ï¼Œä¼˜åŒ–chunkå¤§å°ä»¥æå‡æ€§èƒ½
            async with aiofiles.open(local_path, 'wb') as f:
                async for chunk in response.aiter_bytes(chunk_size=16384):  # å¢åŠ chunkå¤§å°
                    await f.write(chunk)

            return local_path

        except Exception as e:
            return None

    async def download_files_batch(self, download_tasks, max_concurrent=20):
        """æ‰¹é‡å¼‚æ­¥ä¸‹è½½æ–‡ä»¶ï¼Œä¼˜åŒ–æ€§èƒ½"""
        semaphore = asyncio.Semaphore(max_concurrent)

        async def download_with_semaphore(url, local_path, headers=None):
            async with semaphore:
                try:
                    # æ£€æŸ¥äº‹ä»¶å¾ªç¯æ˜¯å¦ä»ç„¶æ´»è·ƒ
                    loop = asyncio.get_event_loop()
                    if loop.is_closed():
                        return None

                    return await self.download_file(url, local_path, headers)
                except Exception as e:
                    return None

        # åˆ›å»ºæ‰€æœ‰ä¸‹è½½ä»»åŠ¡
        tasks = [download_with_semaphore(url, path, headers)
                for url, path, headers in download_tasks]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä¸‹è½½
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r is not None and not isinstance(r, Exception))
        return success_count, len(tasks)


class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - ä¼˜åŒ–çˆ¬è™«é‡å¤æ‰§è¡Œæ•ˆç‡"""

    def __init__(self, storage_root, logger=None, force_refresh=False):
        self.storage_root = Path(storage_root)
        self.logger = logger or logging.getLogger(__name__)
        self.cache_index_file = self.storage_root / "cache_index.json"
        self.cache_index = {}
        self.force_refresh = force_refresh  # æ·»åŠ å¼ºåˆ¶åˆ·æ–°æ ‡å¿—
        self.stats = {
            'total_urls': 0,
            'cached_urls': 0,
            'new_urls': 0,
            'invalid_cache': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        self.load_cache_index()

    def load_cache_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            if self.cache_index_file.exists():
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.cache_index = data.get('entries', {})
                    self.logger.info(f"å·²åŠ è½½ç¼“å­˜ç´¢å¼•ï¼ŒåŒ…å« {len(self.cache_index)} ä¸ªæ¡ç›®")
            else:
                self.cache_index = {}
                self.logger.info("ç¼“å­˜ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ç´¢å¼•")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
            self.cache_index = {}

    def save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            self.storage_root.mkdir(parents=True, exist_ok=True)
            cache_data = {
                'version': '1.0',
                'last_updated': datetime.now(timezone.utc).isoformat(),
                'entries': self.cache_index
            }
            with open(self.cache_index_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ç¼“å­˜ç´¢å¼•å·²ä¿å­˜ï¼ŒåŒ…å« {len(self.cache_index)} ä¸ªæ¡ç›®")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def get_url_hash(self, url):
        """ç”ŸæˆURLçš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

    def is_url_cached_and_valid(self, url, local_path):
        """æ£€æŸ¥URLæ˜¯å¦å·²ç¼“å­˜ä¸”æ•°æ®æœ‰æ•ˆ"""
        url_hash = self.get_url_hash(url)
        self.stats['total_urls'] += 1

        self.logger.info(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
        self.logger.info(f"   URLå“ˆå¸Œ: {url_hash}")
        self.logger.info(f"   æœ¬åœ°è·¯å¾„: {local_path}")

        # æ£€æŸ¥ç¼“å­˜ç´¢å¼•ä¸­æ˜¯å¦å­˜åœ¨
        if url_hash not in self.cache_index:
            self.logger.info(f"   âŒ ç¼“å­˜ç´¢å¼•ä¸­ä¸å­˜åœ¨æ­¤URL")
            self.stats['new_urls'] += 1
            self.stats['cache_misses'] += 1
            return False

        cache_entry = self.cache_index[url_hash]
        self.logger.info(f"   ğŸ“‹ æ‰¾åˆ°ç¼“å­˜æ¡ç›®: {cache_entry.get('processed_time', 'N/A')}")

        # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not local_path.exists():
            self.logger.info(f"   âŒ æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_path}")
            self.stats['invalid_cache'] += 1
            self.stats['cache_misses'] += 1
            return False

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if not self._validate_cached_data(local_path, cache_entry):
            self.logger.info(f"   âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            self.stats['invalid_cache'] += 1
            self.stats['cache_misses'] += 1
            return False

        self.logger.info(f"   âœ… ç¼“å­˜æœ‰æ•ˆï¼Œå‘½ä¸­!")
        self.stats['cached_urls'] += 1
        self.stats['cache_hits'] += 1
        return True

    def _validate_cached_data(self, local_path, cache_entry):
        """éªŒè¯ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§ - æ™ºèƒ½é€‚é…ä¸åŒçš„æ–‡ä»¶ç»“æ„"""
        try:
            self.logger.info(f"   ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")

            # æ£€æŸ¥info.jsonæ–‡ä»¶
            info_file = local_path / "info.json"
            if not info_file.exists():
                self.logger.info(f"   âŒ info.jsonæ–‡ä»¶ä¸å­˜åœ¨")
                return False

            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            # æ£€æŸ¥troubleshootingç¼“å­˜çš„ä¸€è‡´æ€§
            url = cache_entry.get('url', '')
            # troubleshootingç¼“å­˜ç°åœ¨å®Œå…¨ç‹¬ç«‹ç®¡ç†ï¼Œä¸ä¾èµ–cache_indexæ£€æŸ¥

            structure = cache_entry.get('structure', {})
            self.logger.info(f"   ğŸ“Š é¢„æœŸç»“æ„: {structure}")

            # æ™ºèƒ½éªŒè¯ï¼šå¦‚æœæ²¡æœ‰ç»“æ„ä¿¡æ¯ï¼Œä»å®é™…æ–‡ä»¶æ¨æ–­
            if not structure:
                structure = self._infer_structure_from_files(local_path)
                self.logger.info(f"   ğŸ” æ¨æ–­çš„ç»“æ„: {structure}")

            # éªŒè¯guidesç›®å½•å’Œæ–‡ä»¶
            if structure.get('has_guides', False):
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"   âŒ guidesç›®å½•ä¸å­˜åœ¨")
                    return False

                # æ£€æŸ¥guideå­ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ–°çš„ç›®å½•ç»“æ„ï¼šguide_1/guide.jsonï¼‰
                guide_subdirs = list(guides_dir.glob("guide_*"))
                expected_count = structure.get('guides_count', 0)

                # éªŒè¯æ¯ä¸ªguideå­ç›®å½•éƒ½æœ‰guide.jsonæ–‡ä»¶
                valid_guides = 0
                for guide_subdir in guide_subdirs:
                    guide_file = guide_subdir / "guide.json"
                    if guide_file.exists():
                        valid_guides += 1

                if valid_guides != expected_count:
                    self.logger.info(f"   âŒ æŒ‡å—æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {valid_guides}")
                    return False
                else:
                    self.logger.info(f"   âœ… æŒ‡å—æ–‡ä»¶éªŒè¯é€šè¿‡: {valid_guides} ä¸ª")

            # éªŒè¯troubleshootingç›®å½•å’Œæ–‡ä»¶
            if structure.get('has_troubleshooting', False):
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"   âŒ troubleshootingç›®å½•ä¸å­˜åœ¨")
                    return False

                # æ£€æŸ¥troubleshootingå­ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ–°çš„ç›®å½•ç»“æ„ï¼štroubleshooting_1/troubleshooting.jsonï¼‰
                ts_subdirs = list(ts_dir.glob("troubleshooting_*"))
                expected_count = structure.get('troubleshooting_count', 0)

                # éªŒè¯æ¯ä¸ªtroubleshootingå­ç›®å½•éƒ½æœ‰troubleshooting.jsonæ–‡ä»¶
                valid_troubleshooting = 0
                for ts_subdir in ts_subdirs:
                    ts_file = ts_subdir / "troubleshooting.json"
                    if ts_file.exists():
                        valid_troubleshooting += 1

                if valid_troubleshooting != expected_count:
                    self.logger.info(f"   âŒ æ•…éšœæ’é™¤æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {valid_troubleshooting}")
                    return False
                else:
                    self.logger.info(f"   âœ… æ•…éšœæ’é™¤æ–‡ä»¶éªŒè¯é€šè¿‡: {valid_troubleshooting} ä¸ª")

            # éªŒè¯åª’ä½“æ–‡ä»¶
            if structure.get('has_media', False):
                media_dir = local_path / "media"
                if media_dir.exists():
                    media_files = list(media_dir.glob("*"))
                    if len(media_files) == 0 and structure.get('media_count', 0) > 0:
                        self.logger.info(f"   âŒ åª’ä½“æ–‡ä»¶ç¼ºå¤±")
                        return False
                    else:
                        self.logger.info(f"   âœ… åª’ä½“æ–‡ä»¶éªŒè¯é€šè¿‡: {len(media_files)} ä¸ª")

            self.logger.info(f"   âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            self.logger.error(f"   âŒ éªŒè¯ç¼“å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def _infer_structure_from_files(self, local_path):
        """ä»å®é™…æ–‡ä»¶æ¨æ–­ç¼“å­˜ç»“æ„"""
        structure = {
            'has_guides': False,
            'guides_count': 0,
            'has_troubleshooting': False,
            'troubleshooting_count': 0,
            'has_media': False,
            'media_count': 0
        }

        try:
            # æ£€æŸ¥guidesç›®å½•
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                # æ£€æŸ¥æ–°æ ¼å¼ï¼šguide_*/guide.json
                guide_subdirs = list(guides_dir.glob("guide_*"))
                valid_guides = sum(1 for subdir in guide_subdirs if (subdir / "guide.json").exists())

                # å¦‚æœæ²¡æœ‰æ–°æ ¼å¼ï¼Œæ£€æŸ¥æ—§æ ¼å¼ï¼š*.json
                if valid_guides == 0:
                    guide_files = list(guides_dir.glob("*.json"))
                    valid_guides = len(guide_files)

                if valid_guides > 0:
                    structure['has_guides'] = True
                    structure['guides_count'] = valid_guides

            # æ£€æŸ¥troubleshootingç›®å½•
            ts_dir = local_path / "troubleshooting"
            if ts_dir.exists():
                # æ£€æŸ¥æ–°æ ¼å¼ï¼štroubleshooting_*/troubleshooting.json
                ts_subdirs = list(ts_dir.glob("troubleshooting_*"))
                valid_ts = sum(1 for subdir in ts_subdirs if (subdir / "troubleshooting.json").exists())

                # å¦‚æœæ²¡æœ‰æ–°æ ¼å¼ï¼Œæ£€æŸ¥æ—§æ ¼å¼ï¼š*.json
                if valid_ts == 0:
                    ts_files = list(ts_dir.glob("*.json"))
                    valid_ts = len(ts_files)

                if valid_ts > 0:
                    structure['has_troubleshooting'] = True
                    structure['troubleshooting_count'] = valid_ts

            # æ£€æŸ¥mediaç›®å½•
            media_dir = local_path / "media"
            if media_dir.exists():
                media_files = list(media_dir.glob("*"))
                if media_files:
                    structure['has_media'] = True
                    structure['media_count'] = len(media_files)

        except Exception as e:
            self.logger.error(f"æ¨æ–­æ–‡ä»¶ç»“æ„å¤±è´¥: {e}")

        return structure

    def add_to_cache(self, url, local_path, guides_count=0, troubleshooting_count=0, media_count=0):
        """æ·»åŠ URLåˆ°ç¼“å­˜ç´¢å¼•"""
        url_hash = self.get_url_hash(url)

        # åˆ†ææœ¬åœ°æ•°æ®ç»“æ„
        structure = self._analyze_local_structure(local_path, guides_count, troubleshooting_count, media_count)

        cache_entry = {
            'url': url,
            'local_path': str(local_path.relative_to(self.storage_root)),
            'processed_time': datetime.now(timezone.utc).isoformat(),
            'content_hash': self._generate_content_hash(local_path),
            'structure': structure,
            'status': 'complete'
        }

        self.cache_index[url_hash] = cache_entry
        self.logger.info(f"å·²æ·»åŠ åˆ°ç¼“å­˜: {url}")

    def _analyze_local_structure(self, local_path, guides_count=0, troubleshooting_count=0, media_count=0):
        """åˆ†ææœ¬åœ°æ•°æ®ç»“æ„ - ä¸å†åŒ…å«troubleshootingä¿¡æ¯ï¼Œç”±ç‹¬ç«‹ç¼“å­˜ç®¡ç†"""
        structure = {
            'has_guides': False,
            'guides_count': 0,
            'has_media': False,
            'media_count': 0
        }

        try:
            # æ£€æŸ¥guidesç›®å½•
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                guide_files = list(guides_dir.glob("*.json"))
                structure['has_guides'] = len(guide_files) > 0
                structure['guides_count'] = len(guide_files)
            elif guides_count > 0:
                structure['has_guides'] = True
                structure['guides_count'] = guides_count

            # troubleshootingä¿¡æ¯ä¸å†è®°å½•åœ¨cache_indexä¸­ï¼Œç”±troubleshooting_cache.jsonç‹¬ç«‹ç®¡ç†

            # æ£€æŸ¥mediaç›®å½•
            media_dir = local_path / "media"
            if media_dir.exists():
                media_files = list(media_dir.glob("*"))
                structure['has_media'] = len(media_files) > 0
                structure['media_count'] = len(media_files)
            elif media_count > 0:
                structure['has_media'] = True
                structure['media_count'] = media_count

        except Exception as e:
            self.logger.error(f"åˆ†ææœ¬åœ°ç»“æ„æ—¶å‡ºé”™: {e}")

        return structure

    def _find_actual_device_path(self, device_url):
        """æŸ¥æ‰¾è®¾å¤‡URLå¯¹åº”çš„å®é™…å­˜åœ¨è·¯å¾„"""
        try:
            # ä»URLæå–è®¾å¤‡åç§°
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_name = unquote(device_path)

            # åœ¨Deviceç›®å½•ä¸‹æœç´¢åŒ…å«è®¾å¤‡åç§°çš„ç›®å½•
            device_root = self.storage_root / "Device"
            if not device_root.exists():
                return None

            # ç”Ÿæˆå¯èƒ½çš„è®¾å¤‡åç§°å˜ä½“
            name_variants = [
                device_name,
                device_name.replace('%22', '"'),
                device_name.replace('"', '%22'),
                device_path,  # åŸå§‹ç¼–ç ç‰ˆæœ¬
                unquote(device_path)  # è§£ç ç‰ˆæœ¬
            ]

            # ä¼˜å…ˆæŸ¥æ‰¾ç›´æ¥åŒ¹é…çš„ç›®å½•
            for variant in name_variants:
                direct_path = device_root / variant
                if direct_path.exists():
                    self.logger.info(f"æ‰¾åˆ°ç›´æ¥åŒ¹é…è·¯å¾„: {direct_path}")
                    return direct_path

            # é€’å½’æœç´¢åŒ¹é…çš„ç›®å½•
            self.logger.info(f"å¼€å§‹é€’å½’æœç´¢è®¾å¤‡è·¯å¾„ï¼ŒæŸ¥æ‰¾: {name_variants}")

            for path in device_root.rglob("*"):
                if path.is_dir():
                    # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ¹é…ä»»ä½•å˜ä½“
                    for variant in name_variants:
                        if path.name == variant:
                            self.logger.info(f"æ‰¾åˆ°åç§°åŒ¹é…è·¯å¾„: {path}")
                            return path

                    # æ£€æŸ¥æ˜¯å¦æœ‰troubleshooting_cache.jsonæ–‡ä»¶
                    ts_cache_file = path / "troubleshooting_cache.json"
                    if ts_cache_file.exists():
                        # éªŒè¯è¿™æ˜¯å¦æ˜¯æ­£ç¡®çš„è®¾å¤‡ç›®å½•
                        path_str = str(path)
                        for variant in name_variants:
                            if variant in path_str:
                                self.logger.info(f"é€šè¿‡troubleshooting_cache.jsonæ‰¾åˆ°åŒ¹é…è·¯å¾„: {path}")
                                return path

            self.logger.warning(f"æœªæ‰¾åˆ°è®¾å¤‡è·¯å¾„ï¼Œæœç´¢çš„å˜ä½“: {name_variants}")
            return None

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾å®é™…è®¾å¤‡è·¯å¾„å¤±è´¥: {e}")
            return None

    def _generate_content_hash(self, local_path):
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼"""
        try:
            info_file = local_path / "info.json"
            if info_file.exists():
                with open(info_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œæ—¶å‡ºé”™: {e}")
        return ""

    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def is_troubleshooting_section_cached(self, cache_key, device_url):
        """æ£€æŸ¥troubleshootingéƒ¨åˆ†æ˜¯å¦å·²ç¼“å­˜"""
        try:
            # ä½¿ç”¨ä¸save_troubleshooting_cacheç›¸åŒçš„è·¯å¾„é€»è¾‘
            local_path = self._get_troubleshooting_cache_path(cache_key, device_url)
            ts_cache_file = local_path / "troubleshooting_cache.json"

            if not ts_cache_file.exists():
                self.logger.info(f"ğŸ” Troubleshootingç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {ts_cache_file}")
                return False

            # éªŒè¯ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§
            with open(ts_cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            # æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if not isinstance(cached_data, list):
                self.logger.info(f"ğŸ” Troubleshootingç¼“å­˜æ•°æ®æ ¼å¼æ— æ•ˆ")
                return False

            self.logger.info(f"âœ… Troubleshootingéƒ¨åˆ†ç¼“å­˜æœ‰æ•ˆ: {len(cached_data)} ä¸ªé¡¹ç›®")
            return True

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥troubleshootingç¼“å­˜å¤±è´¥: {e}")
            return False

    def _get_troubleshooting_cache_path(self, cache_key, device_url):
        """è·å–troubleshootingç¼“å­˜çš„è·¯å¾„ - ä¸Troubleshootingæ–‡ä»¶å¤¹åœ¨åŒä¸€å±‚çº§"""
        try:
            # troubleshooting_cache.jsonåº”è¯¥ä¸troubleshootingæ–‡ä»¶å¤¹åœ¨åŒä¸€å±‚çº§
            if '/Device/' in device_url:
                # é¦–å…ˆå°è¯•æŸ¥æ‰¾å®é™…å­˜åœ¨troubleshootingæ–‡ä»¶å¤¹çš„è·¯å¾„
                actual_path = self._find_troubleshooting_directory_path(device_url)
                if actual_path:
                    self.logger.info(f"âœ… æ‰¾åˆ°è®¾å¤‡ç›®å½•ç”¨äºtroubleshootingç¼“å­˜: {actual_path}")
                    return actual_path

                # å¦‚æœæ²¡æ‰¾åˆ°å®é™…çš„troubleshootingç›®å½•ï¼Œé¢„æµ‹å°†æ¥çš„è®¾å¤‡ç›®å½•è·¯å¾„
                predicted_path = self._predict_device_directory_path(device_url)
                if predicted_path:
                    self.logger.info(f"ğŸ”® é¢„æµ‹è®¾å¤‡ç›®å½•ç”¨äºtroubleshootingç¼“å­˜: {predicted_path}")
                    return predicted_path

                # æœ€åçš„å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å“ˆå¸Œè·¯å¾„
                self.logger.info(f"ğŸ” æ— æ³•é¢„æµ‹è®¾å¤‡ç›®å½•ï¼Œä½¿ç”¨å“ˆå¸Œè·¯å¾„")
                url_hash = self.get_url_hash(device_url)
                fallback_path = Path(self.storage_root) / "cache" / url_hash[:8]
                return fallback_path
            else:
                # éè®¾å¤‡URLä½¿ç”¨å“ˆå¸Œè·¯å¾„
                url_hash = self.get_url_hash(device_url)
                fallback_path = Path(self.storage_root) / "cache" / url_hash[:8]
                return fallback_path
        except Exception as e:
            self.logger.error(f"è·å–troubleshootingç¼“å­˜è·¯å¾„å¤±è´¥: {e}")
            # ç¡®ä¿æ€»æ˜¯è¿”å›ä¸€ä¸ªæœ‰æ•ˆè·¯å¾„
            url_hash = self.get_url_hash(device_url)
            fallback_path = Path(self.storage_root) / "cache" / url_hash[:8]
            return fallback_path

    def _build_device_path_from_url(self, device_url):
        """åŸºäºURLæ„å»ºè®¾å¤‡è·¯å¾„ - ä¸å®é™…æ–‡ä»¶ä¿å­˜é€»è¾‘ä¿æŒä¸€è‡´"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„è·¯å¾„ä¿¡æ¯
            device_hash = self.get_url_hash(device_url)
            if device_hash in self.cache_index:
                cache_entry = self.cache_index[device_hash]
                cached_path = self.storage_root / cache_entry.get('local_path', '')
                if cached_path.exists():
                    return cached_path

            # æ³¨æ„ï¼šCacheManager ä¸ç›´æ¥è®¿é—®ç½‘ç»œï¼Œè·³è¿‡é¢åŒ…å±‘æ–¹æ³•
            # é¢åŒ…å±‘è·¯å¾„æ„å»ºåº”è¯¥åœ¨å®é™…çš„çˆ¬è™«ç±»ä¸­å¤„ç†

            # å¦‚æœé¢åŒ…å±‘æ–¹æ³•å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„ç›®å½•ç»“æ„å¯ä»¥å‚è€ƒ
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)

            device_root = self.storage_root / "Device"
            if device_root.exists():
                # å°è¯•æ‰¾åˆ°åŒ…å«è®¾å¤‡åç§°çš„ç›®å½•
                device_name = device_path.split('/')[-1]
                for path in device_root.rglob("*"):
                    if path.is_dir() and path.name == device_name:
                        # æ‰¾åˆ°åŒ¹é…çš„è®¾å¤‡ç›®å½•
                        return path

            # æœ€åçš„å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨URLè·¯å¾„æ„å»º
            path_parts = device_path.split('/')
            if len(path_parts) > 0:
                main_category = path_parts[0]
                if len(path_parts) > 1:
                    sub_path = '/'.join(path_parts[1:])
                    return self.storage_root / "Device" / main_category / sub_path
                else:
                    return self.storage_root / "Device" / main_category
            else:
                return self.storage_root / "Device" / device_path

        except Exception as e:
            self.logger.error(f"æ„å»ºè®¾å¤‡è·¯å¾„å¤±è´¥: {e}")
            # æœ€ç»ˆå›é€€åˆ°ç®€å•è·¯å¾„
            device_path = device_url.split('/Device/')[-1].split('?')[0].rstrip('/')
            return self.storage_root / "Device" / device_path.replace('/', '_')

    def _find_troubleshooting_directory_path(self, device_url):
        """æŸ¥æ‰¾å®é™…å­˜åœ¨troubleshootingæ–‡ä»¶å¤¹çš„è·¯å¾„"""
        try:
            # ä»device_urlæå–è®¾å¤‡è·¯å¾„ä¿¡æ¯
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)

            # åœ¨Deviceç›®å½•ä¸‹é€’å½’æœç´¢åŒ…å«troubleshootingæ–‡ä»¶å¤¹çš„ç›®å½•
            device_root = self.storage_root / "Device"
            if not device_root.exists():
                return None

            device_name = device_path.split('/')[-1]
            self.logger.info(f"æœç´¢troubleshootingæ–‡ä»¶å¤¹ï¼Œè®¾å¤‡åç§°: {device_name}")

            # é¦–å…ˆå°è¯•ç›´æ¥æ„å»ºè·¯å¾„
            direct_path = self.storage_root / "Device" / device_path
            if direct_path.exists():
                self.logger.info(f"ç›´æ¥æ‰¾åˆ°è®¾å¤‡ç›®å½•: {direct_path}")
                # å³ä½¿troubleshootingç›®å½•ä¸å­˜åœ¨ï¼Œä¹Ÿè¿”å›è®¾å¤‡ç›®å½•ï¼Œä»¥ä¾¿åç»­åˆ›å»ºcacheæ–‡ä»¶
                return direct_path

            # é€’å½’æœç´¢troubleshootingæ–‡ä»¶å¤¹
            for path in device_root.rglob("troubleshooting"):
                if path.is_dir():
                    parent_path = path.parent

                    # æ£€æŸ¥çˆ¶ç›®å½•åæ˜¯å¦åŒ¹é…è®¾å¤‡åç§°
                    if parent_path.name.lower() == device_name.lower():
                        self.logger.info(f"é€šè¿‡è®¾å¤‡åç§°åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

                    # æ£€æŸ¥URLç¼–ç çš„ç‰ˆæœ¬
                    if device_name.replace('"', '%22') == parent_path.name or device_name.replace('%22', '"') == parent_path.name:
                        self.logger.info(f"é€šè¿‡URLç¼–ç åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

                    # æ£€æŸ¥è·¯å¾„å±‚çº§æ˜¯å¦åŒ¹é…è®¾å¤‡è·¯å¾„ç»“æ„
                    relative_path = parent_path.relative_to(device_root)
                    relative_path_str = str(relative_path)

                    # æ£€æŸ¥è®¾å¤‡è·¯å¾„æ˜¯å¦åŒ…å«åœ¨ç›¸å¯¹è·¯å¾„ä¸­ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
                    if device_path.lower() == relative_path_str.lower():
                        self.logger.info(f"é€šè¿‡å®Œæ•´è·¯å¾„åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

                    # æ£€æŸ¥ç›¸å¯¹è·¯å¾„æ˜¯å¦ä»¥è®¾å¤‡åç§°ç»“å°¾
                    if relative_path_str.lower().endswith(device_name.lower()):
                        self.logger.info(f"é€šè¿‡è·¯å¾„ç»“å°¾åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°troubleshootingç›®å½•ï¼Œå°è¯•æŸ¥æ‰¾è®¾å¤‡ç›®å½•
            for path in device_root.rglob("*"):
                if path.is_dir() and path.name.lower() == device_name.lower():
                    self.logger.info(f"æ‰¾åˆ°è®¾å¤‡ç›®å½•: {path}")
                    return path

            self.logger.info(f"æœªæ‰¾åˆ°troubleshootingæ–‡ä»¶å¤¹ï¼Œè®¾å¤‡è·¯å¾„: {device_path}")
            return None

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾troubleshootingç›®å½•å¤±è´¥: {e}")
            return None

    def _predict_device_directory_path(self, device_url):
        """é¢„æµ‹è®¾å¤‡ç›®å½•çš„è·¯å¾„ï¼ŒåŸºäºURLç»“æ„å’Œç°æœ‰çš„ç›®å½•æ¨¡å¼"""
        try:
            # ä»device_urlæå–è®¾å¤‡è·¯å¾„ä¿¡æ¯
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)
            device_name = device_path.split('/')[-1]  # è·å–è®¾å¤‡åç§°

            # æ„å»ºé¢„æœŸçš„è®¾å¤‡ç›®å½•è·¯å¾„
            device_root = self.storage_root / "Device"

            # æ–¹æ³•1: æŸ¥æ‰¾ç°æœ‰çš„è®¾å¤‡ç›®å½•ï¼ˆæœ€å‡†ç¡®çš„æ–¹æ³•ï¼‰
            if device_root.exists():
                # é€’å½’æœç´¢åŒ¹é…çš„è®¾å¤‡ç›®å½•
                for existing_path in device_root.rglob("*"):
                    if existing_path.is_dir() and existing_path.name == device_name:
                        self.logger.info(f"æ‰¾åˆ°ç°æœ‰è®¾å¤‡ç›®å½•: {existing_path}")
                        return existing_path

                # æœç´¢åŒ…å«è®¾å¤‡åç§°çš„ç›®å½•ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
                for existing_path in device_root.rglob("*"):
                    if existing_path.is_dir():
                        # æ£€æŸ¥ç›®å½•åæ˜¯å¦ä¸è®¾å¤‡åç§°åŒ¹é…ï¼ˆè€ƒè™‘URLç¼–ç ï¼‰
                        if (existing_path.name.lower() == device_name.lower() or
                            existing_path.name.replace('_', ' ').lower() == device_name.replace('_', ' ').lower() or
                            existing_path.name.replace('%22', '"').lower() == device_name.replace('%22', '"').lower()):
                            self.logger.info(f"æ‰¾åˆ°åŒ¹é…çš„è®¾å¤‡ç›®å½•: {existing_path}")
                            return existing_path

            # æ–¹æ³•2: åŸºäºç°æœ‰ç›®å½•ç»“æ„é¢„æµ‹è·¯å¾„
            if device_root.exists():
                # æŸ¥æ‰¾ç›¸ä¼¼çš„ç›®å½•ç»“æ„æ¨¡å¼
                for existing_path in device_root.rglob("*"):
                    if existing_path.is_dir():
                        relative_path = existing_path.relative_to(device_root)
                        path_parts = str(relative_path).split('/')

                        # å¦‚æœæ‰¾åˆ°æ·±åº¦ç›¸ä¼¼çš„ç›®å½•ç»“æ„ï¼Œå°è¯•æ„å»ºç›¸ä¼¼è·¯å¾„
                        if len(path_parts) >= 3:  # è‡³å°‘æœ‰3å±‚æ·±åº¦çš„ç›®å½•
                            # å°è¯•åŸºäºç°æœ‰ç»“æ„æ„å»ºè·¯å¾„
                            # ä¾‹å¦‚ï¼šMac/Mac_Laptop/MacBook_Pro/MacBook_Pro_16/device_name
                            parent_structure = '/'.join(path_parts[:-1])
                            predicted_path = device_root / parent_structure / device_name

                            # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨
                            if predicted_path.parent.exists():
                                self.logger.info(f"é¢„æµ‹è·¯å¾„åŸºäºç°æœ‰ç»“æ„: {predicted_path}")
                                return predicted_path

            # æ–¹æ³•3: ç›´æ¥ä½¿ç”¨URLè·¯å¾„ç»“æ„
            predicted_path = device_root / device_path
            if predicted_path.parent.exists():  # å¦‚æœçˆ¶ç›®å½•å­˜åœ¨ï¼Œè¯´æ˜è·¯å¾„ç»“æ„åˆç†
                self.logger.info(f"é¢„æµ‹è·¯å¾„åŸºäºURLç»“æ„: {predicted_path}")
                return predicted_path

            # æ–¹æ³•4: ä½¿ç”¨ç®€å•çš„è®¾å¤‡åç§°è·¯å¾„ï¼ˆæœ€åçš„å›é€€æ–¹æ¡ˆï¼‰
            simple_path = device_root / device_name
            self.logger.info(f"é¢„æµ‹è·¯å¾„ä½¿ç”¨ç®€å•ç»“æ„: {simple_path}")
            return simple_path

        except Exception as e:
            self.logger.error(f"é¢„æµ‹è®¾å¤‡ç›®å½•è·¯å¾„å¤±è´¥: {e}")
            return None

    def load_troubleshooting_cache(self, cache_key, device_url):
        """åŠ è½½ç¼“å­˜çš„troubleshootingæ•°æ®"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„è·å–æ–¹æ³•
            local_path = self._get_troubleshooting_cache_path(cache_key, device_url)
            ts_cache_file = local_path / "troubleshooting_cache.json"

            if not ts_cache_file.exists():
                return None

            with open(ts_cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            self.logger.info(f"ğŸ“‹ åŠ è½½troubleshootingç¼“å­˜: {len(cached_data)} ä¸ªé¡¹ç›®")
            self.stats['cache_hits'] += 1
            return cached_data

        except Exception as e:
            self.logger.error(f"åŠ è½½troubleshootingç¼“å­˜å¤±è´¥: {e}")
            return None

    def save_troubleshooting_cache(self, cache_key, device_url, troubleshooting_data):
        """ä¿å­˜troubleshootingæ•°æ®åˆ°ç¼“å­˜"""
        try:
            if not troubleshooting_data:
                return

            # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„è·å–æ–¹æ³•
            local_path = self._get_troubleshooting_cache_path(cache_key, device_url)

            # ç°åœ¨_get_troubleshooting_cache_pathæ€»æ˜¯è¿”å›ä¸€ä¸ªè·¯å¾„ï¼Œä¸ä¼šè¿”å›None
            if local_path is None:
                self.logger.error(f"âŒ æ— æ³•ç¡®å®štroubleshootingç¼“å­˜è·¯å¾„")
                return

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path.mkdir(parents=True, exist_ok=True)

            ts_cache_file = local_path / "troubleshooting_cache.json"

            # ç¼“å­˜ä¿æŠ¤æœºåˆ¶ï¼šå¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ä¸”ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if not self.force_refresh and ts_cache_file.exists():
                try:
                    with open(ts_cache_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)

                    # å¦‚æœç°æœ‰æ•°æ®ä¸ä¸ºç©ºä¸”æ–°æ•°æ®ä¸ç°æœ‰æ•°æ®ç›¸åŒï¼Œåˆ™è·³è¿‡ä¿å­˜
                    if existing_data and len(existing_data) >= len(troubleshooting_data):
                        self.logger.info(f"ğŸ”’ ä¿æŠ¤ç°æœ‰troubleshootingç¼“å­˜: {len(existing_data)} ä¸ªé¡¹ç›® (è·³è¿‡è¦†ç›–)")
                        return
                except Exception as e:
                    self.logger.warning(f"è¯»å–ç°æœ‰troubleshootingç¼“å­˜å¤±è´¥ï¼Œå°†ä¿å­˜æ–°æ•°æ®: {e}")

            # ä¿å­˜troubleshootingæ•°æ®
            with open(ts_cache_file, 'w', encoding='utf-8') as f:
                json.dump(troubleshooting_data, f, ensure_ascii=False, indent=2)

            # troubleshootingç¼“å­˜ç°åœ¨å®Œå…¨ç‹¬ç«‹ï¼Œä¸å†æ·»åŠ åˆ°ä¸»ç¼“å­˜ç´¢å¼•
            self.logger.info(f"ğŸ’¾ ä¿å­˜troubleshootingç¼“å­˜: {len(troubleshooting_data)} ä¸ªé¡¹ç›®")

        except Exception as e:
            self.logger.error(f"ä¿å­˜troubleshootingç¼“å­˜å¤±è´¥: {e}")

    def save_troubleshooting_cache_after_directory_creation(self, cache_key, device_url, troubleshooting_data, ts_dir):
        """åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶"""
        try:
            if not troubleshooting_data or not ts_dir:
                return

            # è·å–Troubleshootingç›®å½•çš„çˆ¶ç›®å½•ï¼ˆä¸Troubleshootingæ–‡ä»¶å¤¹åŒå±‚ï¼‰
            cache_dir = ts_dir.parent
            ts_cache_file = cache_dir / "troubleshooting_cache.json"

            # ç¼“å­˜ä¿æŠ¤æœºåˆ¶ï¼šå¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ä¸”ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if not self.force_refresh and ts_cache_file.exists():
                try:
                    with open(ts_cache_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)

                    # å¦‚æœç°æœ‰æ•°æ®ä¸ä¸ºç©ºä¸”æ–°æ•°æ®ä¸ç°æœ‰æ•°æ®ç›¸åŒï¼Œåˆ™è·³è¿‡ä¿å­˜
                    if existing_data and len(existing_data) >= len(troubleshooting_data):
                        self.logger.info(f"ğŸ”’ ä¿æŠ¤ç°æœ‰troubleshootingç¼“å­˜: {len(existing_data)} ä¸ªé¡¹ç›® (è·³è¿‡è¦†ç›–)")
                        return
                except Exception as e:
                    self.logger.warning(f"è¯»å–ç°æœ‰troubleshootingç¼“å­˜å¤±è´¥ï¼Œå°†ä¿å­˜æ–°æ•°æ®: {e}")

            # ä¿å­˜troubleshootingæ•°æ®
            with open(ts_cache_file, 'w', encoding='utf-8') as f:
                json.dump(troubleshooting_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"ğŸ’¾ åœ¨Troubleshootingç›®å½•åˆ›å»ºåä¿å­˜cache: {ts_cache_file} ({len(troubleshooting_data)} ä¸ªé¡¹ç›®)")

        except Exception as e:
            self.logger.error(f"åœ¨ç›®å½•åˆ›å»ºåä¿å­˜troubleshootingç¼“å­˜å¤±è´¥: {e}")

    def _generate_content_hash_for_data(self, data):
        """ä¸ºæ•°æ®ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼"""
        try:
            content_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(content_str.encode('utf-8')).hexdigest()[:16]
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ•°æ®å“ˆå¸Œæ—¶å‡ºé”™: {e}")
            return ""

    def display_cache_report(self):
        """æ˜¾ç¤ºç¼“å­˜æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æ™ºèƒ½ç¼“å­˜æŠ¥å‘Š")
        print("="*60)
        print(f"æ€»URLæ•°é‡: {self.stats['total_urls']}")
        print(f"ç¼“å­˜å‘½ä¸­: {self.stats['cached_urls']} (è·³è¿‡å¤„ç†)")
        print(f"éœ€è¦å¤„ç†: {self.stats['new_urls']} (æ–°å¢æˆ–æ›´æ–°)")
        print(f"æ— æ•ˆç¼“å­˜: {self.stats['invalid_cache']} (éœ€è¦é‡æ–°å¤„ç†)")

        if self.stats['total_urls'] > 0:
            hit_rate = (self.stats['cached_urls'] / self.stats['total_urls']) * 100
            print(f"ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")

        print("="*60)

    def clean_invalid_cache(self):
        """æ¸…ç†æ— æ•ˆçš„ç¼“å­˜æ¡ç›®"""
        invalid_keys = []

        for url_hash, cache_entry in self.cache_index.items():
            local_path = self.storage_root / cache_entry['local_path']
            if not local_path.exists() or not self._validate_cached_data(local_path, cache_entry):
                invalid_keys.append(url_hash)

        for key in invalid_keys:
            del self.cache_index[key]

        if invalid_keys:
            self.logger.info(f"å·²æ¸…ç† {len(invalid_keys)} ä¸ªæ— æ•ˆç¼“å­˜æ¡ç›®")
            self.save_cache_index()

        return len(invalid_keys)


class CombinedIFixitCrawler(EnhancedIFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", verbose=False, use_proxy=True,
                 use_cache=True, force_refresh=False, max_workers=16, max_retries=1,
                 download_videos=False, max_video_size_mb=50, max_connections=None,
                 timeout=5, request_delay=0.01, proxy_switch_freq=1, cache_ttl=24,
                 custom_user_agent=None, burst_mode=False, conservative_mode=False,
                 skip_images=False, debug_mode=False, show_stats=False, enable_resume=True):
        super().__init__(base_url, verbose)

        # ç«‹å³åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œç¡®ä¿loggerå¯ç”¨
        self._setup_logging()

        self.enable_resume = enable_resume
        self.tree_crawler = TreeCrawler(base_url, enable_resume=enable_resume, logger=self.logger, verbose=verbose)
        self.processed_nodes = set()
        self.target_url = None

        # æœ¬åœ°å­˜å‚¨é…ç½®ï¼ˆéœ€è¦åœ¨ç¼“å­˜ç®¡ç†å™¨ä¹‹å‰è®¾ç½®ï¼‰
        # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æ•°æ®ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º ifixit_data
        self.storage_root = os.getenv('IFIXIT_DATA_DIR', 'ifixit_data')
        self.media_folder = "media"

        # ğŸš€ é«˜æ€§èƒ½é…ç½®
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.max_connections = max_connections or (max_workers * 10)
        self.timeout = timeout
        self.request_delay = request_delay
        self.proxy_switch_freq = proxy_switch_freq
        self.cache_ttl = cache_ttl
        self.custom_user_agent = custom_user_agent
        self.burst_mode = burst_mode
        self.conservative_mode = conservative_mode
        self.skip_images = skip_images
        self.debug_mode = debug_mode
        self.show_stats = show_stats

        # ç¼“å­˜é…ç½®
        self.use_cache = use_cache
        self.force_refresh = force_refresh
        self.cache_manager = CacheManager(self.storage_root, self.logger, force_refresh) if use_cache else None

        # ä»£ç†æ± é…ç½®
        self.use_proxy = use_proxy
        # åˆ›å»ºä»£ç†æ± ï¼Œå¤§å°ä¸çº¿ç¨‹æ•°åŒ¹é…
        proxy_pool_size = max(max_workers, 10) if use_proxy else 0
        self.proxy_manager = TunnelProxyManager(pool_size=proxy_pool_size) if use_proxy else None
        self.failed_urls = set()  # æ·»åŠ å¤±è´¥URLé›†åˆ

        # å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨
        self.async_http_manager = None

        # è§†é¢‘å¤„ç†é…ç½®
        self.download_videos = download_videos
        self.max_video_size_mb = max_video_size_mb
        self.video_extensions = ['.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv']

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "retry_success": 0,
            "retry_failed": 0,
            "total_retries": 0,  # æ·»åŠ ç¼ºå¤±çš„total_retriesé”®
            "media_downloaded": 0,
            "media_failed": 0,
            "videos_skipped": 0,
            "videos_downloaded": 0
        }

        # å¹¶å‘é…ç½®
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.thread_local = threading.local()

        # é”™è¯¯å¤„ç†é…ç½®
        self.failed_log_file = "failed_urls.log"

        # Troubleshootingå¤„ç†çŠ¶æ€è·Ÿè¸ª
        self.processed_troubleshooting_paths = set()  # è·Ÿè¸ªå·²å¤„ç†troubleshootingçš„è·¯å¾„
        self._troubleshooting_from_cache = False  # æ ‡è®°troubleshootingæ•°æ®æ˜¯å¦æ¥è‡ªç¼“å­˜

        # åª’ä½“æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç¼“å­˜ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
        self._file_exists_cache = {}  # ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç»“æœ
        self._cache_max_size = 1000   # ç¼“å­˜æœ€å¤§å¤§å°

        # æ‰“å°è§†é¢‘å¤„ç†é…ç½®
        if self.download_videos:
            self.logger.info(f"âœ… è§†é¢‘ä¸‹è½½å·²å¯ç”¨ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {max_video_size_mb}MB")
        else:
            self.logger.info("âš ï¸ è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå°†ä¿ç•™åŸå§‹URL")

    def _load_proxy_config(self):
        """åŠ è½½ä»£ç†é…ç½®"""
        print("ğŸ“‹ ä½¿ç”¨HTTPéš§é“ä»£ç†æœåŠ¡")

        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()

        # éš§é“ä»£ç†å·²åœ¨__init__ä¸­åˆå§‹åŒ–ï¼Œæ— éœ€é¢å¤–é…ç½®

        # å¤åˆ¶æ ‘å½¢çˆ¬è™«çš„é‡è¦æ–¹æ³•åˆ°å½“å‰ç±»
        self.find_exact_path = self.tree_crawler.find_exact_path
        self.extract_breadcrumbs_from_page = self.tree_crawler.extract_breadcrumbs_from_page
        self._build_tree_from_path = self.tree_crawler._build_tree_from_path
        self._find_node_by_url = self.tree_crawler._find_node_by_url
        self.ensure_instruction_url_in_leaf_nodes = self.tree_crawler.ensure_instruction_url_in_leaf_nodes

        # é‡å†™tree_crawlerçš„get_soupæ–¹æ³•ï¼Œè®©å®ƒä½¿ç”¨æˆ‘ä»¬çš„å¢å¼ºç‰ˆæœ¬
        self.tree_crawler.get_soup = self._tree_crawler_get_soup

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO if self.verbose else logging.WARNING,
            format=log_format,
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _check_cache_validity(self, url, local_path):
        """æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ - ä½¿ç”¨æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
        if not self.use_cache or self.force_refresh:
            return False

        # ä½¿ç”¨CacheManagerè¿›è¡Œæ™ºèƒ½ç¼“å­˜æ£€æŸ¥
        if self.cache_manager:
            return self.cache_manager.is_url_cached_and_valid(url, local_path)

        # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰çš„ç®€å•ç¼“å­˜æ£€æŸ¥
        return self._legacy_cache_check(url, local_path)

    def _legacy_cache_check(self, url, local_path):
        """åŸæœ‰çš„ç¼“å­˜æ£€æŸ¥é€»è¾‘ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        if not local_path.exists():
            return False

        # æ£€æŸ¥info.jsonæ˜¯å¦å­˜åœ¨
        info_file = local_path / "info.json"
        if not info_file.exists():
            self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘info.json - {url}")
            return False

        try:
            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            # æ£€æŸ¥guidesç›®å½•
            if 'guides' in info_data:
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘guidesç›®å½• - {url}")
                    return False

            # æ£€æŸ¥troubleshootingç›®å½•
            if 'troubleshooting' in info_data:
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘troubleshootingç›®å½• - {url}")
                    return False

            # æ£€æŸ¥mediaæ–‡ä»¶
            media_dir = local_path / self.media_folder
            if media_dir.exists():
                # ç®€å•æ£€æŸ¥ï¼šå¦‚æœmediaç›®å½•å­˜åœ¨ä½†ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ä¸‹è½½å¤±è´¥
                media_files = list(media_dir.glob("*"))
                if len(media_files) == 0:
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶
                    if self._should_have_media(info_data):
                        self.logger.info(f"ç¼“å­˜æ— æ•ˆ: mediaç›®å½•ä¸ºç©ºä½†åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶ - {url}")
                        return False

            self.logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡çˆ¬å–: {url}")
            return True

        except Exception as e:
            self.logger.error(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e} - {url}")
            return False

    def _should_have_media(self, data):
        """æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åº”è¯¥åŒ…å«åª’ä½“æ–‡ä»¶"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and value:
                    return True
                elif key == 'images' and isinstance(value, list) and value:
                    return True
                elif isinstance(value, (dict, list)):
                    if self._should_have_media(value):
                        return True
        elif isinstance(data, list):
            for item in data:
                if self._should_have_media(item):
                    return True
        return False

    def _log_failed_url(self, url, error, retry_count=0):
        """è®°å½•å¤±è´¥çš„URLåˆ°æ—¥å¿—æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"{timestamp} | {url} | {error} | retry_count: {retry_count}\n"

        try:
            with open(self.failed_log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
        except Exception as e:
            self.logger.error(f"æ— æ³•å†™å…¥å¤±è´¥æ—¥å¿—: {e}")

    def _log_failed_media(self, url, error_msg):
        """è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—"""
        try:
            failed_media_log = "failed_media.log"
            with open(failed_media_log, 'a', encoding='utf-8') as f:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{timestamp}] {url} - {error_msg}\n")
        except Exception as e:
            self.logger.error(f"æ— æ³•å†™å…¥åª’ä½“å¤±è´¥æ—¥å¿—: {e}")

    def _exponential_backoff(self, attempt):
        """æé€Ÿé‡è¯•ç­–ç•¥ - æœ€å°åŒ–ç­‰å¾…æ—¶é—´"""
        # æé€Ÿæ¨¡å¼ï¼šæœ€å°å»¶è¿Ÿï¼Œå¿«é€Ÿé‡è¯•
        base_delay = 0.5  # åŸºç¡€å»¶è¿Ÿ0.5ç§’
        linear_increment = 1  # æ¯æ¬¡é‡è¯•å¢åŠ 1ç§’
        max_delay = 5  # æœ€å¤§å»¶è¿Ÿ5ç§’
        
        delay = min(max_delay, base_delay + (attempt * linear_increment) + random.uniform(0, 0.5))
        time.sleep(delay)
        return delay

    def _is_temporary_error(self, error):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶æ€§é”™è¯¯"""
        temporary_errors = [
            "timeout", "connection", "network", "502", "503", "504",
            "429", "500", "ConnectionError", "Timeout", "ReadTimeout"
        ]
        error_str = str(error).lower()
        return any(temp_err in error_str for temp_err in temporary_errors)

    def _retry_with_backoff(self, func, *args, **kwargs):
        """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶"""
        last_error = None

        for attempt in range(self.max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    self.stats["retry_success"] += 1
                    self.logger.info(f"é‡è¯•æˆåŠŸ (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                return result

            except Exception as e:
                last_error = e
                self.stats["total_retries"] += 1

                if not self._is_temporary_error(e):
                    self.logger.warning(f"æ°¸ä¹…æ€§é”™è¯¯ï¼Œè·³è¿‡æ­¤é¡¹: {e}")
                    # å¯¹äºæ°¸ä¹…æ€§é”™è¯¯ï¼Œè¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    return None

                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    self.logger.warning(f"é‡è¯• {attempt+1}/{self.max_retries} (å»¶è¿Ÿ{delay:.1f}s): {e}")

                    # ç½‘ç»œé”™è¯¯æ—¶åˆ‡æ¢ä»£ç†
                    if self.use_proxy and ("network" in str(e).lower() or "proxy" in str(e).lower() or "timeout" in str(e).lower()):
                        self._switch_proxy(f"é‡è¯•æ—¶ç½‘ç»œé”™è¯¯: {str(e)}")

                    time.sleep(delay)
                else:
                    self.logger.warning(f"é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤é¡¹: {e}")

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›Noneè®©è°ƒç”¨è€…ç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡
        self.stats["retry_failed"] += 1
        if last_error:
            self.logger.warning(f"ä»»åŠ¡å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡: {str(last_error)}")
            return None
        else:
            return None





    def _get_next_proxy(self):
        """è·å–å½“å‰ä»£ç†æˆ–åˆ‡æ¢æ–°ä»£ç†"""
        if not self.use_proxy or not self.proxy_manager:
            return None

        # æ£€æŸ¥å½“å‰çº¿ç¨‹æ˜¯å¦æœ‰æŒ‡å®šçš„ä»£ç†ID
        current_thread = threading.current_thread()
        thread_proxy_id = getattr(current_thread, 'proxy_id', None)

        # å¢åŠ ä»£ç†ä½¿ç”¨è®¡æ•°
        if hasattr(self.proxy_manager, 'proxy_switch_count'):
            self.proxy_manager.proxy_switch_count += 1

            # å¯¹äºå¤šä»£ç†æ± ï¼Œæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹ä»£ç†
            if thread_proxy_id is not None:
                if self.verbose and self.proxy_manager.proxy_switch_count % 10 == 0:
                    print(f"ğŸ”„ å·²å¤„ç† {self.proxy_manager.proxy_switch_count} ä¸ªè¯·æ±‚ï¼ˆå¤šä»£ç†å¹¶å‘ï¼‰")
            elif self.proxy_switch_freq == 1:
                # æ¯æ¬¡è¯·æ±‚éƒ½åˆ‡æ¢ï¼ˆéš§é“ä»£ç†çš„é»˜è®¤è¡Œä¸ºï¼‰
                if self.verbose and self.proxy_manager.proxy_switch_count % 10 == 0:
                    print(f"ğŸ”„ å·²å¤„ç† {self.proxy_manager.proxy_switch_count} ä¸ªè¯·æ±‚ï¼ˆæ¯æ¬¡è‡ªåŠ¨åˆ‡æ¢IPï¼‰")
            elif self.proxy_manager.proxy_switch_count % self.proxy_switch_freq == 0:
                # æŒ‰æŒ‡å®šé¢‘ç‡åˆ‡æ¢
                print(f"ğŸ”„ è¾¾åˆ°åˆ‡æ¢é¢‘ç‡ ({self.proxy_switch_freq})ï¼Œåˆ‡æ¢ä»£ç†IP")

        return self.proxy_manager.get_proxy(thread_proxy_id)

    def _switch_proxy(self, reason="è¯·æ±‚å¤±è´¥"):
        """åˆ‡æ¢ä»£ç†IP"""
        self.logger.warning(f"âš ï¸  ä»£ç†åˆ‡æ¢åŸå› : {reason}")

        if not self.use_proxy or not self.proxy_manager:
            return False

        # å¯¹äºå¤šä»£ç†æ± ï¼Œæ ‡è®°å¤±è´¥ä½†ä¸éœ€è¦åˆ‡æ¢ï¼ˆæ¯ä¸ªçº¿ç¨‹æœ‰ç‹¬ç«‹ä»£ç†ï¼‰
        self.proxy_manager.mark_proxy_failed(None, reason)

        # è·å–æ–°ä»£ç†
        new_proxy = self.proxy_manager.get_proxy()
        return new_proxy is not None

    # éš§é“ä»£ç†ç›¸å…³æ–¹æ³•å·²é›†æˆåˆ°TunnelProxyManagerç±»ä¸­

    async def _init_async_http_manager(self):
        """åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨"""
        if not self.async_http_manager:
            # ä¼˜åŒ–ï¼šæå‡è¿æ¥æ± å¤§å°ä»¥æ”¯æŒæ›´é«˜çš„åª’ä½“ä¸‹è½½å¹¶å‘
            max_connections = max(self.max_workers * 15, 200)  # æå‡è¿æ¥æ± å¤§å°
            max_keepalive = max(self.max_workers * 3, 50)      # æå‡ä¿æŒè¿æ¥æ•°

            self.async_http_manager = AsyncHttpClientManager(
                proxy_manager=self.proxy_manager,
                max_connections=max_connections,
                max_keepalive_connections=max_keepalive,
                timeout=8.0,
                max_retries=self.max_retries
            )
            await self.async_http_manager._init_client()

    async def _close_async_http_manager(self):
        """å…³é—­å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨"""
        if self.async_http_manager:
            await self.async_http_manager._close_client()
            self.async_http_manager = None

    async def get_soup_async(self, url, use_playwright=False):
        """å¼‚æ­¥ç‰ˆæœ¬çš„get_soupæ–¹æ³•"""
        if not url:
            return None

        if url in self.failed_urls:
            self.logger.warning(f"è·³è¿‡å·²çŸ¥å¤±è´¥URL: {url}")
            return None

        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        if '?' in url:
            url += '&lang=en'
        else:
            url += '?lang=en'

        self.stats["total_requests"] += 1

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return await self._get_soup_with_playwright_async(url)

        # å¦åˆ™ä½¿ç”¨å¼‚æ­¥httpxæ–¹æ³•
        return await self._get_soup_httpx_async(url)

    async def _get_soup_httpx_async(self, url):
        """ä½¿ç”¨httpxå¼‚æ­¥è·å–é¡µé¢å†…å®¹"""
        if not self.async_http_manager:
            await self._init_async_http_manager()

        try:
            response = await self.async_http_manager.get_with_retry(url)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response is None:
                self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: å“åº”ä¸ºç©º")
                self.failed_urls.add(url)
                return None
                
            if response.status_code == 404:
                self.logger.warning(f"é¡µé¢ä¸å­˜åœ¨ {url}: 404 Not Found")
                self.failed_urls.add(url)
                # å¯¹äº404é”™è¯¯ï¼ŒæŠ›å‡ºç‰¹å®šå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
                raise httpx.HTTPStatusError("404 Not Found", request=response.request, response=response)
                
            # å¯¹äºå…¶ä»–é”™è¯¯çŠ¶æ€ç 
            if response.status_code >= 400:
                self.logger.error(f"HTTPé”™è¯¯ {url}: {response.status_code}")
                self.failed_urls.add(url)
                raise httpx.HTTPStatusError(f"HTTP {response.status_code}", request=response.request, response=response)

            from bs4 import BeautifulSoup
            return BeautifulSoup(response.content, 'html.parser')

        except httpx.HTTPStatusError:
            # ç›´æ¥å‘ä¸Šä¼ é€’HTTPçŠ¶æ€é”™è¯¯ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            raise

    async def _get_soup_with_playwright_async(self, url):
        """å¼‚æ­¥ç‰ˆæœ¬çš„Playwrighté¡µé¢è·å–"""
        try:
            from playwright.async_api import async_playwright

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´
                await page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                await page.goto(url, wait_until='networkidle')

                # è·å–é¡µé¢å†…å®¹
                content = await page.content()
                await browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            self.logger.error(f"Playwrightå¼‚æ­¥è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return None

    async def _process_tasks_async(self, tasks, guides_data, troubleshooting_data):
        """å¼‚æ­¥å¹¶å‘å¤„ç†ä»»åŠ¡åˆ—è¡¨"""
        # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
        semaphore = asyncio.Semaphore(self.max_workers)

        async def process_single_task(task_type, url):
            """å¤„ç†å•ä¸ªä»»åŠ¡çš„å¼‚æ­¥åŒ…è£…å™¨"""
            async with semaphore:
                try:
                    if task_type == 'guide':
                        return await self._process_guide_task_async(url)
                    else:  # troubleshooting
                        return await self._process_troubleshooting_task_async(url)
                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")
                    return None

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        async_tasks = [process_single_task(task_type, url) for task_type, url in tasks]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*async_tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        completed_count = 0
        total_tasks = len(tasks)

        for i, (result, (task_type, url)) in enumerate(zip(results, tasks)):
            completed_count += 1

            if isinstance(result, Exception):
                print(f"    âŒ [{completed_count}/{total_tasks}] {task_type} ä»»åŠ¡å¼‚å¸¸: {str(result)[:50]}...")
                continue

            if result:
                if task_type == 'guide':
                    guides_data.append(result)
                    print(f"    âœ… [{completed_count}/{total_tasks}] æŒ‡å—: {result.get('title', '')}")
                else:
                    troubleshooting_data.append(result)
                    print(f"    âœ… [{completed_count}/{total_tasks}] æ•…éšœæ’é™¤: {result.get('title', '')}")
            else:
                print(f"    âš ï¸  [{completed_count}/{total_tasks}] {task_type} å¤„ç†å¤±è´¥")

        return guides_data, troubleshooting_data

    async def _process_guide_task_async(self, url):
        """å¼‚æ­¥å¤„ç†æŒ‡å—ä»»åŠ¡"""
        try:
            soup = await self.get_soup_async(url)
            if soup:
                return self.extract_guide_content(soup, url)
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥å¤„ç†æŒ‡å—å¤±è´¥ {url}: {e}")
        return None

    async def _process_troubleshooting_task_async(self, url):
        """å¼‚æ­¥å¤„ç†æ•…éšœæ’é™¤ä»»åŠ¡"""
        try:
            soup = await self.get_soup_async(url)
            if soup:
                return self.extract_troubleshooting_content(soup, url)
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥å¤„ç†æ•…éšœæ’é™¤å¤±è´¥ {url}: {e}")
        return None

    def get_soup(self, url, use_playwright=False):
        """é‡å†™get_soupæ–¹æ³•ï¼Œæ”¯æŒä»£ç†æ± ã€é‡è¯•æœºåˆ¶å’ŒPlaywrightæ¸²æŸ“"""
        if not url:
            return None

        if url in self.failed_urls:
            self.logger.warning(f"è·³è¿‡å·²çŸ¥å¤±è´¥URL: {url}")
            return None

        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        if '?' in url:
            url += '&lang=en'
        else:
            url += '?lang=en'

        self.stats["total_requests"] += 1

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return self._retry_with_backoff(self._get_soup_with_playwright, url)

        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„requestsæ–¹æ³•
        return self._retry_with_backoff(self._get_soup_requests, url)

    def _get_soup_requests(self, url):
        """ä½¿ç”¨requestsè·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒæ™ºèƒ½ä»£ç†åˆ‡æ¢"""
        session = requests.Session()
        retry_strategy = Retry(
            total=0,  # å®Œå…¨ç¦ç”¨urllib3çš„é‡è¯•ï¼Œç”±ä¸Šå±‚å¤„ç†
            backoff_factor=0,  # æ— é€€é¿å»¶è¿Ÿ
            status_forcelist=[],  # ä¸é‡è¯•ä»»ä½•çŠ¶æ€ç 
            raise_on_status=False  # ä¸åœ¨çŠ¶æ€ç é”™è¯¯æ—¶ç«‹å³æŠ›å‡ºå¼‚å¸¸
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è·å–ä»£ç†
        proxies = self._get_next_proxy() if self.use_proxy else None

        try:
            response = session.get(
                url,
                headers=self.headers,
                timeout=5,  # å‡å°‘è¶…æ—¶æ—¶é—´ä»8ç§’åˆ°5ç§’
                proxies=proxies
            )
            response.raise_for_status()

            from bs4 import BeautifulSoup
            return BeautifulSoup(response.content, 'html.parser')

        except (requests.exceptions.ProxyError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ConnectionError,
                requests.exceptions.ReadTimeout,
                requests.exceptions.Timeout) as e:
            # ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†
            if self.use_proxy:
                print(f"ğŸ”„ ç½‘ç»œé”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†: {str(e)[:100]}")
                if self._switch_proxy(f"ç½‘ç»œé”™è¯¯: {str(e)[:50]}"):
                    try:
                        # ä½¿ç”¨æ–°ä»£ç†é‡è¯•ä¸€æ¬¡
                        new_proxies = self._get_next_proxy()
                        response = session.get(
                            url,
                            headers=self.headers,
                            timeout=8,
                            proxies=new_proxies
                        )
                        response.raise_for_status()

                        from bs4 import BeautifulSoup
                        return BeautifulSoup(response.content, 'html.parser')
                    except Exception as retry_e:
                        print(f"âš ï¸ ä»£ç†é‡è¯•ä¹Ÿå¤±è´¥: {str(retry_e)[:50]}")
                        # ä¸è¦æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›Noneè®©ä¸Šå±‚å¤„ç†
                        return None
            # å¦‚æœåˆ‡æ¢ä»£ç†å¤±è´¥æˆ–ä¸ä½¿ç”¨ä»£ç†ï¼Œè¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            print(f"âš ï¸ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡æ­¤URL: {str(e)[:100]}")
            return None

    def _get_soup_with_playwright(self, url):
        """ä½¿ç”¨Playwrightè·å–JavaScriptæ¸²æŸ“åçš„é¡µé¢å†…å®¹"""
        try:
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                page.goto(url, wait_until='networkidle')

                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            print(f"Playwrightè·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return None

    def _create_local_directory(self, path):
        """åˆ›å»ºæœ¬åœ°ç›®å½•ç»“æ„"""
        full_path = Path(self.storage_root) / path
        full_path.mkdir(parents=True, exist_ok=True)
        return full_path

    def _get_file_size_from_url(self, url):
        """è·å–URLæ–‡ä»¶çš„å¤§å°ï¼ˆMBï¼‰"""
        try:
            # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
            if '.thumbnail.medium' in url:
                url = url.replace('.thumbnail.medium', '.medium')

            # ä½¿ç”¨ä¸ä¸‹è½½ç›¸åŒçš„è¯·æ±‚å¤´
            media_headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://www.ifixit.com/",
                "Origin": "https://www.ifixit.com",
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "same-site",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }

            proxies = self._get_next_proxy() if self.use_proxy else None
            response = requests.head(url, headers=media_headers, timeout=10, proxies=proxies)
            if response.status_code == 200:
                content_length = response.headers.get('content-length')
                if content_length:
                    size_bytes = int(content_length)
                    size_mb = size_bytes / (1024 * 1024)
                    return size_mb
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° {url}: {e}")
        return None

    def _is_video_file(self, url):
        """æ£€æŸ¥URLæ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
        if not url:
            return False
        url_path = url.split('?')[0].lower()
        return any(url_path.endswith(ext) for ext in self.video_extensions)

    async def _download_media_file_async(self, url, local_dir, filename=None):
        """å¼‚æ­¥ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œè§†é¢‘å¤„ç†ç­–ç•¥ï¼Œæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰"""
        if not url or not url.startswith('http'):
            return None

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        local_path = local_dir / self.media_folder / filename

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸ä½¿ç”¨è·¨é¡µé¢å»é‡
        is_troubleshooting = "troubleshooting" in str(local_dir)

        if not is_troubleshooting:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰
            existing_file_path = self._find_existing_media_file(url, filename)
            if existing_file_path:
                # æ–‡ä»¶å·²å­˜åœ¨äºå…¶ä»–ä½ç½®ï¼Œè¿”å›ç›¸å¯¹äºå½“å‰ç›®å½•çš„è·¯å¾„
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                self.stats["media_downloaded"] += 1
                return existing_file_path

        # å¦‚æœæ–‡ä»¶åœ¨å½“å‰ç›®å½•å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„
        if local_path.exists():
            if self.verbose:
                self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨äºå½“å‰ç›®å½•: {filename}")
            self.stats["media_downloaded"] += 1
            # å¯¹äºtroubleshootingç›®å½•ï¼Œè¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
            if is_troubleshooting:
                return str(local_path.relative_to(local_dir))
            else:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
                try:
                    return str(local_path.relative_to(Path(self.storage_root)))
                except ValueError:
                    # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                    return str(local_path.relative_to(local_dir))

        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
        if self._is_video_file(url):
            if not self.download_videos:
                self.logger.info(f"è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰: {url}")
                self.stats["videos_skipped"] += 1
                return url

            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size_mb = await self._get_file_size_from_url_async(url)
            if file_size_mb and file_size_mb > self.max_video_size_mb:
                self.logger.warning(f"è·³è¿‡å¤§è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB > {self.max_video_size_mb}MB): {url}")
                self.stats["videos_skipped"] += 1
                return url

            self.logger.info(f"å‡†å¤‡ä¸‹è½½è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB): {url}")

        try:
            result = await self._download_media_file_impl_async(url, local_dir, filename)
            if self._is_video_file(url) and result != url:
                self.stats["videos_downloaded"] += 1
            return result
        except Exception as e:
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æœ€ç»ˆå¤±è´¥ {url}: {e}")
            self.stats["media_failed"] += 1
            self._log_failed_url(url, f"åª’ä½“ä¸‹è½½å¤±è´¥: {str(e)}")
            # è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—
            self._log_failed_media(url, str(e))
            return url

    async def _get_file_size_from_url_async(self, url):
        """å¼‚æ­¥è·å–URLæ–‡ä»¶çš„å¤§å°ï¼ˆMBï¼‰"""
        try:
            # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
            if '.thumbnail.medium' in url:
                url = url.replace('.thumbnail.medium', '.medium')

            if not self.async_http_manager:
                await self._init_async_http_manager()

            response = await self.async_http_manager.get(url, headers=self.async_http_manager.media_headers)
            if response.status_code == 200:
                content_length = response.headers.get('content-length')
                if content_length:
                    size_bytes = int(content_length)
                    size_mb = size_bytes / (1024 * 1024)
                    return size_mb
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° {url}: {e}")
        return None

    async def _download_media_file_impl_async(self, url, local_dir, filename):
        """å¼‚æ­¥åª’ä½“æ–‡ä»¶ä¸‹è½½çš„å…·ä½“å®ç°"""
        local_path = local_dir / self.media_folder / filename

        # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')
            if self.verbose:
                self.logger.info(f"URLæ ¼å¼ä¿®å¤: .thumbnail.medium -> .medium")

        if not self.async_http_manager:
            await self._init_async_http_manager()

        try:
            # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
            if self.async_http_manager:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                local_path.parent.mkdir(parents=True, exist_ok=True)
                
                # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
                # è®¾ç½®åª’ä½“ä¸‹è½½çš„è¯·æ±‚å¤´
                media_headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Referer': 'https://www.ifixit.com/'
                }
                response = await self.async_http_manager.get_with_retry(url, headers=media_headers)
                if response is None:
                    return None
                    
                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                async with aiofiles.open(local_path, 'wb') as f:
                    async for chunk in response.aiter_bytes(chunk_size=16384):
                        await f.write(chunk)
                        
                self.stats["media_downloaded"] += 1
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename}")
                else:
                    # ç®€åŒ–çš„è¿›åº¦æç¤º
                    if self.stats["media_downloaded"] % 10 == 0:
                        print(f"      ğŸ“¥ å·²ä¸‹è½½ {self.stats['media_downloaded']} ä¸ªåª’ä½“æ–‡ä»¶...")
            else:
                self.logger.error(f"å¼‚æ­¥HTTPå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¸‹è½½åª’ä½“æ–‡ä»¶: {url}")
                return None
        except Exception as e:
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™è¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
        is_troubleshooting = "troubleshooting" in str(local_dir)
        if is_troubleshooting:
            return str(local_path.relative_to(local_dir))
        else:
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
            try:
                return str(local_path.relative_to(Path(self.storage_root)))
            except ValueError:
                # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                return str(local_path.relative_to(local_dir))

    async def _download_media_file(self, url, local_dir, filename=None):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¼‚æ­¥ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œè§†é¢‘å¤„ç†ç­–ç•¥ï¼Œæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰"""
        if not url or not url.startswith('http'):
            return None

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        local_path = local_dir / self.media_folder / filename

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸ä½¿ç”¨è·¨é¡µé¢å»é‡
        is_troubleshooting = "troubleshooting" in str(local_dir)

        # ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ï¼Œé¿å…é‡å¤çš„æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
        if self._check_file_exists_cached(local_path):
            if self.verbose:
                self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨äºå½“å‰ç›®å½•: {filename}")
            self.stats["media_downloaded"] += 1
            # å¯¹äºtroubleshootingç›®å½•ï¼Œè¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
            if is_troubleshooting:
                return str(local_path.relative_to(local_dir))
            else:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
                try:
                    return str(local_path.relative_to(Path(self.storage_root)))
                except ValueError:
                    # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                    return str(local_path.relative_to(local_dir))

        if not is_troubleshooting:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰
            existing_file_path = self._find_existing_media_file(url, filename)
            if existing_file_path:
                # æ–‡ä»¶å·²å­˜åœ¨äºå…¶ä»–ä½ç½®ï¼Œè¿”å›ç›¸å¯¹äºå½“å‰ç›®å½•çš„è·¯å¾„
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                self.stats["media_downloaded"] += 1
                return existing_file_path

        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
        if self._is_video_file(url):
            if not self.download_videos:
                self.logger.info(f"è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰: {url}")
                self.stats["videos_skipped"] += 1
                return url

            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size_mb = await self._get_file_size_from_url_async(url)
            if file_size_mb and file_size_mb > self.max_video_size_mb:
                self.logger.warning(f"è·³è¿‡å¤§è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB > {self.max_video_size_mb}MB): {url}")
                self.stats["videos_skipped"] += 1
                return url

            self.logger.info(f"å‡†å¤‡ä¸‹è½½è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB): {url}")

        try:
            # ä½¿ç”¨å¼‚æ­¥å®ç°
            result = await self._download_media_file_impl(url, local_dir, filename)
            if self._is_video_file(url) and result != url:
                self.stats["videos_downloaded"] += 1
            return result
        except Exception as e:
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æœ€ç»ˆå¤±è´¥ {url}: {e}")
            self.stats["media_failed"] += 1
            self._log_failed_url(url, f"åª’ä½“ä¸‹è½½å¤±è´¥: {str(e)}")
            # è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—
            self._log_failed_media(url, str(e))
            return url

    def _find_existing_media_file(self, url, filename):
        """æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„åª’ä½“æ–‡ä»¶ï¼ˆè·¨é¡µé¢å»é‡ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        try:
            # åŸºäºURLçš„MD5å“ˆå¸Œæ¥æŸ¥æ‰¾æ–‡ä»¶
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]

            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"media_{url_hash}"
            if cache_key in self._file_exists_cache:
                return self._file_exists_cache[cache_key]

            # åœ¨æ•´ä¸ªstorage_rootç›®å½•ä¸‹æœç´¢å…·æœ‰ç›¸åŒå“ˆå¸Œçš„æ–‡ä»¶
            storage_path = Path(self.storage_root)
            if not storage_path.exists():
                self._file_exists_cache[cache_key] = None
                return None

            # æœç´¢æ‰€æœ‰mediaç›®å½•ä¸‹çš„æ–‡ä»¶
            for media_dir in storage_path.rglob("media"):
                if media_dir.is_dir():
                    # æŸ¥æ‰¾å…·æœ‰ç›¸åŒå“ˆå¸Œå‰ç¼€çš„æ–‡ä»¶
                    for existing_file in media_dir.glob(f"{url_hash}.*"):
                        if existing_file.is_file():
                            # è¿”å›ç›¸å¯¹äºstorage_rootçš„è·¯å¾„
                            result = str(existing_file.relative_to(storage_path))
                            # ç¼“å­˜ç»“æœ
                            self._cache_file_exists_result(cache_key, result)
                            return result

            # ç¼“å­˜æœªæ‰¾åˆ°çš„ç»“æœ
            self._cache_file_exists_result(cache_key, None)
            return None

        except Exception as e:
            if self.verbose:
                self.logger.warning(f"æŸ¥æ‰¾å·²å­˜åœ¨åª’ä½“æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def _cache_file_exists_result(self, cache_key, result):
        """ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç»“æœ"""
        # å¦‚æœç¼“å­˜è¿‡å¤§ï¼Œæ¸…ç†ä¸€åŠ
        if len(self._file_exists_cache) >= self._cache_max_size:
            # æ¸…ç†ç¼“å­˜çš„å‰ä¸€åŠ
            keys_to_remove = list(self._file_exists_cache.keys())[:self._cache_max_size // 2]
            for key in keys_to_remove:
                del self._file_exists_cache[key]

        self._file_exists_cache[cache_key] = result

    def _check_file_exists_cached(self, file_path):
        """å¸¦ç¼“å­˜çš„æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥"""
        cache_key = f"exists_{str(file_path)}"

        if cache_key in self._file_exists_cache:
            return self._file_exists_cache[cache_key]

        exists = file_path.exists()
        self._cache_file_exists_result(cache_key, exists)
        return exists

    async def _download_media_file_impl(self, url, local_dir, filename):
        """åª’ä½“æ–‡ä»¶ä¸‹è½½çš„å…·ä½“å®ç°ï¼ˆå¼‚æ­¥ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        local_path = local_dir / self.media_folder / filename
        local_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')
            if self.verbose:
                self.logger.info(f"URLæ ¼å¼ä¿®å¤: .thumbnail.medium -> .medium")

        # ç¡®ä¿å¼‚æ­¥HTTPç®¡ç†å™¨å·²åˆå§‹åŒ–
        if not self.async_http_manager:
            await self._init_async_http_manager()

        try:
            # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
            if self.async_http_manager:
                # è®¾ç½®åª’ä½“ä¸‹è½½çš„è¯·æ±‚å¤´
                media_headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Referer': 'https://www.ifixit.com/'
                }
                
                # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
                response = await self.async_http_manager.get_with_retry(url, headers=media_headers)
                if response is None:
                    return None
                
                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                async with aiofiles.open(local_path, 'wb') as f:
                    async for chunk in response.aiter_bytes(chunk_size=16384):
                        await f.write(chunk)
                        
                return local_path
            else:
                self.logger.error(f"å¼‚æ­¥HTTPå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¸‹è½½åª’ä½“æ–‡ä»¶: {url}")
                return None
        except Exception as e:
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™è¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
        is_troubleshooting = "troubleshooting" in str(local_dir)
        if is_troubleshooting:
            return str(local_path.relative_to(local_dir))
        else:
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
            try:
                return str(local_path.relative_to(Path(self.storage_root)))
            except ValueError:
                # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                return str(local_path.relative_to(local_dir))

    async def _process_media_urls_async(self, data, local_dir):
        """å¼‚æ­¥é€’å½’å¤„ç†æ•°æ®ä¸­çš„åª’ä½“URLï¼Œä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
        media_tasks = []

        def collect_media_urls(obj, path=""):
            """æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„åª’ä½“URL"""
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                        media_tasks.append((current_path, value, obj, key))
                    elif key == 'images' and isinstance(value, list):
                        for i, img_item in enumerate(value):
                            if isinstance(img_item, str) and img_item.startswith('http'):
                                media_tasks.append((f"{current_path}[{i}]", img_item, value, i))
                            elif isinstance(img_item, dict) and 'url' in img_item:
                                img_url = img_item['url']
                                if isinstance(img_url, str) and img_url.startswith('http'):
                                    media_tasks.append((f"{current_path}[{i}].url", img_url, img_item, 'url'))
                    elif key == 'videos' and isinstance(value, list):
                        for i, video_item in enumerate(value):
                            if isinstance(video_item, str) and video_item.startswith('http'):
                                if self.download_videos:
                                    media_tasks.append((f"{current_path}[{i}]", video_item, value, i))
                            elif isinstance(video_item, dict) and 'url' in video_item:
                                video_url = video_item['url']
                                if isinstance(video_url, str) and video_url.startswith('http'):
                                    if self.download_videos:
                                        media_tasks.append((f"{current_path}[{i}].url", video_url, video_item, 'url'))
                    elif isinstance(value, (dict, list)):
                        collect_media_urls(value, current_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    collect_media_urls(item, f"{path}[{i}]" if path else f"[{i}]")

        # æ”¶é›†æ‰€æœ‰åª’ä½“URL
        collect_media_urls(data)

        if not media_tasks:
            return

        # åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡ - ä¼˜åŒ–å¹¶å‘æ•°ä»¥æå‡æ€§èƒ½
        max_concurrent_downloads = min(20, max(self.max_workers * 2, 10))  # å¢åŠ åª’ä½“ä¸‹è½½å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent_downloads)

        async def download_single_media(path, url, container, key):
            async with semaphore:
                try:
                    local_path = await self._download_media_file(url, local_dir)
                    if local_path and local_path != url:
                        container[key] = local_path
                    return True
                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥åª’ä½“ä¸‹è½½å¤±è´¥ {url}: {e}")
                    self.stats["media_failed"] += 1
                    return False

        # å¹¶å‘ä¸‹è½½æ‰€æœ‰åª’ä½“æ–‡ä»¶
        download_tasks = [download_single_media(path, url, container, key)
                         for path, url, container, key in media_tasks]

        if download_tasks:
            print(f"      ğŸ“¥ å¼€å§‹é«˜å¹¶å‘å¼‚æ­¥ä¸‹è½½ {len(download_tasks)} ä¸ªåª’ä½“æ–‡ä»¶ï¼ˆå¹¶å‘æ•°: {max_concurrent_downloads}ï¼‰...")
            start_time = time.time()
            results = await asyncio.gather(*download_tasks, return_exceptions=True)
            end_time = time.time()
            success_count = sum(1 for r in results if r is True)
            download_time = end_time - start_time
            print(f"      âœ… å¼‚æ­¥ä¸‹è½½å®Œæˆ: {success_count}/{len(download_tasks)} æˆåŠŸï¼Œè€—æ—¶ {download_time:.2f}ç§’")

    def _process_media_urls(self, data, local_dir):
        """é€’å½’å¤„ç†æ•°æ®ä¸­çš„åª’ä½“URLï¼Œä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„åª’ä½“URL
        media_urls = []

        def collect_urls(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                        media_urls.append((obj, key, value))
                    elif key == 'images' and isinstance(value, list):
                        for i, img_item in enumerate(value):
                            if isinstance(img_item, str) and img_item.startswith('http'):
                                media_urls.append((value, i, img_item))
                            elif isinstance(img_item, dict) and 'url' in img_item:
                                img_url = img_item['url']
                                if isinstance(img_url, str) and img_url.startswith('http'):
                                    media_urls.append((img_item, 'url', img_url))
                    elif key == 'videos' and isinstance(value, list):
                        for i, video_item in enumerate(value):
                            if isinstance(video_item, str) and video_item.startswith('http'):
                                if self.download_videos:
                                    media_urls.append((value, i, video_item))
                            elif isinstance(video_item, dict) and 'url' in video_item:
                                video_url = video_item['url']
                                if isinstance(video_url, str) and video_url.startswith('http'):
                                    if self.download_videos:
                                        media_urls.append((video_item, 'url', video_url))
                    elif isinstance(value, (dict, list)):
                        collect_urls(value)
            elif isinstance(obj, list):
                for item in obj:
                    collect_urls(item)

        collect_urls(data)

        # å¦‚æœæœ‰åª’ä½“URLéœ€è¦ä¸‹è½½ï¼Œå°è¯•ä½¿ç”¨å¼‚æ­¥æ–¹å¼
        if media_urls:
            try:
                # æ£€æµ‹æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
                asyncio.get_running_loop()
                # å¦‚æœå·²ç»åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œä½¿ç”¨åŒæ­¥å›é€€æ–¹æ¡ˆé¿å…åµŒå¥—äº‹ä»¶å¾ªç¯é—®é¢˜
                if self.verbose:
                    self.logger.info("æ£€æµ‹åˆ°å¼‚æ­¥ç¯å¢ƒï¼Œä½¿ç”¨åŒæ­¥å›é€€æ–¹æ¡ˆå¤„ç†åª’ä½“æ–‡ä»¶")
                self._process_media_urls_sync_fallback(data, local_dir)
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥å®‰å…¨åœ°åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                try:
                    if self.verbose:
                        self.logger.info("åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯è¿›è¡Œå¼‚æ­¥åª’ä½“å¤„ç†")
                    asyncio.run(self._process_collected_media_urls_async(media_urls, local_dir))
                except Exception as e:
                    # å¦‚æœå¼‚æ­¥å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†
                    self.logger.warning(f"å¼‚æ­¥åª’ä½“å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†: {e}")
                    self._process_media_urls_sync_fallback(data, local_dir)
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ï¼Œç›´æ¥ä½¿ç”¨åŒæ­¥å¤„ç†
                self.logger.warning(f"äº‹ä»¶å¾ªç¯æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨åŒæ­¥å¤„ç†: {e}")
                self._process_media_urls_sync_fallback(data, local_dir)

    async def _process_collected_media_urls_async(self, media_urls, local_dir):
        """å¼‚æ­¥å¤„ç†æ”¶é›†åˆ°çš„åª’ä½“URL"""
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        download_tasks = []

        async def download_and_update(container, key, url):
            try:
                local_path = await self._download_media_file(url, local_dir)
                if local_path and local_path != url:
                    container[key] = local_path
                return True
            except Exception as e:
                self.logger.error(f"åª’ä½“ä¸‹è½½å¤±è´¥ {url}: {e}")
                return False

        for container, key, url in media_urls:
            task = download_and_update(container, key, url)
            download_tasks.append(task)

        if download_tasks:
            # é™åˆ¶å¹¶å‘æ•°
            semaphore = asyncio.Semaphore(min(15, len(download_tasks)))

            async def limited_download(task):
                async with semaphore:
                    return await task

            limited_tasks = [limited_download(task) for task in download_tasks]
            results = await asyncio.gather(*limited_tasks, return_exceptions=True)
            success_count = sum(1 for r in results if r is True)
            print(f"      ğŸ“¥ æ‰¹é‡åª’ä½“ä¸‹è½½å®Œæˆ: {success_count}/{len(download_tasks)} æˆåŠŸ")

    def _download_media_file_sync(self, url, local_dir, filename=None):
        """åŒæ­¥ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆç”¨äºå›é€€æ–¹æ¡ˆï¼‰"""
        if not url or not url.startswith('http'):
            return url

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        local_path = local_dir / self.media_folder / filename

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›æœ¬åœ°è·¯å¾„
        if local_path.exists():
            return str(local_path.relative_to(local_dir))

        # ä¿®å¤URLæ ¼å¼
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')

        try:
            # åˆ›å»ºç›®å½•
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨requestsåŒæ­¥ä¸‹è½½
            media_headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://www.ifixit.com/",
                "Origin": "https://www.ifixit.com",
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "same-site",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }
            proxies = self._get_next_proxy() if self.use_proxy else None
            response = requests.get(url, headers=media_headers, timeout=8, proxies=proxies, verify=False)
            response.raise_for_status()

            # ä¿å­˜æ–‡ä»¶
            with open(local_path, 'wb') as f:
                f.write(response.content)

            self.stats["media_downloaded"] += 1
            return str(local_path.relative_to(local_dir))

        except Exception as e:
            self.logger.error(f"åŒæ­¥åª’ä½“ä¸‹è½½å¤±è´¥ {url}: {e}")
            self.stats["media_failed"] += 1
            return url

    def _process_media_urls_sync_fallback(self, data, local_dir):
        """åŒæ­¥å¤„ç†åª’ä½“URLçš„å›é€€æ–¹æ¡ˆ"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                    # ä½¿ç”¨åŒæ­¥ä¸‹è½½ä½œä¸ºå›é€€
                    data[key] = self._download_media_file_sync(value, local_dir)
                elif key == 'images' and isinstance(value, list):
                    for i, img_item in enumerate(value):
                        if isinstance(img_item, str) and img_item.startswith('http'):
                            value[i] = self._download_media_file_sync(img_item, local_dir)
                        elif isinstance(img_item, dict) and 'url' in img_item:
                            img_url = img_item['url']
                            if isinstance(img_url, str) and img_url.startswith('http'):
                                img_item['url'] = self._download_media_file_sync(img_url, local_dir)
                elif key == 'videos' and isinstance(value, list):
                    for i, video_item in enumerate(value):
                        if isinstance(video_item, str) and video_item.startswith('http'):
                            if self.download_videos:
                                value[i] = self._download_media_file_sync(video_item, local_dir)
                        elif isinstance(video_item, dict) and 'url' in video_item:
                            video_url = video_item['url']
                            if isinstance(video_url, str) and video_url.startswith('http'):
                                if self.download_videos:
                                    video_item['url'] = self._download_media_file_sync(video_url, local_dir)
                elif isinstance(value, (dict, list)):
                    self._process_media_urls_sync_fallback(value, local_dir)
        elif isinstance(data, list):
            for item in data:
                self._process_media_urls_sync_fallback(item, local_dir)

    def _tree_crawler_get_soup(self, url):
        """ä¸ºtree_crawleræä¾›çš„get_soupæ–¹æ³•ï¼Œåœ¨éœ€è¦é¢åŒ…å±‘å¯¼èˆªæ—¶ä½¿ç”¨Playwright"""
        # å¯¹äºéœ€è¦æå–é¢åŒ…å±‘å¯¼èˆªçš„é¡µé¢ï¼Œä½¿ç”¨Playwright
        if '/Teardown/' in url or '/Guide/' in url or '/Device/' in url:
            return self.get_soup(url, use_playwright=True)
        else:
            return self.get_soup(url, use_playwright=False)

    def extract_real_name_from_url(self, url):
        """
        ä»URLä¸­æå–çœŸå®çš„nameå­—æ®µï¼ˆURLè·¯å¾„çš„æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰
        """
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_url = url.split('?')[0].split('#')[0]

        # ç§»é™¤æœ«å°¾çš„æ–œæ 
        clean_url = clean_url.rstrip('/')

        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µ
        path_parts = clean_url.split('/')
        if path_parts:
            name = path_parts[-1]
            # URLè§£ç 
            import urllib.parse
            name = urllib.parse.unquote(name)
            # ä¿®å¤å¼•å·ç¼–ç é—®é¢˜ï¼šå°† \" è½¬æ¢ä¸º "
            name = name.replace('\\"', '"')
            return name

        return ""

    def extract_real_title_from_page(self, soup):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ
        """
        if not soup:
            return ""

        # ä¼˜å…ˆä»h1æ ‡ç­¾æå–
        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        # å…¶æ¬¡ä»titleæ ‡ç­¾æå–
        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""



    # ç¬¬ä¸€ä¸ªextract_real_view_statisticsæ–¹æ³•å·²ç§»é™¤ï¼Œä½¿ç”¨ä¸‹é¢çš„å®ç°

    def _is_specified_target_url(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        """
        if not self.target_url or not url:
            return False

        # æ ‡å‡†åŒ–URLè¿›è¡Œæ¯”è¾ƒ
        def normalize_url(u):
            import urllib.parse
            # URLè§£ç ï¼Œç„¶åæ ‡å‡†åŒ–
            decoded = urllib.parse.unquote(u)
            return decoded.rstrip('/').lower()

        return normalize_url(url) == normalize_url(self.target_url)

    def discover_subcategories_from_page(self, soup, base_url):
        """
        åŠ¨æ€å‘ç°é¡µé¢ä¸­çš„å­ç±»åˆ«é“¾æ¥
        """
        if not soup:
            return []

        subcategories = []

        # æŸ¥æ‰¾è®¾å¤‡ç±»åˆ«é“¾æ¥çš„å¤šç§é€‰æ‹©å™¨
        category_selectors = [
            'a[href*="/Device/"]',  # åŒ…å«/Device/çš„é“¾æ¥
            '.category-link',       # ç±»åˆ«é“¾æ¥ç±»
            '.device-link',         # è®¾å¤‡é“¾æ¥ç±»
            '.subcategory',         # å­ç±»åˆ«ç±»
            'a[href^="/Device/"]'   # ä»¥/Device/å¼€å¤´çš„é“¾æ¥
        ]

        found_links = set()

        for selector in category_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/Device/' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
                    skip_patterns = [
                        '/Guide/', '/Troubleshooting/', '/Edit/', '/History/',
                        '/Answers/', '?revision', 'action=edit', 'action=create'
                    ]

                    if not any(pattern in full_url for pattern in skip_patterns):
                        # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå­ç±»åˆ«ï¼ˆURLæ¯”å½“å‰é¡µé¢æ›´æ·±ä¸€å±‚ï¼‰
                        if full_url != base_url and full_url not in found_links:
                            # æå–é“¾æ¥æ–‡æœ¬ä½œä¸ºåç§°
                            link_text = link.get_text().strip()
                            if link_text:
                                subcategories.append({
                                    'name': self.extract_real_name_from_url(full_url),
                                    'url': full_url,
                                    'title': link_text
                                })
                                found_links.add(full_url)

        return subcategories

    def extract_real_view_statistics(self, soup, url):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®
        """
        if not soup:
            return {}

        stats = {}

        # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®çš„å„ç§å¯èƒ½ä½ç½®
        stat_selectors = [
            '.view-count',
            '.stats-item',
            '.view-stats',
            '[data-stats]',
            '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # è§£æç»Ÿè®¡æ•°æ®æ ¼å¼
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»APIæˆ–å…¶ä»–ä½ç½®è·å–
        if not stats:
            # å°è¯•ä»é¡µé¢çš„JavaScriptæ•°æ®ä¸­æå–
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        # æŸ¥æ‰¾å¯èƒ½çš„ç»Ÿè®¡æ•°æ®
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def build_correct_url_from_path(self, base_url, path_segments):
        """
        æ ¹æ®è·¯å¾„æ®µæ­£ç¡®æ„å»ºURLï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
        """
        if not path_segments:
            return base_url

        # ä»base_urlå¼€å§‹ï¼Œé€æ­¥æ›¿æ¢è·¯å¾„æ®µ
        current_url = base_url

        for segment in path_segments:
            # è·å–å½“å‰URLçš„è·¯å¾„éƒ¨åˆ†
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(current_url)
            path_parts = [p for p in str(parsed.path).split('/') if p]

            # æ·»åŠ æ–°çš„è·¯å¾„æ®µ
            path_parts.append(str(segment))

            # é‡æ–°æ„å»ºURL
            new_path = '/' + '/'.join(path_parts)
            new_parsed = parsed._replace(path=new_path)
            current_url = urlunparse(new_parsed)

        return current_url

    def restructure_target_content(self, node, is_target_page=False):
        """
        é‡æ„ç›®æ ‡æ–‡ä»¶çš„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨guides[]å’Œtroubleshooting[]è€Œéchildren[]
        """
        if not node or not isinstance(node, dict):
            return node

        # å¦‚æœè¿™æ˜¯ç›®æ ‡é¡µé¢ä¸”æœ‰childrenåŒ…å«guideså’Œtroubleshooting
        if is_target_page and 'children' in node and node['children']:
            guides = []
            troubleshooting = []
            real_children = []

            for child in node['children']:
                if 'steps' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæŒ‡å—
                    guide_data = {k: v for k, v in child.items() if k not in ['children']}
                    guides.append(guide_data)
                elif 'causes' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæ•…éšœæ’é™¤
                    ts_data = {k: v for k, v in child.items() if k not in ['children']}
                    troubleshooting.append(ts_data)
                else:
                    # è¿™æ˜¯çœŸæ­£çš„å­ç±»åˆ«
                    real_children.append(child)

            # é‡æ„èŠ‚ç‚¹
            if guides:
                node['guides'] = guides
            if troubleshooting:
                node['troubleshooting'] = troubleshooting

            # åªæœ‰çœŸæ­£çš„å­ç±»åˆ«æ‰ä¿ç•™åœ¨childrenä¸­
            if real_children:
                node['children'] = real_children
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„å­ç±»åˆ«ï¼Œç§»é™¤childrenå­—æ®µ
                if 'children' in node:
                    del node['children']

        return node

    def fix_node_data(self, node, soup=None):
        """ä¿®å¤èŠ‚ç‚¹çš„nameå­—æ®µï¼Œåªåœ¨æœ€ç»ˆäº§å“é¡µé¢æ·»åŠ å…¶ä»–å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_root_device = url == f"{self.base_url}/Device"

        # æ‰€æœ‰èŠ‚ç‚¹éƒ½ä¿®å¤nameå­—æ®µ
        real_name = self.extract_real_name_from_url(url)
        if real_name:
            node['name'] = real_name

        # åªæœ‰åœ¨deep_crawl_product_contentä¸­è¢«è¯†åˆ«ä¸ºç›®æ ‡é¡µé¢æ—¶æ‰æ·»åŠ å…¶ä»–å­—æ®µ
        # è¿™é‡Œä¸æ·»åŠ titleã€instruction_urlã€view_statisticsç­‰å­—æ®µ
        # è¿™äº›å­—æ®µå°†åœ¨deep_crawl_product_contentæ–¹æ³•ä¸­æ·»åŠ 

        return node

    def extract_what_you_need_enhanced(self, guide_url):
        """
        å¢å¼ºç‰ˆ"What You Need"éƒ¨åˆ†æå–æ–¹æ³•ï¼Œç¡®ä¿çœŸå®é¡µé¢è®¿é—®å’Œæ•°æ®å®Œæ•´æ€§
        """
        if not guide_url:
            return {}

        print(f"    å¢å¼ºæå–What You Need: {guide_url}")

        try:
            # ä½¿ç”¨Playwrightè·å–å®Œæ•´æ¸²æŸ“çš„é¡µé¢
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´ç¡®ä¿è‹±æ–‡å†…å®¹
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…å®Œå…¨åŠ è½½
                page.goto(guide_url, wait_until='networkidle')
                page.wait_for_timeout(5000)  # å¢åŠ ç­‰å¾…æ—¶é—´ç¡®ä¿å®Œå…¨åŠ è½½

                # è·å–é¡µé¢HTML
                html_content = page.content()
                browser.close()

                # è§£æHTML
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')

                # å°è¯•å¤šç§æ–¹æ³•æå–"What You Need"æ•°æ®
                what_you_need = {}

                # æ–¹æ³•1ï¼šä»Reactç»„ä»¶çš„data-propsä¸­æå–ï¼ˆæœ€å‡†ç¡®ï¼‰
                what_you_need = self._extract_from_react_props_enhanced(soup)

                # æ–¹æ³•2ï¼šå¦‚æœReactæ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢çš„What You NeedåŒºåŸŸæå–
                if not what_you_need:
                    what_you_need = self._extract_from_what_you_need_section(soup)

                # æ–¹æ³•3ï¼šå¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢çš„äº§å“é“¾æ¥æå–
                if not what_you_need:
                    what_you_need = self._extract_from_product_links(soup)

                # æ–¹æ³•4ï¼šæœ€åå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
                if not what_you_need:
                    what_you_need = self._extract_from_page_text(soup)

                if what_you_need:
                    print(f"    æˆåŠŸæå–åˆ°: {list(what_you_need.keys())}")
                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    total_items = sum(len(items) if isinstance(items, list) else 1
                                    for items in what_you_need.values())
                    print(f"    æ€»è®¡é¡¹ç›®æ•°: {total_items}")
                else:
                    print(f"    æœªæ‰¾åˆ°What You Needæ•°æ®")

                return what_you_need

        except Exception as e:
            print(f"    å¢å¼ºæå–å¤±è´¥: {str(e)}")
            return {}

    def _extract_from_react_props_enhanced(self, soup):
        """ä»Reactç»„ä»¶çš„data-propsä¸­æå–What you needæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾å¤šç§å¯èƒ½çš„Reactç»„ä»¶
            component_selectors = [
                'div[data-name="GuideTopComponent"]',
                'div[data-name="GuideComponent"]',
                'div[data-name="WhatYouNeedComponent"]',
                'div[data-props*="tools"]',
                'div[data-props*="parts"]',
                'div[data-props*="kits"]',
                'div[data-props*="materials"]'
            ]

            for selector in component_selectors:
                component = soup.select_one(selector)
                if component:
                    data_props = component.get('data-props')
                    if data_props:
                        try:
                            import json
                            import html

                            # HTMLè§£ç å¹¶è§£æJSON
                            decoded_props = html.unescape(data_props)
                            props_data = json.loads(decoded_props)

                            # æå–productData
                            product_data = props_data.get('productData', {})

                            # æå–Fix Kits/Parts
                            kits = product_data.get('kits', [])
                            if kits:
                                fix_kits = []
                                parts = []
                                materials = []
                                for kit in kits:
                                    name = kit.get('name', '').strip()
                                    if name:
                                        name = self.clean_product_name(name)
                                        if name:
                                            if 'kit' in name.lower() or 'fix kit' in name.lower():
                                                fix_kits.append(name)
                                            elif 'material' in name.lower():
                                                materials.append(name)
                                            else:
                                                parts.append(name)

                                if fix_kits:
                                    what_you_need['Fix Kits'] = fix_kits
                                if parts:
                                    what_you_need['Parts'] = parts
                                if materials:
                                    what_you_need['Materials'] = materials

                            # æå–Tools
                            tools = product_data.get('tools', [])
                            if tools:
                                tool_names = []
                                for tool in tools:
                                    name = tool.get('name', '').strip()
                                    if name:
                                        name = self.clean_product_name(name)
                                        if name:
                                            tool_names.append(name)

                                if tool_names:
                                    what_you_need['Tools'] = tool_names

                            # æå–å…¶ä»–å¯èƒ½çš„ç±»åˆ«
                            for category_key in ['supplies', 'components', 'accessories']:
                                category_items = product_data.get(category_key, [])
                                if category_items:
                                    category_names = []
                                    for item in category_items:
                                        name = item.get('name', '').strip()
                                        if name:
                                            name = self.clean_product_name(name)
                                            if name:
                                                category_names.append(name)

                                    if category_names:
                                        category_title = category_key.title()
                                        what_you_need[category_title] = category_names

                            if what_you_need:
                                break

                        except Exception as e:
                            continue

            return what_you_need

        except Exception as e:
            return what_you_need

    def _extract_from_what_you_need_section(self, soup):
        """ä»é¡µé¢çš„What You Needä¸“é—¨åŒºåŸŸæå–æ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾What You Needæ ‡é¢˜
            what_you_need_headers = soup.find_all(string=lambda text: text and
                'what you need' in text.lower())

            for header in what_you_need_headers:
                parent = header.parent
                if parent:
                    # æŸ¥æ‰¾çˆ¶çº§å®¹å™¨
                    container = parent.find_parent(['div', 'section'])
                    if container:
                        # æŸ¥æ‰¾å·¥å…·å’Œé›¶ä»¶åˆ—è¡¨
                        lists = container.find_all(['ul', 'ol'])
                        for list_elem in lists:
                            items = list_elem.find_all('li')
                            if items:
                                category_items = []
                                for item in items:
                                    text = item.get_text().strip()
                                    if text and len(text) > 2:
                                        cleaned_text = self.clean_product_name(text)
                                        if cleaned_text:
                                            category_items.append(cleaned_text)

                                if category_items:
                                    # æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ç±»åˆ«
                                    context = container.get_text().lower()
                                    if 'tool' in context:
                                        what_you_need['Tools'] = category_items
                                    elif 'part' in context:
                                        what_you_need['Parts'] = category_items
                                    elif 'kit' in context:
                                        what_you_need['Fix Kits'] = category_items
                                    else:
                                        what_you_need['Items'] = category_items
                                    break

                        if what_you_need:
                            break

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_product_links(self, soup):
        """ä»é¡µé¢çš„äº§å“é“¾æ¥ä¸­æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾äº§å“é“¾æ¥
            product_links = soup.find_all('a', href=lambda x: x and
                any(domain in x for domain in ['ifixit.com/products', 'ifixit.com/store']))

            tools = []
            parts = []
            fix_kits = []

            for link in product_links:
                text = link.get_text().strip()
                if text and len(text) > 2:
                    cleaned_text = self.clean_product_name(text)
                    if cleaned_text:
                        text_lower = cleaned_text.lower()
                        if any(tool_word in text_lower for tool_word in
                               ['screwdriver', 'spudger', 'pick', 'driver', 'tool']):
                            tools.append(cleaned_text)
                        elif any(kit_word in text_lower for kit_word in
                                ['kit', 'set']):
                            fix_kits.append(cleaned_text)
                        else:
                            parts.append(cleaned_text)

            if tools:
                what_you_need['Tools'] = list(dict.fromkeys(tools))  # å»é‡
            if parts:
                what_you_need['Parts'] = list(dict.fromkeys(parts))
            if fix_kits:
                what_you_need['Fix Kits'] = list(dict.fromkeys(fix_kits))

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_html_structure(self, soup):
        """ä»HTMLç»“æ„ä¸­ç›´æ¥æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾"What You Need"æ ‡é¢˜
            what_you_need_headers = soup.find_all(string=lambda text: text and
                any(phrase in text.lower() for phrase in ['what you need', 'tools', 'parts', 'materials']))

            for header in what_you_need_headers:
                parent = header.parent
                if parent:
                    # æŸ¥æ‰¾åç»­çš„åˆ—è¡¨æˆ–å†…å®¹
                    next_elements = parent.find_next_siblings(['ul', 'ol', 'div', 'section'])
                    for elem in next_elements:
                        # æå–åˆ—è¡¨é¡¹
                        items = elem.find_all(['li', 'a'])
                        if items:
                            category_items = []
                            for item in items:
                                text = item.get_text().strip()
                                if text and len(text) > 2:
                                    text = self.clean_product_name(text)
                                    if text:
                                        category_items.append(text)

                            if category_items:
                                # æ ¹æ®å†…å®¹åˆ¤æ–­ç±»åˆ«
                                header_text = header.lower()
                                if 'tool' in header_text:
                                    what_you_need['Tools'] = category_items
                                elif 'part' in header_text:
                                    what_you_need['Parts'] = category_items
                                elif 'kit' in header_text:
                                    what_you_need['Fix Kits'] = category_items
                                else:
                                    what_you_need['Items'] = category_items
                                break

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_page_text(self, soup):
        """ä»é¡µé¢æ–‡æœ¬ä¸­æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾åŒ…å«å·¥å…·å’Œé›¶ä»¶ä¿¡æ¯çš„æ–‡æœ¬
            text_content = soup.get_text()
            lines = text_content.split('\n')

            current_category = None
            items = []

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»åˆ«æ ‡é¢˜
                line_lower = line.lower()
                if any(keyword in line_lower for keyword in ['tools:', 'parts:', 'materials:', 'what you need:']):
                    if current_category and items:
                        what_you_need[current_category] = items

                    if 'tool' in line_lower:
                        current_category = 'Tools'
                    elif 'part' in line_lower:
                        current_category = 'Parts'
                    elif 'material' in line_lower:
                        current_category = 'Materials'
                    else:
                        current_category = 'Items'

                    items = []

                # æ£€æŸ¥æ˜¯å¦æ˜¯é¡¹ç›®
                elif current_category and len(line) > 2 and len(line) < 100:
                    cleaned_item = self.clean_product_name(line)
                    if cleaned_item:
                        items.append(cleaned_item)

            # æ·»åŠ æœ€åä¸€ä¸ªç±»åˆ«
            if current_category and items:
                what_you_need[current_category] = items

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_troubleshooting_images_from_section(self, section):
        """ä»troubleshooting sectionä¸­æå–å›¾ç‰‡ï¼Œä½¿ç”¨enhanced_crawlerçš„è¿‡æ»¤é€»è¾‘å¹¶ä¸‹è½½åˆ°æœ¬åœ°"""
        images = []
        seen_urls = set()

        try:
            if not section:
                if self.verbose:
                    print("âŒ Sectionä¸ºç©º")
                return images

            # æŸ¥æ‰¾è¯¥sectionå†…çš„æ‰€æœ‰å›¾ç‰‡
            img_elements = section.find_all('img')
            if self.verbose:
                print(f"ğŸ“· æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")

            for img in img_elements:
                try:
                    # æ£€æŸ¥æ˜¯å¦åœ¨å•†ä¸šæ¨å¹¿åŒºåŸŸå†…
                    if self._is_element_in_promotional_area(img):
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ¨å¹¿åŒºåŸŸå›¾ç‰‡")
                        continue

                    # é¢å¤–æ£€æŸ¥ï¼šæ£€æŸ¥å›¾ç‰‡çš„altæ–‡æœ¬æ˜¯å¦åŒ…å«æ¨èå†…å®¹ç‰¹å¾
                    alt_text = img.get('alt', '').strip().lower()
                    if self._is_recommendation_image_alt(alt_text):
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ¨èå†…å®¹å›¾ç‰‡: {alt_text}")
                        continue

                    img_src = img.get('src') or img.get('data-src')
                    if img_src and isinstance(img_src, str):
                        # æ„å»ºå®Œæ•´URL
                        if img_src.startswith("//"):
                            img_src = "https:" + img_src
                        elif img_src.startswith("/"):
                            img_src = self.base_url + img_src

                        # ä½¿ç”¨enhanced_crawlerçš„è¿‡æ»¤é€»è¾‘
                        if self._is_valid_troubleshooting_image_enhanced(img_src, img):
                            # ç¡®ä¿ä½¿ç”¨mediumå°ºå¯¸
                            if 'guide-images.cdn.ifixit.com' in img_src and not img_src.endswith('.medium'):
                                # ç§»é™¤ç°æœ‰çš„å°ºå¯¸åç¼€å¹¶æ·»åŠ .medium
                                if '.standard' in img_src or '.large' in img_src or '.small' in img_src:
                                    img_src = img_src.replace('.standard', '.medium').replace('.large', '.medium').replace('.small', '.medium')
                                elif not any(suffix in img_src for suffix in ['.medium', '.jpg', '.png', '.gif']):
                                    img_src += '.medium'

                            # å»é‡å¹¶æ·»åŠ 
                            if img_src not in seen_urls:
                                seen_urls.add(img_src)

                                # è·å–å›¾ç‰‡æè¿°ï¼ˆä½¿ç”¨åŸå§‹altæ–‡æœ¬ï¼Œä¸æ˜¯å°å†™ç‰ˆæœ¬ï¼‰
                                alt_text = img.get('alt', '').strip()
                                title_text = img.get('title', '').strip()
                                description = alt_text or title_text or "Block Image"

                                # æ£€æŸ¥æè¿°æ˜¯å¦æ˜¯å•†ä¸šå†…å®¹
                                if not self._is_commercial_text(description):
                                    # åœ¨troubleshootingæå–é˜¶æ®µï¼Œå…ˆä¿å­˜åŸå§‹URL
                                    # å›¾ç‰‡ä¸‹è½½å°†åœ¨ä¿å­˜é˜¶æ®µè¿›è¡Œï¼Œç¡®ä¿ä¸‹è½½åˆ°æ­£ç¡®çš„ç›®å½•
                                    images.append({
                                        "url": img_src,  # ä¿å­˜åŸå§‹URLï¼Œç¨ååœ¨ä¿å­˜æ—¶ä¸‹è½½
                                        "description": description
                                    })

                                    if self.verbose:
                                        print(f"âœ… æ·»åŠ å›¾ç‰‡: {description}")
                                else:
                                    if self.verbose:
                                        print(f"â­ï¸ è·³è¿‡å•†ä¸šæè¿°: {description}")
                            else:
                                if self.verbose:
                                    print(f"â­ï¸ è·³è¿‡é‡å¤URL: {img_src}")
                        else:
                            if self.verbose:
                                print(f"â­ï¸ è·³è¿‡æ— æ•ˆURL: {img_src}")
                    else:
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ— æ•ˆsrc")
                except Exception as e:
                    if self.verbose:
                        print(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue

            if self.verbose:
                print(f"ğŸ“· æœ€ç»ˆæå–åˆ° {len(images)} ä¸ªå›¾ç‰‡")
            return images

        except Exception as e:
            if self.verbose:
                print(f"âŒ æå–å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return images

    def _remove_promotional_content_from_section(self, section):
        """ç§»é™¤sectionä¸­çš„æ¨å¹¿å†…å®¹"""
        try:
            # ç§»é™¤æ¨å¹¿ç›¸å…³çš„å…ƒç´ 
            promotional_selectors = [
                '.promo', '.promotion', '.ad', '.advertisement',
                '.product-card', '.shop-card', '.buy-now',
                '[class*="promo"]', '[class*="shop"]', '[class*="buy"]'
            ]

            for selector in promotional_selectors:
                elements = section.select(selector)
                for elem in elements:
                    elem.decompose()

        except Exception:
            pass

    def _is_valid_troubleshooting_image_enhanced(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„troubleshootingå›¾ç‰‡ - ä½¿ç”¨enhanced_crawlerçš„é€»è¾‘"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # è¿‡æ»¤æ‰å°å›¾æ ‡ã€logoå’Œå•†ä¸šå›¾ç‰‡
            skip_keywords = [
                'icon', 'logo', 'flag', 'avatar', 'ad', 'banner',
                'promo', 'cart', 'buy', 'purchase', 'shop', 'header',
                'footer', 'nav', 'menu', 'sidebar'
            ]
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            # å¦‚æœæœ‰img_elemï¼Œæ£€æŸ¥altå±æ€§
            if img_elem:
                alt_text = img_elem.get('alt', '').lower()
                title_text = img_elem.get('title', '').lower()

                # æ’é™¤å•†ä¸šæè¿°
                commercial_keywords = [
                    'buy', 'purchase', 'cart', 'shop', 'price', 'review',
                    'advertisement', 'sponsor'
                ]
                if any(keyword in alt_text or keyword in title_text for keyword in commercial_keywords):
                    return False

                # è¿‡æ»¤è£…é¥°æ€§å›¾ç‰‡
                decorative_alt_keywords = [
                    'decoration', 'ornament', 'divider', 'spacer',
                    'bullet', 'arrow', 'pointer', 'separator'
                ]
                if any(keyword in alt_text for keyword in decorative_alt_keywords):
                    return False

            return True

        except Exception:
            return False



    def _is_valid_troubleshooting_image(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„troubleshootingå›¾ç‰‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # åªå…è®¸æ¥è‡ªguide-images.cdn.ifixit.comçš„å›¾ç‰‡
            if 'guide-images.cdn.ifixit.com' not in img_src_lower:
                return False

            # è¿‡æ»¤æ‰æ˜æ˜¾çš„è£…é¥°æ€§å›¾ç‰‡å’Œå›¾æ ‡
            skip_keywords = [
                'icon', 'logo', 'avatar', 'badge', 'star', 'rating',
                'promo', 'ad', 'banner', 'shop', 'buy', 'cart'
            ]
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            # å¦‚æœæœ‰img_elemï¼Œæ£€æŸ¥altå±æ€§
            if img_elem:
                alt_text = img_elem.get('alt', '').lower()
                if alt_text:
                    decorative_alt_keywords = [
                        'decoration', 'ornament', 'divider', 'spacer',
                        'bullet', 'arrow', 'pointer', 'separator'
                    ]
                    if any(keyword in alt_text for keyword in decorative_alt_keywords):
                        return False

            return True

        except Exception:
            return False

        except Exception:
            return False

    def _is_element_in_promotional_area(self, element):
        """æ£€æŸ¥å…ƒç´ æ˜¯å¦åœ¨æ¨å¹¿åŒºåŸŸå†… - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´ç²¾ç¡®åœ°è¯†åˆ«æ¨å¹¿å®¹å™¨"""
        try:
            # å‘ä¸Šéå†DOMæ ‘æ£€æŸ¥æ˜¯å¦åœ¨æ¨å¹¿å®¹å™¨ä¸­
            current = element
            level = 0
            while current and current.name and level < 5:  # é™åˆ¶éå†å±‚æ•°
                # ä¼˜å…ˆé€šè¿‡classåç§°è¯†åˆ«æ¨å¹¿å®¹å™¨
                classes = current.get('class', [])
                if isinstance(classes, list):
                    class_str = ' '.join(classes).lower()
                else:
                    class_str = str(classes).lower()

                # æ˜ç¡®çš„æ¨å¹¿å®¹å™¨classåç§°
                promo_class_keywords = [
                    'guide-recommendation', 'guide-promo', 'view-guide',
                    'product-box', 'product-purchase', 'buy-box', 'purchase-box',
                    'product-card', 'shop-card', 'buy-now',
                    'parts-finder', 'find-parts'
                ]

                # å¦‚æœæœ‰æ˜ç¡®çš„æ¨å¹¿classï¼Œç›´æ¥è¿”å›True
                if any(keyword in class_str for keyword in promo_class_keywords):
                    return True

                # å¯¹äºæ²¡æœ‰æ˜ç¡®classçš„å…ƒç´ ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ–‡æœ¬æ£€æµ‹
                # åªæœ‰å½“å…ƒç´ ç›¸å¯¹è¾ƒå°ä¸”æ˜ç¡®åŒ…å«æ¨å¹¿å†…å®¹æ—¶æ‰è®¤ä¸ºæ˜¯æ¨å¹¿å®¹å™¨
                if (level <= 2 and  # åªæ£€æŸ¥å‰3å±‚
                    self._is_specific_promotional_container(current)):
                    return True

                current = current.parent
                level += 1
            return False
        except Exception:
            return False

    def _is_specific_promotional_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šçš„æ¨å¹¿å®¹å™¨ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æµ‹ï¼‰"""
        try:
            text = element.get_text().lower()

            # æ–‡æœ¬å¤ªé•¿çš„ä¸å¤ªå¯èƒ½æ˜¯æ¨å¹¿å®¹å™¨
            if len(text) > 300:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"View Guide"æŒ‰é’®
            has_view_guide = 'view guide' in text

            # æ£€æŸ¥æ—¶é—´æ ¼å¼æ¨¡å¼
            import re
            time_pattern = r'\d+\s*(minute|hour)s?(\s*-\s*\d+\s*(minute|hour)s?)?'
            has_time_pattern = bool(re.search(time_pattern, text))

            # æ£€æŸ¥éš¾åº¦ç­‰çº§
            difficulty_levels = ['very easy', 'easy', 'moderate', 'difficult', 'very difficult']
            has_difficulty = any(level in text for level in difficulty_levels)

            # æ£€æŸ¥æŒ‡å—ç›¸å…³ç‰¹å¾
            guide_features = ['how to', 'guide', 'tutorial', 'install', 'replace', 'repair', 'fix']
            has_guide_features = any(feature in text for feature in guide_features)

            # æ£€æŸ¥è´­ä¹°ç›¸å…³ç‰¹å¾
            purchase_features = ['buy', 'purchase', '$', 'â‚¬', 'Â£', 'Â¥', 'add to cart', 'shop']
            has_purchase = any(feature in text for feature in purchase_features)

            # æ¨å¹¿å®¹å™¨çš„åˆ¤æ–­æ¡ä»¶ï¼š
            # 1. åŒ…å«è´­ä¹°ç›¸å…³å†…å®¹ OR
            # 2. åŒ…å«"View Guide"æŒ‰é’® OR
            # 3. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’Œéš¾åº¦ç­‰çº§ OR
            # 4. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’ŒæŒ‡å—ç‰¹å¾
            return (
                has_purchase or
                has_view_guide or
                (has_time_pattern and has_difficulty) or
                (has_time_pattern and has_guide_features)
            )
        except Exception:
            return False

    def _is_recommendation_image_alt(self, alt_text):
        """æ£€æŸ¥å›¾ç‰‡çš„altæ–‡æœ¬æ˜¯å¦åŒ…å«æ¨èå†…å®¹ç‰¹å¾"""
        try:
            if not alt_text:
                return False

            alt_text_lower = alt_text.lower()

            # ç²¾ç¡®çš„æ¨èæŒ‡å—æ ‡é¢˜æ¨¡å¼ - æ›´å…¨é¢çš„æ¨¡å¼åŒ¹é…
            guide_title_patterns = [
                # ç‰¹å®šæ¨¡å¼
                r'how to boot.*into safe mode',
                r'how to use internet recovery',
                r'how to recover data from',
                r'how to install.*to.*ssd',
                r'how to replace.*battery',
                r'how to repair.*screen',
                r'how to fix.*display',
                r'how to troubleshoot',
                r'how to run.*with disk utility',
                r'how to start up.*in.*recovery mode',
                r'how to create a bootable',
                # é€šç”¨æ¨¡å¼
                r'how to .*removal',
                r'how to .*replace',
                r'how to .*install',
                r'how to .*repair',
                r'how to .*upgrade',
                r'how to .*fix',
                r'replacement.*guide',
                r'repair.*guide',
                r'installation.*guide',
                r'removal.*guide',
                # ç‰¹å®šè®¾å¤‡æ¨¡å¼
                r'macbook pro.*unibody.*removal',
                r'macbook pro.*key.*removal',
                r'macbook air.*display',
                r'macbook.*retina.*replacement'
            ]

            # æ£€æŸ¥ç²¾ç¡®çš„æ¨èæŒ‡å—æ ‡é¢˜æ¨¡å¼
            import re
            for pattern in guide_title_patterns:
                if re.search(pattern, alt_text_lower):
                    return True

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å—æ¨èçš„å…³é”®ç‰¹å¾
            recommendation_keywords = [
                'how to use', 'how to install', 'how to replace', 'how to repair',
                'guide', 'tutorial', 'installation', 'replacement', 'repair guide'
            ]

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨èç›¸å…³çš„è¯æ±‡
            has_recommendation_keywords = any(keyword in alt_text_lower for keyword in recommendation_keywords)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è®¾å¤‡/äº§å“åç§°ï¼ˆé€šå¸¸æ¨èå†…å®¹ä¼šåŒ…å«å…·ä½“çš„è®¾å¤‡åï¼‰
            device_keywords = ['macbook', 'iphone', 'ipad', 'ssd', 'battery', 'screen', 'display']
            has_device_keywords = any(keyword in alt_text_lower for keyword in device_keywords)

            # å¦‚æœåŒæ—¶åŒ…å«æ¨èå…³é”®è¯å’Œè®¾å¤‡å…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯æ¨èå†…å®¹
            return has_recommendation_keywords and has_device_keywords

        except Exception:
            return False

    def _is_guide_recommendation_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡å—æ¨èå®¹å™¨ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´ç²¾ç¡®åœ°è¯†åˆ«æ¨èæ¡†"""
        try:
            # åªæ£€æŸ¥å½“å‰å…ƒç´ çš„ç›´æ¥æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…æ‹¬æ·±å±‚å­å…ƒç´ 
            # è¿™æ ·å¯ä»¥é¿å…æ•´ä¸ªæ–‡æ¡£è¢«è¯¯åˆ¤ä¸ºæ¨å¹¿å®¹å™¨
            if element.name in ['html', '[document]', 'body']:
                return False

            # æ£€æŸ¥classå±æ€§ï¼Œä¼˜å…ˆé€šè¿‡classè¯†åˆ«
            classes = element.get('class', [])
            if isinstance(classes, list):
                class_str = ' '.join(classes).lower()
            else:
                class_str = str(classes).lower()

            # é€šè¿‡classåç§°å¿«é€Ÿè¯†åˆ«
            guide_class_keywords = [
                'guide-recommendation', 'guide-promo', 'view-guide',
                'recommendation', 'promo-guide', 'guide-card', 'related-guide'
            ]
            if any(keyword in class_str for keyword in guide_class_keywords):
                return True

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯æ¨èæ¡†ï¼ˆé™¤éæœ‰æ˜ç¡®çš„classæ ‡è¯†ï¼‰
            if len(text) > 500:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"View Guide"æŒ‰é’®æ–‡æœ¬
            has_view_guide = 'view guide' in text

            # æ£€æŸ¥æ—¶é—´æ ¼å¼æ¨¡å¼ (å¦‚ "30 minutes - 1 hour", "5 minutes", "2 hours")
            import re
            time_pattern = r'\d+\s*(minute|hour)s?(\s*-\s*\d+\s*(minute|hour)s?)?'
            has_time_pattern = bool(re.search(time_pattern, text))

            # æ£€æŸ¥éš¾åº¦ç­‰çº§
            difficulty_levels = ['very easy', 'easy', 'moderate', 'difficult', 'very difficult']
            has_difficulty = any(level in text for level in difficulty_levels)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å—ç›¸å…³çš„å…³é”®è¯
            guide_keywords = ['how to', 'guide', 'tutorial', 'install', 'replace', 'repair', 'fix']
            has_guide_keywords = any(keyword in text for keyword in guide_keywords)

            # æ¨èæ¡†çš„ç‰¹å¾ï¼š
            # 1. åŒ…å«"View Guide"æŒ‰é’® OR
            # 2. åŒæ—¶åŒ…å«æ—¶é—´ä¿¡æ¯å’Œéš¾åº¦ç­‰çº§ OR
            # 3. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’ŒæŒ‡å—å…³é”®è¯
            is_recommendation = (
                has_view_guide or
                (has_time_pattern and has_difficulty) or
                (has_time_pattern and has_guide_keywords)
            )

            # æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼šæ¨èæ¡†é€šå¸¸æ¯”è¾ƒç®€æ´
            return is_recommendation and len(text) < 400

        except Exception:
            return False

    def _is_product_purchase_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“è´­ä¹°å®¹å™¨ - ä»enhanced_crawlerç§»æ¤ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·³è¿‡æ–‡æ¡£çº§åˆ«çš„å…ƒç´ 
            if element.name in ['html', '[document]', 'body']:
                return False

            # æ£€æŸ¥classå±æ€§ï¼Œä¼˜å…ˆé€šè¿‡classè¯†åˆ«
            classes = element.get('class', [])
            if isinstance(classes, list):
                class_str = ' '.join(classes).lower()
            else:
                class_str = str(classes).lower()

            # é€šè¿‡classåç§°å¿«é€Ÿè¯†åˆ«
            product_class_keywords = [
                'product-box', 'product-purchase', 'buy-box', 'purchase-box',
                'product-card', 'shop-card', 'buy-now'
            ]
            if any(keyword in class_str for keyword in product_class_keywords):
                return True

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯è´­ä¹°æ¡†
            if len(text) > 400:
                return False

            import re
            has_price = bool(re.search(r'[\$â‚¬Â£Â¥]\s*\d+\.?\d*', text))
            has_buy = 'buy' in text
            has_reviews = bool(re.search(r'\d+\.?\d*\s*(reviews?|stars?)', text))
            # è´­ä¹°æ¡†é€šå¸¸åŒ…å«ä»·æ ¼ã€è´­ä¹°æŒ‰é’®æˆ–è¯„ä»·ä¿¡æ¯
            return (has_price or has_buy or has_reviews) and len(text) < 200
        except Exception:
            return False

    def _is_parts_finder_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯é…ä»¶æŸ¥æ‰¾å®¹å™¨ - ä»enhanced_crawlerç§»æ¤ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·³è¿‡æ–‡æ¡£çº§åˆ«çš„å…ƒç´ 
            if element.name in ['html', '[document]', 'body']:
                return False

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯é…ä»¶æŸ¥æ‰¾æ¡†
            if len(text) > 500:
                return False

            has_parts_text = any(keyword in text for keyword in [
                'find your parts', 'select my model', 'compatible replacement'
            ])
            return has_parts_text and len(text) < 300
        except Exception:
            return False

    def _is_commercial_text(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯å•†ä¸šå†…å®¹ - ä»enhanced_crawlerç§»æ¤"""
        try:
            text_lower = text.lower()
            # å•†ä¸šå†…å®¹å…³é”®è¯
            commercial_keywords = [
                'buy', 'purchase', 'view guide', 'find your parts', 'select my model',
                'add to cart', 'quality guarantee', 'compatible replacement'
            ]

            # æ£€æŸ¥ä»·æ ¼æ¨¡å¼
            import re
            if re.search(r'[\$â‚¬Â£Â¥]\s*\d+\.?\d*', text):
                return True

            # æ£€æŸ¥è¯„åˆ†æ¨¡å¼
            if re.search(r'\d+\.?\d*\s*(reviews?|stars?)', text_lower):
                return True

            # æ£€æŸ¥å•†ä¸šå…³é”®è¯
            keyword_count = sum(1 for keyword in commercial_keywords if keyword in text_lower)
            return keyword_count >= 1

        except Exception:
            return False

    def _is_image_in_commercial_area(self, img_elem):
        """æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åœ¨å•†ä¸šåŒºåŸŸå†…"""
        try:
            # å‘ä¸ŠæŸ¥æ‰¾çˆ¶å…ƒç´ ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å•†ä¸šç›¸å…³çš„å®¹å™¨ä¸­
            current = img_elem
            for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
                if not current or not hasattr(current, 'parent'):
                    break

                current = current.parent
                if not current:
                    break

                # æ£€æŸ¥classå±æ€§
                class_attr = current.get('class', [])
                if isinstance(class_attr, list):
                    class_str = ' '.join(class_attr).lower()
                else:
                    class_str = str(class_attr).lower()

                # å•†ä¸šåŒºåŸŸçš„classå…³é”®è¯
                commercial_keywords = [
                    'shop', 'store', 'buy', 'purchase', 'cart', 'checkout',
                    'product', 'price', 'sale', 'offer', 'deal', 'promo',
                    'advertisement', 'ad-', 'banner', 'sponsor'
                ]

                if any(keyword in class_str for keyword in commercial_keywords):
                    return True

                # æ£€æŸ¥idå±æ€§
                id_attr = current.get('id', '')
                if id_attr and any(keyword in id_attr.lower() for keyword in commercial_keywords):
                    return True

            return False
        except Exception:
            return False

    def _is_valid_guide_image(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„guideå›¾ç‰‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # åªå…è®¸æ¥è‡ªguide-images.cdn.ifixit.comçš„å›¾ç‰‡
            if 'guide-images.cdn.ifixit.com' not in img_src_lower:
                return False

            # è¿‡æ»¤æ‰æ˜æ˜¾çš„è£…é¥°æ€§å›¾ç‰‡
            skip_keywords = ['icon', 'logo', 'avatar', 'badge', 'star', 'rating']
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            return True

        except Exception:
            return False

    def extract_guide_content(self, guide_url):
        """é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨ç®€åŒ–çš„å›¾ç‰‡è¿‡æ»¤é€»è¾‘"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºæœ¬å†…å®¹
        guide_data = super().extract_guide_content(guide_url)

        if not guide_data:
            return None

        # ç®€åŒ–å›¾ç‰‡å¤„ç†ï¼šåªç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„URLæ ¼å¼ï¼Œä¸é‡æ–°æå–
        if 'steps' in guide_data and guide_data['steps']:
            for step in guide_data['steps']:
                if 'images' in step and step['images']:
                    filtered_images = []
                    seen_urls = set()

                    for img_src in step['images']:
                        if isinstance(img_src, str) and img_src:
                            # ç¡®ä¿ä½¿ç”¨å®Œæ•´URL
                            if img_src.startswith("//"):
                                img_src = "https:" + img_src
                            elif img_src.startswith("/"):
                                img_src = self.base_url + img_src

                            # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡
                            if self._is_valid_guide_image(img_src):
                                # ç¡®ä¿ä½¿ç”¨mediumå°ºå¯¸
                                if 'guide-images.cdn.ifixit.com' in img_src and not img_src.endswith('.medium'):
                                    # ç§»é™¤ç°æœ‰çš„å°ºå¯¸åç¼€å¹¶æ·»åŠ .medium
                                    if '.standard' in img_src or '.large' in img_src or '.small' in img_src:
                                        img_src = img_src.replace('.standard', '.medium').replace('.large', '.medium').replace('.small', '.medium')
                                    elif not any(suffix in img_src for suffix in ['.medium', '.jpg', '.png', '.gif']):
                                        img_src += '.medium'

                                # å»é‡
                                if img_src not in seen_urls:
                                    seen_urls.add(img_src)
                                    filtered_images.append(img_src)

                    step['images'] = filtered_images

        return guide_data

    def clean_product_name(self, text):
        """æ¸…ç†äº§å“åç§°ï¼Œç§»é™¤ä»·æ ¼ã€è¯„åˆ†ç­‰æ— å…³ä¿¡æ¯"""
        if not text:
            return ""

        # ç§»é™¤ä»·æ ¼ä¿¡æ¯
        text = re.sub(r'\$[\d.,]+.*$', '', text).strip()
        # ç§»é™¤è¯„åˆ†ä¿¡æ¯
        text = re.sub(r'[\d.]+\s*out of.*$', '', text).strip()
        text = re.sub(r'Sale price.*$', '', text).strip()
        text = re.sub(r'Rated [\d.]+.*$', '', text).strip()
        # ç§»é™¤å¸¸è§çš„æ— å…³è¯æ±‡
        unwanted_words = ['View', 'view', 'æŸ¥çœ‹', 'Add to cart', 'æ·»åŠ åˆ°è´­ç‰©è½¦', 'Buy', 'Sale', 'Available']
        for word in unwanted_words:
            if text.strip() == word:
                return ""

        # ç§»é™¤æè¿°æ€§æ–‡æœ¬
        text = re.sub(r'This kit contains.*$', '', text).strip()
        text = re.sub(r'Available for sale.*$', '', text).strip()

        return text.strip()

    async def crawl_combined_tree_async(self, start_url, category_name=None):
        """
        å¼‚æ­¥æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯
        await self._init_async_http_manager()

        try:
            # è®¾ç½®ç›®æ ‡URL
            self.target_url = start_url

            # æ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥
            if self.cache_manager and self.use_cache and not self.force_refresh:
                print("ğŸ” æ‰§è¡Œæ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥...")
                self._perform_cache_precheck(start_url, category_name)
                self.cache_manager.display_cache_report()

            try:
                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
                print("ğŸ“Š é˜¶æ®µ 1/3: æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
                print("   æ­£åœ¨åˆ†æé¡µé¢ç»“æ„å’Œåˆ†ç±»å±‚æ¬¡...")
                base_tree = await self._crawl_tree_async(start_url, category_name)

                if not base_tree:
                    print("âŒ æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
                    return None

                print(f"âœ… åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆ")
                print("=" * 60)

                # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
                print("ğŸ“ é˜¶æ®µ 2/3: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
                print("   æ­£åœ¨å¤„ç†èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯å’Œå…ƒæ•°æ®...")
                enriched_tree = await self._enrich_tree_with_detailed_content_async(base_tree)

                # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
                print("ğŸ” é˜¶æ®µ 3/3: æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
                print("   æ­£åœ¨æå–æŒ‡å—å’Œæ•…éšœæ’é™¤è¯¦ç»†ä¿¡æ¯...")
                final_tree = await self._deep_crawl_product_content_async(enriched_tree)

                return final_tree
            except Exception as e:
                self.logger.error(f"âœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print(f"âœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                # è¿”å›å·²ç»çˆ¬å–çš„æ•°æ®ï¼Œé¿å…å®Œå…¨å¤±è´¥
                return None

        finally:
            # ç¡®ä¿å…³é—­å¼‚æ­¥HTTPå®¢æˆ·ç«¯
            await self._close_async_http_manager()

    async def _crawl_tree_async(self, start_url, category_name=None):
        """å¼‚æ­¥ç‰ˆæœ¬çš„æ ‘å½¢ç»“æ„æ„å»º"""
        # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„tree_crawlerï¼Œå› ä¸ºå®ƒä¸»è¦æ˜¯æ•°æ®å¤„ç†
        # æˆ–è€…æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå¼‚æ­¥ç‰ˆæœ¬
        return self.tree_crawler.crawl_tree(start_url, category_name)

    async def _enrich_tree_with_detailed_content_async(self, tree):
        """å¼‚æ­¥ç‰ˆæœ¬çš„æ ‘èŠ‚ç‚¹å†…å®¹ä¸°å¯ŒåŒ–"""
        # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•å¤„ç†æ¯ä¸ªèŠ‚ç‚¹
        return await self._enrich_node_async(tree)

    async def _enrich_node_async(self, node):
        """å¼‚æ­¥å¤„ç†å•ä¸ªèŠ‚ç‚¹çš„å†…å®¹ä¸°å¯ŒåŒ–"""
        if isinstance(node, dict):
            # å¦‚æœèŠ‚ç‚¹æœ‰URLï¼Œå¼‚æ­¥è·å–è¯¦ç»†å†…å®¹
            if 'url' in node and node['url']:
                try:
                    soup = await self.get_soup_async(node['url'])
                    if soup:
                        # æå–èŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
                        node = await self._extract_node_details_async(node, soup)
                except httpx.HTTPStatusError as e:
                    if e.response.status_code == 404:
                        self.logger.warning(f"é¡µé¢ä¸å­˜åœ¨ {node.get('url', '')}: 404 Not Found")
                        print(f"âš ï¸ é¡µé¢ä¸å­˜åœ¨: {node.get('url', '')}")
                        # æ ‡è®°èŠ‚ç‚¹ä¸ºæ— æ•ˆä½†ç»§ç»­å¤„ç†
                        node['valid'] = False
                        node['error'] = "404 Not Found"
                    else:
                        self.logger.error(f"å¼‚æ­¥å¤„ç†èŠ‚ç‚¹å¤±è´¥ {node.get('url', '')}: {e}")
                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥å¤„ç†èŠ‚ç‚¹å¤±è´¥ {node.get('url', '')}: {e}")
                    # æ ‡è®°èŠ‚ç‚¹ä½†ä¸ä¸­æ–­å¤„ç†
                    node['error'] = str(e)

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in node and node['children']:
                # å¹¶å‘å¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
                child_tasks = [self._enrich_node_async(child) for child in node['children']]
                try:
                    node['children'] = await asyncio.gather(*child_tasks, return_exceptions=True)

                    # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœï¼Œä½†è®°å½•é”™è¯¯
                    filtered_children = []
                    for child in node['children']:
                        if isinstance(child, Exception):
                            self.logger.error(f"å­èŠ‚ç‚¹å¤„ç†å¤±è´¥: {child}")
                        else:
                            filtered_children.append(child)
                    node['children'] = filtered_children
                except Exception as e:
                    self.logger.error(f"å¤„ç†å­èŠ‚ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # ä¿ç•™ç°æœ‰å­èŠ‚ç‚¹ï¼Œä¸è®©æ•´ä¸ªå¤„ç†å¤±è´¥

        return node

    async def _extract_node_details_async(self, node, soup):
        """å¼‚æ­¥æå–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„èŠ‚ç‚¹ä¿¡æ¯æå–é€»è¾‘
        # ç›®å‰ä¿æŒåŸæœ‰é€»è¾‘
        return node

    async def _deep_crawl_product_content_async(self, tree):
        """å¼‚æ­¥æ·±å…¥çˆ¬å–äº§å“é¡µé¢å†…å®¹"""
        return await self._deep_crawl_node_async(tree)

    async def _deep_crawl_node_async(self, node):
        """å¼‚æ­¥æ·±å…¥çˆ¬å–å•ä¸ªèŠ‚ç‚¹çš„äº§å“å†…å®¹"""
        if isinstance(node, dict):
            # å¦‚æœæ˜¯äº§å“é¡µé¢ï¼Œæå–æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
            if node.get('type') == 'product' and node.get('url'):
                try:
                    # ä½¿ç”¨ç°æœ‰çš„å¼‚æ­¥å¹¶å‘å¤„ç†æ–¹æ³•
                    guides_data, troubleshooting_data = await self._extract_product_content_async(node['url'])

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥æå–äº§å“å†…å®¹å¤±è´¥ {node['url']}: {e}")

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in node and node['children']:
                child_tasks = [self._deep_crawl_node_async(child) for child in node['children']]
                node['children'] = await asyncio.gather(*child_tasks, return_exceptions=True)

                # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
                node['children'] = [child for child in node['children']
                                  if not isinstance(child, Exception)]

        return node

    async def _extract_product_content_async(self, product_url):
        """å¼‚æ­¥æå–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹"""
        try:
            soup = await self.get_soup_async(product_url)
            if not soup:
                return [], []

            # ä½¿ç”¨ç°æœ‰çš„å†…å®¹æå–é€»è¾‘
            guides_data = []
            troubleshooting_data = []

            # æŸ¥æ‰¾æŒ‡å—å’Œæ•…éšœæ’é™¤é“¾æ¥
            guide_links = []
            troubleshooting_links = []

            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„é“¾æ¥æå–é€»è¾‘
            # ç›®å‰ä½¿ç”¨ç°æœ‰çš„æ–¹æ³•ä½œä¸ºå‚è€ƒ

            # å¦‚æœæœ‰é“¾æ¥ï¼Œä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†
            if guide_links or troubleshooting_links:
                tasks = []
                for url in guide_links:
                    tasks.append(('guide', url))
                for url in troubleshooting_links:
                    tasks.append(('troubleshooting', url))

                if tasks:
                    guides_data, troubleshooting_data = await self._process_tasks_async(
                        tasks, guides_data, troubleshooting_data)

            return guides_data, troubleshooting_data

        except Exception as e:
            self.logger.error(f"å¼‚æ­¥æå–äº§å“å†…å®¹å¤±è´¥ {product_url}: {e}")
            return [], []

    def crawl_combined_tree(self, start_url, category_name=None):
        """
        æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"ğŸš€ å¼€å§‹æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # è®¾ç½®ç›®æ ‡URL
        self.target_url = start_url

        # æ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥
        if self.cache_manager and self.use_cache and not self.force_refresh:
            print("ğŸ” æ‰§è¡Œæ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥...")
            self._perform_cache_precheck(start_url, category_name)
            self.cache_manager.display_cache_report()

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
        print("ğŸ“Š é˜¶æ®µ 1/3: æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
        print("   æ­£åœ¨åˆ†æé¡µé¢ç»“æ„å’Œåˆ†ç±»å±‚æ¬¡...")
        base_tree = self.tree_crawler.crawl_tree(start_url, category_name)

        if not base_tree:
            print("âŒ æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
            return None

        print(f"âœ… åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆ")
        print("=" * 60)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        print("ğŸ“ é˜¶æ®µ 2/3: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
        print("   æ­£åœ¨å¤„ç†èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯å’Œå…ƒæ•°æ®...")
        enriched_tree = self.enrich_tree_with_detailed_content(base_tree)

        # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
        print("ğŸ” é˜¶æ®µ 3/3: æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
        print("   æ­£åœ¨æå–æŒ‡å—å’Œæ•…éšœæ’é™¤è¯¦ç»†ä¿¡æ¯...")
        final_tree = self.deep_crawl_product_content(enriched_tree)

        return final_tree

    def _perform_cache_precheck(self, start_url, category_name=None):
        """æ‰§è¡Œç¼“å­˜é¢„æ£€æŸ¥ï¼Œåˆ†æå“ªäº›å†…å®¹éœ€è¦é‡æ–°å¤„ç†"""
        try:
            # æ¸…ç†æ— æ•ˆç¼“å­˜
            cleaned_count = self.cache_manager.clean_invalid_cache()
            if cleaned_count > 0:
                print(f"   å·²æ¸…ç† {cleaned_count} ä¸ªæ— æ•ˆç¼“å­˜æ¡ç›®")

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„é¢„æ£€æŸ¥é€»è¾‘
            # æ¯”å¦‚æ£€æŸ¥ç›®æ ‡URLçš„æ•´ä½“ç¼“å­˜çŠ¶æ€
            print("   ç¼“å­˜é¢„æ£€æŸ¥å®Œæˆ")

        except Exception as e:
            self.logger.error(f"ç¼“å­˜é¢„æ£€æŸ¥å¤±è´¥: {e}")

    def _get_node_cache_path(self, url, node_name):
        """è·å–èŠ‚ç‚¹çš„ç¼“å­˜è·¯å¾„ï¼Œç¡®ä¿ä¸ä¿å­˜æ—¶çš„è·¯å¾„ä¸€è‡´"""
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜ç´¢å¼•ä¸­æ˜¯å¦æœ‰è¿™ä¸ªURLçš„è®°å½•
        if self.cache_manager:
            url_hash = self.cache_manager.get_url_hash(url)
            if url_hash in self.cache_manager.cache_index:
                cache_entry = self.cache_manager.cache_index[url_hash]
                cached_path = cache_entry.get('local_path', '')
                if cached_path:
                    return Path(self.storage_root) / cached_path

        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰è®°å½•ï¼Œå°è¯•æŸ¥æ‰¾å®é™…å­˜åœ¨çš„è·¯å¾„
        if '/Device/' in url:
            # é¦–å…ˆå°è¯•æŸ¥æ‰¾å®é™…å­˜åœ¨çš„è®¾å¤‡ç›®å½•
            actual_path = self._find_actual_device_path_for_cache(url)
            if actual_path:
                return actual_path

            # å¦‚æœæ‰¾ä¸åˆ°å®é™…è·¯å¾„ï¼Œä½¿ç”¨æ ‡å‡†è·¯å¾„æ„å»ºé€»è¾‘
            device_path = url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            if device_path:
                import urllib.parse
                device_path = urllib.parse.unquote(device_path)

                # è§£æè·¯å¾„å±‚çº§ï¼Œç›´æ¥ä»ä¸»è¦è®¾å¤‡ç±»å‹å¼€å§‹
                path_parts = device_path.split('/')
                if len(path_parts) > 0:
                    # æ‰¾åˆ°ä¸»è¦è®¾å¤‡ç±»å‹ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼‰
                    main_category = path_parts[0]

                    # å¦‚æœæœ‰å­è·¯å¾„ï¼Œä¿ç•™å®Œæ•´çš„å±‚çº§ç»“æ„ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
                    if len(path_parts) > 1:
                        sub_path = '/'.join(path_parts[1:])
                        return Path(self.storage_root) / "Device" / main_category / sub_path
                    else:
                        return Path(self.storage_root) / "Device" / main_category
                else:
                    return Path(self.storage_root) / "Device" / device_path
            else:
                return Path(self.storage_root) / "Device"
        else:
            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨èŠ‚ç‚¹åç§°ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
            safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")
            return Path(self.storage_root) / "Device" / safe_name

    def _find_actual_device_path_for_cache(self, url):
        """ä¸ºç¼“å­˜æŸ¥æ‰¾å®é™…å­˜åœ¨çš„è®¾å¤‡è·¯å¾„"""
        try:
            device_path = url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)

            device_name = device_path.split('/')[-1]
            device_root = Path(self.storage_root) / "Device"

            if not device_root.exists():
                return None

            # é€’å½’æœç´¢åŒ¹é…çš„è®¾å¤‡ç›®å½•
            for path in device_root.rglob("*"):
                if path.is_dir() and path.name == device_name:
                    # æ£€æŸ¥è¿™ä¸ªç›®å½•æ˜¯å¦åŒ…å«info.jsonæ–‡ä»¶ï¼ˆç¡®è®¤æ˜¯è®¾å¤‡ç›®å½•ï¼‰
                    info_file = path / "info.json"
                    if info_file.exists():
                        return path

            return None

        except Exception as e:
            if self.verbose:
                self.logger.warning(f"æŸ¥æ‰¾å®é™…è®¾å¤‡è·¯å¾„å¤±è´¥: {e}")
            return None

    def _load_cached_node_data(self, local_path):
        """ä»ç¼“å­˜åŠ è½½èŠ‚ç‚¹æ•°æ®"""
        try:
            # åŠ è½½åŸºæœ¬ä¿¡æ¯
            info_file = local_path / "info.json"
            if not info_file.exists():
                return None

            with open(info_file, 'r', encoding='utf-8') as f:
                node_data = json.load(f)

            # åŠ è½½guidesæ•°æ®
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                guides = []
                for guide_file in sorted(guides_dir.glob("guide_*.json")):
                    with open(guide_file, 'r', encoding='utf-8') as f:
                        guides.append(json.load(f))
                if guides:
                    node_data['guides'] = guides

            # åŠ è½½troubleshootingæ•°æ®
            ts_dir = local_path / "troubleshooting"
            if ts_dir.exists():
                troubleshooting = []
                for ts_file in sorted(ts_dir.glob("troubleshooting_*.json")):
                    with open(ts_file, 'r', encoding='utf-8') as f:
                        troubleshooting.append(json.load(f))
                if troubleshooting:
                    node_data['troubleshooting'] = troubleshooting

            return node_data

        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None

    def enrich_tree_with_detailed_content(self, node):
        """ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ­£ç¡®çš„å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            return node

        # æ£€æŸ¥ç¼“å­˜ - ä½¿ç”¨ä¸ä¿å­˜æ—¶ä¸€è‡´çš„è·¯å¾„æ„å»ºé€»è¾‘
        local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

        if self.verbose:
            print(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
            print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

        if self._check_cache_validity(url, local_path):
            self.stats["cache_hits"] += 1
            if self.verbose:
                print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡å¤„ç†: {node.get('name', '')}")
            else:
                print(f"   âœ… è·³è¿‡å·²ç¼“å­˜: {node.get('name', '')}")
            return node

        self.processed_nodes.add(url)
        self.stats["cache_misses"] += 1

        time.sleep(random.uniform(0.5, 1.0))
        soup = self.get_soup(url)

        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½ç»è¿‡fix_node_dataå¤„ç†
        node = self.fix_node_data(node, soup)

        if self.verbose:
            self.logger.info(f"å¤„ç†èŠ‚ç‚¹: {node.get('name', '')} - {url}")
        else:
            print(f"   ğŸ“„ å¤„ç†èŠ‚ç‚¹: {node.get('name', '')}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            children_count = len(node['children'])
            if children_count > 0:
                print(f"   ğŸŒ³ å¤„ç† {children_count} ä¸ªå­èŠ‚ç‚¹...")
            enriched_children = []
            for i, child in enumerate(node['children'], 1):
                if children_count > 1:
                    print(f"      â””â”€ [{i}/{children_count}] {child.get('name', 'Unknown')}")
                enriched_child = self.enrich_tree_with_detailed_content(child)
                if enriched_child:
                    enriched_children.append(enriched_child)
            node['children'] = enriched_children

        return node

    def _process_content_concurrently(self, guide_links, device_url, skip_troubleshooting, soup):
        """å¹¶å‘å¤„ç†guideså’Œtroubleshootingå†…å®¹"""
        guides_data = []
        troubleshooting_data = []

        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []

        # æ·»åŠ guideä»»åŠ¡
        for guide_link in guide_links:
            tasks.append(('guide', guide_link))

        # æ·»åŠ troubleshootingä»»åŠ¡
        if not skip_troubleshooting:
            print(f"    ğŸ”§ æ­£åœ¨æœç´¢æ•…éšœæ’é™¤é¡µé¢...")

            # æ£€æŸ¥troubleshootingéƒ¨åˆ†çš„ç¼“å­˜
            troubleshooting_cache_key = f"{device_url}#troubleshooting"
            cached_troubleshooting = self._check_troubleshooting_cache(troubleshooting_cache_key, device_url)

            if cached_troubleshooting is not None:
                print(f"    âœ… ä½¿ç”¨ç¼“å­˜çš„æ•…éšœæ’é™¤æ•°æ® ({len(cached_troubleshooting)} ä¸ª)")
                troubleshooting_data = cached_troubleshooting
                # æ ‡è®°ä¸ºå·²ä»ç¼“å­˜åŠ è½½ï¼Œé¿å…é‡å¤ä¿å­˜
                self._troubleshooting_from_cache = True
            else:
                print(f"    ğŸ” æœªæ‰¾åˆ°æ•…éšœæ’é™¤ç¼“å­˜ï¼Œå¼€å§‹æå–...")
                troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, device_url)
                print(f"    ğŸ”§ æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢")
                self._troubleshooting_from_cache = False

                for ts_link in troubleshooting_links:
                    if isinstance(ts_link, dict):
                        ts_url = ts_link.get('url', '')
                    else:
                        ts_url = ts_link
                    if ts_url:
                        tasks.append(('troubleshooting', ts_url))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        if self.max_workers > 1 and len(tasks) > 1:
            print(f"    ğŸ”„ å¯åŠ¨å¹¶å‘å¤„ç† ({self.max_workers} çº¿ç¨‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡)...")

        if len(tasks) > 0:
            # ä½¿ç”¨ä¼ ç»Ÿçº¿ç¨‹æ± ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            print(f"    ğŸ”„ å¯åŠ¨çº¿ç¨‹æ± å¹¶å‘å¤„ç† ({self.max_workers} çº¿ç¨‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡)...")
            with ThreadPoolExecutor(max_workers=min(self.max_workers, len(tasks))) as executor:
                future_to_task = {}
                completed_count = 0
                total_tasks = len(tasks)

                for i, (task_type, url) in enumerate(tasks):
                    if task_type == 'guide':
                        future = executor.submit(self._process_guide_task_with_proxy, url, i)
                    else:  # troubleshooting
                        future = executor.submit(self._process_troubleshooting_task_with_proxy, url, i)
                    future_to_task[future] = (task_type, url)
                    # æ˜¾ç¤ºä»»åŠ¡å¼€å§‹æ—¶é—´
                    current_time = time.strftime("%H:%M:%S", time.localtime())
                    proxy_id = i % (self.proxy_manager.pool_size if self.proxy_manager else 1)
                    print(f"    ğŸš€ [{current_time}] å¯åŠ¨ {task_type} ä»»åŠ¡ (ä»£ç†#{proxy_id}): {url.split('/')[-1]}")

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task_type, url = future_to_task[future]
                    completed_count += 1
                    current_time = time.strftime("%H:%M:%S", time.localtime())
                    try:
                        result = future.result()
                        if result:
                            if task_type == 'guide':
                                guides_data.append(result)
                                print(f"    âœ… [{current_time}] [{completed_count}/{total_tasks}] æŒ‡å—: {result.get('title', '')}")
                            else:
                                troubleshooting_data.append(result)
                                print(f"    âœ… [{current_time}] [{completed_count}/{total_tasks}] æ•…éšœæ’é™¤: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{current_time}] [{completed_count}/{total_tasks}] {task_type} å¤„ç†å¤±è´¥")
                    except Exception as e:
                        print(f"    âŒ [{current_time}] [{completed_count}/{total_tasks}] {task_type} ä»»åŠ¡å¤±è´¥: {str(e)[:50]}...")
                        self.logger.error(f"å¹¶å‘ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")
        else:
            # å•çº¿ç¨‹å¤„ç†
            print(f"    ğŸ”„ é¡ºåºå¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
            for i, (task_type, url) in enumerate(tasks, 1):
                try:
                    print(f"    â³ [{i}/{len(tasks)}] æ­£åœ¨å¤„ç† {task_type}...")
                    if task_type == 'guide':
                        result = self._process_guide_task(url)
                        if result:
                            guides_data.append(result)
                            print(f"    âœ… [{i}/{len(tasks)}] æŒ‡å—: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{i}/{len(tasks)}] æŒ‡å—å¤„ç†å¤±è´¥")
                    else:
                        result = self._process_troubleshooting_task(url)
                        if result:
                            troubleshooting_data.append(result)
                            print(f"    âœ… [{i}/{len(tasks)}] æ•…éšœæ’é™¤: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{i}/{len(tasks)}] æ•…éšœæ’é™¤å¤„ç†å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")

        # ä¸åœ¨è¿™é‡Œä¿å­˜troubleshootingç¼“å­˜ï¼Œç­‰å¾…æ–‡ä»¶ç³»ç»Ÿä¿å­˜é˜¶æ®µ
        # if troubleshooting_data and not skip_troubleshooting and not getattr(self, '_troubleshooting_from_cache', False):
        #     troubleshooting_cache_key = f"{device_url}#troubleshooting"
        #     print(f"    ğŸ’¾ ä¿å­˜æ•…éšœæ’é™¤æ•°æ®åˆ°ç¼“å­˜ ({len(troubleshooting_data)} ä¸ª)")
        #     self._save_troubleshooting_cache(troubleshooting_cache_key, device_url, troubleshooting_data)

        return guides_data, troubleshooting_data

    def _process_guide_task(self, guide_url):
        """å¤„ç†å•ä¸ªguideä»»åŠ¡"""
        try:
            guide_content = self.extract_guide_content(guide_url)
            if guide_content:
                guide_content['url'] = guide_url

                # ç¡®ä¿"What You Need"éƒ¨åˆ†è¢«æ­£ç¡®æå–
                if 'what_you_need' not in guide_content or not guide_content['what_you_need']:
                    print(f"  é‡æ–°å°è¯•æå–What You Needéƒ¨åˆ†: {guide_url}")
                    what_you_need = self.extract_what_you_need_enhanced(guide_url)
                    if what_you_need:
                        guide_content['what_you_need'] = what_you_need
                        print(f"  æˆåŠŸæå–What You Need: {list(what_you_need.keys())}")

                return guide_content
        except Exception as e:
            self.logger.error(f"å¤„ç†guideå¤±è´¥ {guide_url}: {e}")
            self._log_failed_url(guide_url, f"Guideå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_troubleshooting_task(self, ts_url):
        """å¤„ç†å•ä¸ªtroubleshootingä»»åŠ¡"""
        try:
            ts_content = self.extract_troubleshooting_content(ts_url)
            if ts_content:
                ts_content['url'] = ts_url
                return ts_content
        except Exception as e:
            self.logger.error(f"å¤„ç†troubleshootingå¤±è´¥ {ts_url}: {e}")
            self._log_failed_url(ts_url, f"Troubleshootingå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_guide_task_with_proxy(self, guide_url, thread_id):
        """å¤„ç†å•ä¸ªguideä»»åŠ¡ï¼ˆå¸¦ç‹¬ç«‹ä»£ç†ï¼‰"""
        try:
            # ä¸ºå½“å‰çº¿ç¨‹è®¾ç½®ä»£ç†ID
            threading.current_thread().proxy_id = thread_id

            guide_content = self.extract_guide_content(guide_url)
            if guide_content:
                guide_content['url'] = guide_url

                # ç¡®ä¿"What You Need"éƒ¨åˆ†è¢«æ­£ç¡®æå–
                if 'what_you_need' not in guide_content or not guide_content['what_you_need']:
                    print(f"  é‡æ–°å°è¯•æå–What You Needéƒ¨åˆ†: {guide_url}")
                    what_you_need = self.extract_what_you_need_enhanced(guide_url)
                    if what_you_need:
                        guide_content['what_you_need'] = what_you_need
                        print(f"  æˆåŠŸæå–What You Need: {list(what_you_need.keys())}")

                return guide_content
        except Exception as e:
            self.logger.error(f"å¤„ç†guideå¤±è´¥ {guide_url}: {e}")
            self._log_failed_url(guide_url, f"Guideå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_troubleshooting_task_with_proxy(self, ts_url, thread_id):
        """å¤„ç†å•ä¸ªtroubleshootingä»»åŠ¡ï¼ˆå¸¦ç‹¬ç«‹ä»£ç†ï¼‰"""
        try:
            # ä¸ºå½“å‰çº¿ç¨‹è®¾ç½®ä»£ç†ID
            threading.current_thread().proxy_id = thread_id

            ts_content = self.extract_troubleshooting_content(ts_url)
            if ts_content:
                ts_content['url'] = ts_url
                return ts_content
        except Exception as e:
            self.logger.error(f"å¤„ç†troubleshootingå¤±è´¥ {ts_url}: {e}")
            self._log_failed_url(ts_url, f"Troubleshootingå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _check_troubleshooting_cache(self, cache_key, device_url):
        """æ£€æŸ¥troubleshootingéƒ¨åˆ†çš„ç¼“å­˜"""
        if not self.cache_manager:
            return None

        if self.cache_manager.is_troubleshooting_section_cached(cache_key, device_url):
            return self.cache_manager.load_troubleshooting_cache(cache_key, device_url)
        return None

    def _should_have_troubleshooting_content(self, url):
        """æ£€æŸ¥é¡µé¢æ˜¯å¦åº”è¯¥æœ‰troubleshootingå†…å®¹"""
        try:
            # å¯¹äºäº§å“é¡µé¢ï¼Œé€šå¸¸éƒ½åº”è¯¥æœ‰troubleshootingå†…å®¹
            # è¿™é‡Œå¯ä»¥æ ¹æ®URLæ¨¡å¼æˆ–é¡µé¢ç±»å‹åˆ¤æ–­
            if '/Device/' in url and not url.endswith('/Device'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å…·ä½“çš„äº§å“é¡µé¢è€Œä¸æ˜¯åˆ†ç±»é¡µé¢
                # å¯ä»¥é€šè¿‡è®¿é—®é¡µé¢å¿«é€Ÿæ£€æŸ¥æ˜¯å¦æœ‰troubleshootingé“¾æ¥
                return True
            return False
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰troubleshootingå†…å®¹å¤±è´¥: {e}")
            return False

    def _mark_cache_invalid(self, url, local_path):
        """æ ‡è®°ç¼“å­˜ä¸ºæ— æ•ˆå¹¶ä»ç´¢å¼•ä¸­ç§»é™¤"""
        try:
            if self.cache_manager:
                url_hash = self.cache_manager.get_url_hash(url)
                if url_hash in self.cache_manager.cache_index:
                    del self.cache_manager.cache_index[url_hash]
                    self.cache_manager.save_cache_index()
                    print(f"    ğŸ—‘ï¸ å·²ä»ç¼“å­˜ç´¢å¼•ä¸­ç§»é™¤æ— æ•ˆæ¡ç›®: {url}")
        except Exception as e:
            self.logger.error(f"æ ‡è®°ç¼“å­˜æ— æ•ˆå¤±è´¥: {e}")

    def _is_troubleshooting_cached(self, cache_key):
        """æ£€æŸ¥troubleshootingæ˜¯å¦å·²ç¼“å­˜"""
        if not self.cache_manager:
            return False
        return self.cache_manager.is_troubleshooting_section_cached(cache_key, "")

    def _save_troubleshooting_cache(self, cache_key, device_url, troubleshooting_data):
        """ä¿å­˜troubleshootingåˆ°ç¼“å­˜"""
        if self.cache_manager:
            self.cache_manager.save_troubleshooting_cache(cache_key, device_url, troubleshooting_data)

    def _print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)

        total_requests = self.stats["total_requests"]
        cache_hits = self.stats["cache_hits"]
        cache_misses = self.stats["cache_misses"]

        if total_requests > 0:
            cache_hit_rate = (cache_hits / (cache_hits + cache_misses)) * 100 if (cache_hits + cache_misses) > 0 else 0
            print(f"ğŸŒ æ€»è¯·æ±‚æ•°: {total_requests}")
            print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_hits} ({cache_hit_rate:.1f}%)")
            print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­: {cache_misses}")

        retry_success = self.stats["retry_success"]
        retry_failed = self.stats["retry_failed"]
        total_retries = retry_success + retry_failed

        if total_retries > 0:
            retry_success_rate = (retry_success / total_retries) * 100
            print(f"ğŸ” é‡è¯•æˆåŠŸ: {retry_success}/{total_retries} ({retry_success_rate:.1f}%)")
            print(f"âŒ é‡è¯•å¤±è´¥: {retry_failed}")

        media_downloaded = self.stats["media_downloaded"]
        media_failed = self.stats["media_failed"]
        total_media = media_downloaded + media_failed

        if total_media > 0:
            media_success_rate = (media_downloaded / total_media) * 100
            print(f"ğŸ“ åª’ä½“ä¸‹è½½æˆåŠŸ: {media_downloaded}/{total_media} ({media_success_rate:.1f}%)")
            print(f"ğŸ“ åª’ä½“ä¸‹è½½å¤±è´¥: {media_failed}")

        # è§†é¢‘å¤„ç†ç»Ÿè®¡
        videos_downloaded = self.stats.get("videos_downloaded", 0)
        videos_skipped = self.stats.get("videos_skipped", 0)
        total_videos = videos_downloaded + videos_skipped

        if total_videos > 0:
            print(f"ğŸ¥ è§†é¢‘æ–‡ä»¶å¤„ç†:")
            print(f"   âœ… å·²ä¸‹è½½: {videos_downloaded}")
            print(f"   â­ï¸ å·²è·³è¿‡: {videos_skipped}")
            if not self.download_videos:
                print(f"   ğŸ’¡ æç¤º: è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå¦‚éœ€ä¸‹è½½è¯·è®¾ç½® download_videos=True")
            else:
                print(f"   ğŸ“ å¤§å°é™åˆ¶: {self.max_video_size_mb}MB")

        # æ€§èƒ½ä¼˜åŒ–ç»Ÿè®¡
        cache_size = len(getattr(self, '_file_exists_cache', {}))
        if cache_size > 0:
            print(f"âš¡ æ€§èƒ½ä¼˜åŒ–ç»Ÿè®¡:")
            print(f"   ğŸ“‹ æ–‡ä»¶ç¼“å­˜æ¡ç›®: {cache_size}")
            print(f"   ğŸš€ å¼‚æ­¥ä¸‹è½½ä¼˜åŒ–: å·²å¯ç”¨")
            print(f"   ğŸ”„ é«˜å¹¶å‘ä¸‹è½½: å·²å¯ç”¨")

        if self.use_proxy and hasattr(self, 'proxy_switch_count'):
            print(f"ğŸ”„ ä»£ç†åˆ‡æ¢æ¬¡æ•°: {self.proxy_switch_count}")

        print("=" * 60)
        


    def count_detailed_nodes(self, node):
        """
        ç»Ÿè®¡æ ‘ä¸­åŒ…å«è¯¦ç»†å†…å®¹çš„èŠ‚ç‚¹æ•°é‡
        """
        count = {'guides': 0, 'troubleshooting': 0, 'products': 0, 'categories': 0}

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„guideså’Œtroubleshootingæ•°ç»„
        if 'guides' in node and isinstance(node['guides'], list):
            count['guides'] += len(node['guides'])

        if 'troubleshooting' in node and isinstance(node['troubleshooting'], list):
            count['troubleshooting'] += len(node['troubleshooting'])

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹
        if 'title' in node and 'steps' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹ï¼ˆä¸åœ¨guidesæ•°ç»„ä¸­ï¼‰
            count['guides'] += 1
        elif 'causes' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹ï¼ˆä¸åœ¨troubleshootingæ•°ç»„ä¸­ï¼‰
            count['troubleshooting'] += 1
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            count['products'] += 1
        else:
            # åˆ†ç±»èŠ‚ç‚¹
            count['categories'] += 1

        # é€’å½’ç»Ÿè®¡å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for child in node['children']:
                child_count = self.count_detailed_nodes(child)
                for key in count:
                    count[key] += child_count[key]

        return count

    def extract_guides_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æŒ‡å—é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§é¡µé¢å¸ƒå±€
        """
        guides = []
        seen_guide_urls = set()

        if not soup:
            return guides

        # æŸ¥æ‰¾æ‰€æœ‰æŒ‡å—é“¾æ¥ï¼ˆåŒ…æ‹¬Guideå’ŒTeardownï¼‰
        guide_links = soup.select("a[href*='/Guide/'], a[href*='/Teardown/']")

        for link in guide_links:
            href = link.get("href")
            text = link.get_text().strip()

            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
            skip_patterns = [
                "Create Guide", "åˆ›å»ºæŒ‡å—", "Guide/new", "Guide/edit", "/edit/",
                "Guide/history", "/history/", "Edit Guide", "ç¼–è¾‘æŒ‡å—",
                "Version History", "ç‰ˆæœ¬å†å²", "Teardown/edit", "#comment", "permalink=comment"
            ]

            if href and text and not any(pattern in href or pattern in text for pattern in skip_patterns):
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_guide_urls:
                    seen_guide_urls.add(href)
                    guides.append({
                        "title": text,
                        "url": href
                    })

        return guides

    def extract_troubleshooting_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æ•…éšœæ’é™¤é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬
        """
        troubleshooting_links = []
        seen_ts_urls = set()

        if not soup:
            return troubleshooting_links

        # æŸ¥æ‰¾æ‰€æœ‰æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
        # 1. ç›´æ¥çš„Troubleshootingé“¾æ¥
        ts_links = soup.select("a[href*='/Troubleshooting/']")

        # 2. æŸ¥æ‰¾åŒ…å«æ•…éšœæ’é™¤æ–‡æœ¬çš„é“¾æ¥
        all_links = soup.select("a[href]")
        for link in all_links:
            text = link.get_text().strip().lower()
            href = link.get("href", "")

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
            troubleshooting_keywords = [
                'troubleshooting', 'æ•…éšœæ’é™¤', 'æ•…éšœ', 'problems', 'issues',
                'won\'t turn on', 'not working', 'broken'
            ]

            if any(keyword in text for keyword in troubleshooting_keywords) or '/Troubleshooting/' in href:
                ts_links.append(link)

        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        for link in ts_links:
            href = link.get("href")
            text = link.get_text().strip()

            if href and text:
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_ts_urls and '/Troubleshooting/' in href:
                    seen_ts_urls.add(href)
                    troubleshooting_links.append({
                        "title": text,
                        "url": href
                    })

        return troubleshooting_links

    def extract_troubleshooting_content(self, troubleshooting_url):
        """
        æå–æ•…éšœæ’é™¤é¡µé¢çš„è¯¦ç»†å†…å®¹ - å®Œå…¨æŒ‰ç…§combined_crawler.pyçš„é€»è¾‘
        """
        if not troubleshooting_url:
            return None

        # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in troubleshooting_url:
            troubleshooting_url = troubleshooting_url.replace('zh.ifixit.com', 'www.ifixit.com')

        # æ·»åŠ lang=enå‚æ•°
        if '?' in troubleshooting_url:
            if 'lang=' not in troubleshooting_url:
                troubleshooting_url += '&lang=en'
        else:
            troubleshooting_url += '?lang=en'

        print(f"Crawling troubleshooting content: {troubleshooting_url}")

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(random.uniform(1, 2))

        soup = self.get_soup(troubleshooting_url)
        if not soup:
            return None

        troubleshooting_data = {
            "url": troubleshooting_url,
            "title": "",
            "causes": []
        }

        try:
            # æå–æ ‡é¢˜
            title_elem = soup.select_one("h1")
            if title_elem:
                title_text = title_elem.get_text().strip()
                title_text = re.sub(r'\s+', ' ', title_text)
                troubleshooting_data["title"] = title_text
                print(f"æå–æ ‡é¢˜: {title_text}")

            # åŠ¨æ€æå–é¡µé¢ä¸Šçš„çœŸå®å­—æ®µå†…å®¹ï¼ˆå¦‚first_stepsç­‰ï¼‰
            dynamic_sections = self.extract_dynamic_sections(soup)
            troubleshooting_data.update(dynamic_sections)

            # æå–causeså¹¶ä¸ºæ¯ä¸ªcauseæå–å¯¹åº”çš„å›¾ç‰‡å’Œè§†é¢‘
            troubleshooting_data["causes"] = self.extract_causes_sections_with_media(soup)

            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•…éšœæ’é™¤é¡µé¢
            if not self._is_valid_troubleshooting_page(troubleshooting_data):
                print(f"è·³è¿‡é€šç”¨æˆ–æ— æ•ˆçš„æ•…éšœæ’é™¤é¡µé¢: {troubleshooting_data.get('title', '')}")
                return None

            # æå–ç»Ÿè®¡æ•°æ®
            statistics = self.extract_page_statistics(soup)

            # é‡æ„ç»Ÿè®¡æ•°æ®ç»“æ„ - å°†view_statisticsç§»åŠ¨åˆ°titleä¸‹é¢
            # é‡æ–°æ„å»ºtroubleshooting_dataï¼Œç¡®ä¿å­—æ®µé¡ºåºæ­£ç¡®
            ordered_ts_data = {}
            ordered_ts_data["url"] = troubleshooting_data["url"]
            ordered_ts_data["title"] = troubleshooting_data["title"]

            # å°†ç»Ÿè®¡æ•°æ®ç´§è·Ÿåœ¨titleåé¢ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if statistics:
                # åˆ›å»ºview_statisticså¯¹è±¡ï¼ŒåªåŒ…å«æ—¶é—´ç›¸å…³çš„ç»Ÿè®¡
                view_stats = {}
                if 'past_24_hours' in statistics:
                    view_stats['past_24_hours'] = statistics['past_24_hours']
                if 'past_7_days' in statistics:
                    view_stats['past_7_days'] = statistics['past_7_days']
                if 'past_30_days' in statistics:
                    view_stats['past_30_days'] = statistics['past_30_days']
                if 'all_time' in statistics:
                    view_stats['all_time'] = statistics['all_time']

                if view_stats:
                    ordered_ts_data["view_statistics"] = view_stats
                if 'completed' in statistics:
                    ordered_ts_data["completed"] = statistics['completed']
                if 'favorites' in statistics:
                    ordered_ts_data["favorites"] = statistics['favorites']

            # æ·»åŠ åŠ¨æ€æå–çš„å­—æ®µï¼ˆæŒ‰é¡µé¢å®é™…å­—æ®µåç§°ï¼‰
            for key, value in troubleshooting_data.items():
                if key not in ["url", "title", "causes", "images", "videos"] and value:
                    ordered_ts_data[key] = value

            # æ·»åŠ å…¶ä»–å­—æ®µ
            if "causes" in troubleshooting_data and troubleshooting_data["causes"]:
                ordered_ts_data["causes"] = troubleshooting_data["causes"]

            troubleshooting_data = ordered_ts_data

            # æ‰“å°æå–ç»“æœç»Ÿè®¡
            dynamic_fields = [k for k in troubleshooting_data.keys()
                            if k not in ['url', 'title', 'causes', 'view_statistics', 'completed', 'favorites']]

            print(f"æå–å®Œæˆ:")
            for field in dynamic_fields:
                content = troubleshooting_data.get(field, '')
                if content:
                    print(f"  {field}: {len(content)} å­—ç¬¦")

            causes = troubleshooting_data.get('causes', [])
            print(f"  Causes: {len(causes)} ä¸ª")

            # ç»Ÿè®¡æ¯ä¸ªcauseä¸­çš„å›¾ç‰‡å’Œè§†é¢‘æ•°é‡
            total_images = sum(len(cause.get('images', [])) for cause in causes)
            total_videos = sum(len(cause.get('videos', [])) for cause in causes)
            print(f"  Images (in causes): {total_images} ä¸ª")
            print(f"  Videos (in causes): {total_videos} ä¸ª")

            if statistics:
                print(f"  Statistics: {len(statistics)} é¡¹")

            return troubleshooting_data

        except Exception as e:
            print(f"æå–æ•…éšœæ’é™¤å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _is_valid_troubleshooting_page(self, troubleshooting_data):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å…·ä½“æ•…éšœæ’é™¤é¡µé¢ï¼Œè€Œä¸æ˜¯é€šç”¨é¡µé¢"""
        if not troubleshooting_data:
            return False

        title = troubleshooting_data.get('title', '').lower()
        causes = troubleshooting_data.get('causes', [])

        # æ£€æŸ¥æ˜¯å¦ä¸ºé€šç”¨é¡µé¢çš„æ ‡é¢˜
        generic_keywords = [
            'common problems', 'fix common', 'general troubleshooting',
            'troubleshooting guide', 'basic troubleshooting', 'general issues',
            'common issues', 'general problems'
        ]

        for keyword in generic_keywords:
            if keyword in title:
                print(f"æ£€æµ‹åˆ°é€šç”¨é¡µé¢æ ‡é¢˜å…³é”®è¯: {keyword}")
                return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ•…éšœåŸå› 
        if not causes or len(causes) == 0:
            print("æ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„æ•…éšœåŸå› ")
            return False

        # æ£€æŸ¥causeså†…å®¹æ˜¯å¦è¶³å¤Ÿå…·ä½“
        if len(causes) < 2:  # è‡³å°‘è¦æœ‰2ä¸ªå…·ä½“çš„æ•…éšœåŸå› 
            print(f"æ•…éšœåŸå› å¤ªå°‘: {len(causes)} ä¸ª")
            return False

        # æ£€æŸ¥causesæ˜¯å¦æœ‰å®é™…å†…å®¹
        valid_causes = 0
        for cause in causes:
            content = cause.get('content', '')
            if content and len(content.strip()) > 50:  # å†…å®¹è¦è¶³å¤Ÿè¯¦ç»†
                valid_causes += 1

        if valid_causes < 2:
            print(f"æœ‰æ•ˆçš„æ•…éšœåŸå› å¤ªå°‘: {valid_causes} ä¸ª")
            return False

        print(f"éªŒè¯é€šè¿‡: {len(causes)} ä¸ªæ•…éšœåŸå› , {valid_causes} ä¸ªæœ‰æ•ˆ")
        return True

    def deep_crawl_product_content(self, node, skip_troubleshooting=False):
        """æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶æ­£ç¡®æ„å»ºæ•°æ®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        node_name = node.get('name', 'Unknown')
        is_specified_target = self._is_specified_target_url(url)

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        children_count = len(node.get('children', []))
        print(f"ğŸ” æ·±åº¦çˆ¬å–èŠ‚ç‚¹: {node_name} (å­èŠ‚ç‚¹: {children_count})")

        if self.verbose:
            print(f"   URL: {url}")
            print(f"   æ˜¯å¦æŒ‡å®šç›®æ ‡: {is_specified_target}")
            print(f"   æ˜¯å¦è·³è¿‡troubleshooting: {skip_troubleshooting}")
            if node.get('children'):
                child_names = [child.get('name', 'Unknown') for child in node['children'][:3]]
                if len(node['children']) > 3:
                    child_names.append(f"...ç­‰{len(node['children'])}ä¸ª")
                print(f"   å­èŠ‚ç‚¹: {', '.join(child_names)}")

        # åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡é¡µé¢ï¼Œéœ€è¦å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
        # 1. æŒ‡å®šçš„ç›®æ ‡URL - å¿…é¡»å¤„ç†
        # 2. æ²¡æœ‰å­ç±»åˆ«çš„é¡µé¢ - å¿…é¡»å¤„ç†
        # 3. æœ‰å­ç±»åˆ«ä½†å­ç±»åˆ«ä¸ºç©ºæ•°ç»„çš„é¡µé¢ - å¿…é¡»å¤„ç†
        # 4. æœ‰å­ç±»åˆ«ä¸”å­ç±»åˆ«éç©ºçš„é¡µé¢ - ä¸åº”è¯¥è¢«å½“ä½œç›®æ ‡é¡µé¢ï¼Œåº”è¯¥é€’å½’å¤„ç†å­èŠ‚ç‚¹
        has_real_children = node.get('children') and len(node.get('children', [])) > 0

        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (is_specified_target or not has_real_children)
        )

        if is_target_page:
            # æ£€æŸ¥æ·±åº¦å†…å®¹çš„ç¼“å­˜
            local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

            if self.verbose:
                print(f"ğŸ” æ£€æŸ¥æ·±åº¦å†…å®¹ç¼“å­˜: {url}")
                print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

            if self._check_cache_validity(url, local_path):
                if self.verbose:
                    print(f"âœ… æ·±åº¦å†…å®¹ç¼“å­˜å‘½ä¸­ï¼Œæ£€æŸ¥troubleshootingç¼“å­˜: {node.get('name', '')}")
                else:
                    print(f"   âœ… ä¸»ç¼“å­˜å‘½ä¸­ï¼Œæ£€æŸ¥troubleshootingç¼“å­˜: {node.get('name', '')}")

                # ä»ç¼“å­˜åŠ è½½æ•°æ®
                try:
                    cached_node = self._load_cached_node_data(local_path)
                    if cached_node:
                        # å³ä½¿ä¸»ç¼“å­˜å‘½ä¸­ï¼Œä¹Ÿè¦æ£€æŸ¥troubleshootingç¼“å­˜
                        troubleshooting_cache_key = f"{url}#troubleshooting"
                        cached_troubleshooting = self._check_troubleshooting_cache(troubleshooting_cache_key, url)

                        if cached_troubleshooting is not None:
                            print(f"    âœ… ä½¿ç”¨ç¼“å­˜çš„æ•…éšœæ’é™¤æ•°æ® ({len(cached_troubleshooting)} ä¸ª)")
                            cached_node['troubleshooting'] = cached_troubleshooting
                            return cached_node
                        else:
                            print(f"    ğŸ” æœªæ‰¾åˆ°æ•…éšœæ’é™¤ç¼“å­˜ï¼Œæ£€æŸ¥ä¸»ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰æ•°æ®")
                            # æ£€æŸ¥ä¸»ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰troubleshootingæ•°æ®
                            if 'troubleshooting' in cached_node and cached_node['troubleshooting']:
                                print(f"    âœ… ä¸»ç¼“å­˜ä¸­å·²æœ‰æ•…éšœæ’é™¤æ•°æ® ({len(cached_node['troubleshooting'])} ä¸ª)")
                                return cached_node

                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰troubleshootingå†…å®¹ä½†ç¼“å­˜ç¼ºå¤±
                            should_have_troubleshooting = self._should_have_troubleshooting_content(url)
                            if should_have_troubleshooting:
                                print(f"    âš ï¸ é¡µé¢åº”è¯¥æœ‰æ•…éšœæ’é™¤å†…å®¹ä½†ç¼“å­˜ç¼ºå¤±ï¼Œä»…é‡æ–°çˆ¬å–æ•…éšœæ’é™¤éƒ¨åˆ†")
                                # åªé‡æ–°çˆ¬å–troubleshootingéƒ¨åˆ†ï¼Œä¸é‡æ–°å¤„ç†æ•´ä¸ªé¡µé¢
                                try:
                                    soup = self.get_soup(url)
                                    if soup:
                                        troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, url)
                                        if troubleshooting_links:
                                            print(f"    ğŸ”§ æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢ï¼Œå¼€å§‹å¤„ç†...")
                                            troubleshooting_data = []
                                            for ts_link in troubleshooting_links:
                                                if isinstance(ts_link, dict):
                                                    ts_url = ts_link.get('url', '')
                                                else:
                                                    ts_url = ts_link
                                                if ts_url:
                                                    ts_content = self.extract_troubleshooting_content(ts_url)
                                                    if ts_content:
                                                        ts_content['url'] = ts_url
                                                        troubleshooting_data.append(ts_content)

                                            if troubleshooting_data:
                                                cached_node['troubleshooting'] = troubleshooting_data
                                                # ä¸åœ¨è¿™é‡Œä¿å­˜troubleshootingç¼“å­˜ï¼Œç­‰å¾…æ–‡ä»¶ç³»ç»Ÿä¿å­˜é˜¶æ®µ
                                                # troubleshooting_cache_key = f"{url}#troubleshooting"
                                                # self._save_troubleshooting_cache(troubleshooting_cache_key, url, troubleshooting_data)
                                                print(f"    âœ… æˆåŠŸè¡¥å……æ•…éšœæ’é™¤æ•°æ® ({len(troubleshooting_data)} ä¸ª)")
                                    return cached_node
                                except Exception as e:
                                    print(f"    âŒ é‡æ–°çˆ¬å–æ•…éšœæ’é™¤å¤±è´¥: {e}")
                                    return cached_node
                            else:
                                print(f"    âœ… é¡µé¢ä¸éœ€è¦æ•…éšœæ’é™¤å†…å®¹ï¼Œä½¿ç”¨ä¸»ç¼“å­˜")
                                return cached_node
                except Exception as e:
                    if self.verbose:
                        print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡æ–°å¤„ç†: {e}")

            if self.verbose:
                print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹æ·±åº¦å¤„ç†: {node.get('name', '')}")
            print(f"ğŸ¯ æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")
            print(f"   ğŸ“ URL: {url}")

            try:
                print(f"   ğŸŒ æ­£åœ¨è·å–é¡µé¢å†…å®¹...")
                soup = self.get_soup(url)
                if soup:
                    # åŸºæœ¬æ•°æ®ä¿®å¤
                    node = self.fix_node_data(node, soup)

                    # åªåœ¨ç›®æ ‡äº§å“é¡µé¢æ·»åŠ è¿™äº›å­—æ®µ
                    is_root_device = url == f"{self.base_url}/Device"
                    if not is_root_device:
                        # æ·»åŠ titleå­—æ®µ
                        real_title = self.extract_real_title_from_page(soup)
                        if real_title:
                            node['title'] = real_title

                        # æ·»åŠ instruction_urlå­—æ®µ
                        node['instruction_url'] = ""

                        # æ·»åŠ view_statisticså­—æ®µ
                        real_stats = self.extract_real_view_statistics(soup, url)
                        if real_stats:
                            node['view_statistics'] = real_stats

                    print(f"   ğŸ” æ­£åœ¨æœç´¢æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹...")
                    guides = self.extract_guides_from_device_page(soup, url)
                    guide_links = [guide["url"] for guide in guides]
                    print(f"   ğŸ“– æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†troubleshootingï¼ˆåŸºäºæ–°çš„é€»è¾‘ï¼‰
                    should_process_troubleshooting = (
                        not skip_troubleshooting and
                        not self._is_troubleshooting_processed_in_parent_path(url)
                    )

                    # åˆå§‹åŒ–å˜é‡
                    current_path = url.split('/Device/')[-1].split('?')[0].rstrip('/') if '/Device/' in url else ''
                    cached_troubleshooting = None

                    # å¦‚æœåº”è¯¥å¤„ç†troubleshootingï¼Œå…ˆæ£€æŸ¥ç¼“å­˜
                    if should_process_troubleshooting and current_path:
                        # é¦–å…ˆæ£€æŸ¥troubleshootingç¼“å­˜
                        troubleshooting_cache_key = f"{url}#troubleshooting"
                        cached_troubleshooting = self._check_troubleshooting_cache(troubleshooting_cache_key, url)

                        if cached_troubleshooting is not None:
                            # å¦‚æœæœ‰ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œä¸éœ€è¦é‡æ–°å¤„ç†
                            print(f"   âœ… å‘ç°troubleshootingç¼“å­˜ ({len(cached_troubleshooting)} ä¸ª)ï¼Œè·³è¿‡é‡æ–°å¤„ç†")
                            should_process_troubleshooting = False
                            # æ ‡è®°å½“å‰è·¯å¾„å·²å¤„ç†troubleshooting
                            self.processed_troubleshooting_paths.add(current_path)
                        else:
                            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œæ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦æœ‰troubleshootingå†…å®¹
                            troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, url)
                            if troubleshooting_links:
                                # æ ‡è®°å½“å‰è·¯å¾„å·²å¤„ç†troubleshooting
                                self.processed_troubleshooting_paths.add(current_path)
                                print(f"   âœ… åœ¨è·¯å¾„ '{current_path}' é¦–æ¬¡å‘ç°troubleshootingï¼Œå¼€å§‹å¤„ç†...")
                                print(f"   ğŸ”§ åç»­å­è·¯å¾„å°†è·³è¿‡troubleshootingå¤„ç†")
                            else:
                                should_process_troubleshooting = False

                    # ä½¿ç”¨å¹¶å‘å¤„ç†guideså’Œtroubleshooting
                    print(f"   âš™ï¸  å¼€å§‹å¤„ç†è¯¦ç»†å†…å®¹...")

                    # å¦‚æœæœ‰ç¼“å­˜çš„troubleshootingæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                    if cached_troubleshooting is not None:
                        print(f"   ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„troubleshootingæ•°æ® ({len(cached_troubleshooting)} ä¸ª)")
                        # åªå¤„ç†guidesï¼Œtroubleshootingä½¿ç”¨ç¼“å­˜
                        guides_data, _ = self._process_content_concurrently(
                            guide_links, url, True, soup  # skip_troubleshooting=True
                        )
                        troubleshooting_data = cached_troubleshooting
                    else:
                        # æ­£å¸¸å¤„ç†guideså’Œtroubleshooting
                        guides_data, troubleshooting_data = self._process_content_concurrently(
                            guide_links, url, not should_process_troubleshooting, soup
                        )

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                    # å¤„ç†å­ç±»åˆ« - æ— è®ºæ˜¯å¦ä¸ºæŒ‡å®šç›®æ ‡éƒ½è¦ä¿æŒå±‚çº§ç»“æ„
                    if is_specified_target:
                        # å¦‚æœæ˜¯æŒ‡å®šç›®æ ‡ï¼Œå°è¯•å‘ç°æ–°çš„å­ç±»åˆ«
                        subcategories = self.discover_subcategories_from_page(soup, url)
                        if subcategories:
                            print(f"  å‘ç° {len(subcategories)} ä¸ªå­ç±»åˆ«ï¼Œå¼€å§‹å¤„ç†...")

                            subcategory_children = []
                            for subcat in subcategories:
                                print(f"    å¤„ç†å­ç±»åˆ«: {subcat['name']}")

                                subcat_node = {
                                    'name': subcat['name'],
                                    'url': subcat['url'],
                                    'title': subcat['title']
                                }

                                # æ£€æŸ¥å­ç±»åˆ«æ˜¯å¦åº”è¯¥è·³è¿‡troubleshootingï¼ˆåŸºäºçˆ¶è·¯å¾„æ˜¯å¦å·²å¤„ç†ï¼‰
                                parent_processed_ts = current_path in self.processed_troubleshooting_paths if current_path else False
                                should_skip_ts = self._should_skip_troubleshooting_for_subcategory(subcat_node, parent_processed_ts)
                                processed_subcat = self.deep_crawl_product_content(subcat_node, skip_troubleshooting=should_skip_ts)
                                if processed_subcat:
                                    subcategory_children.append(processed_subcat)

                            if subcategory_children:
                                node['children'] = subcategory_children
                                print(f"  æˆåŠŸå¤„ç† {len(subcategory_children)} ä¸ªå­ç±»åˆ«")

                    # ä¿æŒç°æœ‰çš„childrenç»“æ„ï¼Œä¸è¦åˆ é™¤å±‚çº§å…³ç³»

                    print(f"  æ€»è®¡: {len(guides_data)} ä¸ªæŒ‡å—, {len(troubleshooting_data)} ä¸ªæ•…éšœæ’é™¤")

            except Exception as e:
                print(f"  âœ— æ·±å…¥çˆ¬å–æ—¶å‡ºé”™: {str(e)}")

            # å³ä½¿æ˜¯ç›®æ ‡é¡µé¢ï¼Œä¹Ÿè¦å¤„ç†å…¶å­èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
            # è¿™ç¡®ä¿äº†å³ä½¿æ˜¯æœ‰å†…å®¹çš„é¡µé¢ï¼Œå…¶å­ç±»åˆ«ä¹Ÿä¼šè¢«é€’å½’å¤„ç†
            if 'children' in node and node['children'] and len(node['children']) > 0:
                children_count = len(node['children'])
                print(f"ğŸ”„ é€’å½’å¤„ç†ç›®æ ‡é¡µé¢çš„ {children_count} ä¸ªå­èŠ‚ç‚¹...")

                # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å·²å¤„ç†troubleshooting
                current_url = node.get('url', '')
                current_path = current_url.split('/Device/')[-1].split('?')[0].rstrip('/') if '/Device/' in current_url else ''
                child_should_skip_troubleshooting = (
                    skip_troubleshooting or
                    (current_path and current_path in self.processed_troubleshooting_paths)
                )

                for i, child in enumerate(node['children']):
                    if children_count > 1:
                        print(f"   â””â”€ [{i+1}/{children_count}] å¤„ç†ç›®æ ‡é¡µé¢çš„å­èŠ‚ç‚¹: {child.get('name', 'Unknown')}")
                    node['children'][i] = self.deep_crawl_product_content(child, child_should_skip_troubleshooting)

        elif 'children' in node and node['children']:
            children_count = len(node['children'])
            if children_count > 0:
                print(f"ğŸ”„ é€’å½’å¤„ç† {children_count} ä¸ªå­èŠ‚ç‚¹...")

            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å·²å¤„ç†troubleshooting
            current_url = node.get('url', '')
            current_path = current_url.split('/Device/')[-1].split('?')[0].rstrip('/') if '/Device/' in current_url else ''
            child_should_skip_troubleshooting = (
                skip_troubleshooting or
                (current_path and current_path in self.processed_troubleshooting_paths)
            )

            for i, child in enumerate(node['children']):
                if children_count > 1:
                    print(f"   â””â”€ [{i+1}/{children_count}] å¤„ç†å­èŠ‚ç‚¹: {child.get('name', 'Unknown')}")
                node['children'][i] = self.deep_crawl_product_content(child, child_should_skip_troubleshooting)

        return node
        
    def _check_target_completeness(self, target_name):
        """æ£€æŸ¥ç›®æ ‡äº§å“ç›®å½•æ˜¯å¦å·²å­˜åœ¨å®Œæ•´çš„æ–‡ä»¶ç»“æ„"""
        if not target_name:
            return False, None

        safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
        target_dir = Path(self.storage_root) / "Device" / safe_name

        if not target_dir.exists():
            return False, target_dir

        # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶ç»“æ„
        info_file = target_dir / "info.json"
        if not info_file.exists():
            return False, target_dir

        # æ£€æŸ¥æ˜¯å¦æœ‰guidesæˆ–troubleshootingç›®å½•
        guides_dir = target_dir / "guides"
        troubleshooting_dir = target_dir / "troubleshooting"

        has_content = False
        if guides_dir.exists() and list(guides_dir.glob("*.json")):
            has_content = True
        if troubleshooting_dir.exists() and list(troubleshooting_dir.glob("*.json")):
            has_content = True

        return has_content, target_dir

    def _get_target_root_dir(self, target_url):
        """ä»ç›®æ ‡URLè·å–çœŸå®çš„è®¾å¤‡è·¯å¾„ï¼Œç›´æ¥ä»ä¸»è¦è®¾å¤‡ç±»å‹å¼€å§‹"""
        if not target_url:
            return None

        # ä»URLä¸­æå–Deviceåçš„è·¯å¾„
        if '/Device/' in target_url:
            # æå–Deviceåé¢çš„è·¯å¾„éƒ¨åˆ†
            device_path = target_url.split('/Device/')[-1]
            # ç§»é™¤æŸ¥è¯¢å‚æ•°
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            # ç§»é™¤æœ«å°¾çš„æ–œæ 
            device_path = device_path.rstrip('/')

            if device_path:
                # URLè§£ç 
                import urllib.parse
                device_path = urllib.parse.unquote(device_path)

                # è§£æè·¯å¾„å±‚çº§ï¼Œç›´æ¥ä»ä¸»è¦è®¾å¤‡ç±»å‹å¼€å§‹
                path_parts = device_path.split('/')
                if len(path_parts) > 0:
                    # æ‰¾åˆ°ä¸»è¦è®¾å¤‡ç±»å‹ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼‰
                    main_category = path_parts[0]

                    # å¦‚æœæœ‰å­è·¯å¾„ï¼Œä¿ç•™å®Œæ•´çš„å±‚çº§ç»“æ„ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
                    if len(path_parts) > 1:
                        sub_path = '/'.join(path_parts[1:])
                        return Path(self.storage_root) / "Device" / main_category / sub_path
                    else:
                        return Path(self.storage_root) / "Device" / main_category
                else:
                    return Path(self.storage_root) / "Device" / device_path
            else:
                # å¦‚æœæ˜¯æ ¹Deviceé¡µé¢ï¼Œä½¿ç”¨Deviceä½œä¸ºæ ¹ç›®å½•
                return Path(self.storage_root) / "Device"

        # å¦‚æœä¸æ˜¯Device URLï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ä½œä¸ºåå¤‡ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
        safe_name = target_url.replace("/", "_").replace("\\", "_").replace(":", "_")
        return Path(self.storage_root) / "Device" / safe_name

    def _validate_path_structure(self, url):
        """éªŒè¯å¹¶æå–çœŸå®çš„é¡µé¢è·¯å¾„ç»“æ„ - é€šç”¨åŠ¨æ€æ–¹æ³•"""
        print(f"   ğŸ” éªŒè¯è·¯å¾„ç»“æ„: {url}")

        # è·å–é¡µé¢å†…å®¹ä»¥æå–é¢åŒ…å±‘å¯¼èˆª
        soup = self.get_soup(url)
        if not soup:
            return None

        # é¦–å…ˆå°è¯•æå–é¢åŒ…å±‘å¯¼èˆª
        breadcrumbs = self._extract_real_breadcrumbs(soup)
        if breadcrumbs:
            print(f"   ğŸ“ æ‰¾åˆ°é¢åŒ…å±‘å¯¼èˆª: {' > '.join([b['name'] for b in breadcrumbs])}")
            return breadcrumbs

        # å¦‚æœæ²¡æœ‰é¢åŒ…å±‘ï¼Œé€šè¿‡åŠ¨æ€æ–¹å¼æ„å»ºè·¯å¾„ç»“æ„
        return self._build_dynamic_breadcrumbs(url)

    def _build_dynamic_breadcrumbs(self, url):
        """åŠ¨æ€æ„å»ºé¢åŒ…å±‘å¯¼èˆª - ä¸ä¾èµ–ç¡¬ç¼–ç è·¯å¾„"""
        if '/Device/' not in url:
            return None

        device_path = url.split('/Device/')[-1]
        if '?' in device_path:
            device_path = device_path.split('?')[0]
        device_path = device_path.rstrip('/')

        if not device_path:
            return [{"name": "Device", "url": self.base_url + "/Device"}]

        import urllib.parse
        device_path = urllib.parse.unquote(device_path)

        # åˆ†è§£è·¯å¾„ä¸ºå„ä¸ªéƒ¨åˆ†
        path_parts = device_path.split('/')
        breadcrumbs = [{"name": "Device", "url": self.base_url + "/Device"}]

        # åŠ¨æ€æ„å»ºæ¯ä¸€çº§çš„é¢åŒ…å±‘
        current_path = "/Device"
        for i, part in enumerate(path_parts):
            if not part:
                continue

            current_path += "/" + urllib.parse.quote(part, safe='')
            current_url = self.base_url + current_path

            # å°è¯•ä»è¯¥çº§åˆ«é¡µé¢æå–çœŸå®åç§°
            real_name = self._get_real_category_name(current_url, part)
            if real_name:
                breadcrumbs.append({
                    "name": real_name,
                    "url": current_url
                })
            else:
                # å¦‚æœæ— æ³•è·å–çœŸå®åç§°ï¼Œä½¿ç”¨å¤„ç†åçš„è·¯å¾„åç§°
                display_name = part.replace('_', ' ').replace('%22', '"')
                breadcrumbs.append({
                    "name": display_name,
                    "url": current_url
                })

        print(f"   ğŸ“ åŠ¨æ€æ„å»ºé¢åŒ…å±‘: {' > '.join([b['name'] for b in breadcrumbs])}")
        return breadcrumbs

    def _get_real_category_name(self, url, fallback_name):
        """ä»é¡µé¢è·å–çœŸå®çš„ç±»åˆ«åç§°"""
        try:
            soup = self.get_soup(url)
            if not soup:
                return None

            # å°è¯•ä»é¡µé¢æ ‡é¢˜è·å–åç§°
            title_elem = soup.find('h1')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text and title_text != "Device":
                    return title_text

            # å°è¯•ä»é¢åŒ…å±‘è·å–åç§°
            breadcrumb_elems = soup.select('[data-testid="breadcrumb"] a, .breadcrumb a')
            for elem in breadcrumb_elems:
                if elem.get('href', '').endswith(url.split(self.base_url)[-1]):
                    return elem.get_text(strip=True)

        except Exception as e:
            print(f"   âš ï¸  è·å–ç±»åˆ«åç§°å¤±è´¥: {e}")

        return None


    def _extract_real_breadcrumbs(self, soup):
        """ä»é¡µé¢æå–çœŸå®çš„é¢åŒ…å±‘å¯¼èˆª"""
        breadcrumbs = []

        # æ–¹æ³•1: æŸ¥æ‰¾æ–°ç‰ˆChakra UIé¢åŒ…å±‘å¯¼èˆª
        chakra_breadcrumb = soup.select_one("nav[aria-label='breadcrumb'].chakra-breadcrumb")
        if chakra_breadcrumb:
            breadcrumb_items = chakra_breadcrumb.select("li[itemtype='http://schema.org/ListItem']")
            if breadcrumb_items:
                print(f"   ğŸ“ æ‰¾åˆ°Chakra UIé¢åŒ…å±‘å¯¼èˆªï¼Œå…±{len(breadcrumb_items)}é¡¹")
                for item in breadcrumb_items:  # ä¸åå‘ï¼Œä¿æŒåŸå§‹é¡ºåº
                    # ä»data-nameå±æ€§è·å–åç§°
                    name = item.get("data-name")
                    if name:
                        # å°è¯•è·å–é“¾æ¥
                        link = item.select_one("a")
                        if link and link.get('href'):
                            href = link.get('href')
                            if '%22' in href:
                                href = href.replace('%22', '"')
                            full_url = self.base_url + href if href.startswith('/') else href
                            breadcrumbs.append({"name": name, "url": full_url})
                        else:
                            # å½“å‰é¡µé¢ï¼ˆæ²¡æœ‰é“¾æ¥ï¼‰
                            breadcrumbs.append({"name": name, "url": ""})
                    else:
                        # å¤‡é€‰ï¼šä»é“¾æ¥æ–‡æœ¬è·å–
                        link = item.select_one("a")
                        if link:
                            text = link.get_text().strip()
                            href = link.get('href')
                            if text and href:
                                if '%22' in href:
                                    href = href.replace('%22', '"')
                                full_url = self.base_url + href if href.startswith('/') else href
                                breadcrumbs.append({"name": text, "url": full_url})

                if breadcrumbs:
                    print(f"   âœ… Chakra UIé¢åŒ…å±‘æå–æˆåŠŸ: {' > '.join([b['name'] for b in breadcrumbs])}")
                    return breadcrumbs

        # æ–¹æ³•2: æŸ¥æ‰¾ä¼ ç»Ÿé¢åŒ…å±‘å¯¼èˆªå…ƒç´ 
        breadcrumb_selectors = [
            'nav[aria-label="breadcrumb"]',
            '.breadcrumb',
            '.breadcrumbs',
            '[data-testid="breadcrumb"]'
        ]

        for selector in breadcrumb_selectors:
            breadcrumb_nav = soup.select_one(selector)
            if breadcrumb_nav:
                # è·å–é¢åŒ…å±‘åˆ—è¡¨é¡¹
                breadcrumb_items = breadcrumb_nav.find_all('li')
                if breadcrumb_items:
                    print(f"   ğŸ“ æ‰¾åˆ°ä¼ ç»Ÿé¢åŒ…å±‘å¯¼èˆªï¼Œå…±{len(breadcrumb_items)}é¡¹")
                    # æå–æ‰€æœ‰é¢åŒ…å±‘é¡¹ç›®
                    temp_breadcrumbs = []
                    for item in breadcrumb_items:
                        link = item.find('a', href=True)
                        if link:
                            href = link.get('href')
                            text = link.get_text(strip=True)
                            if href and text:
                                # å¤„ç†ç‰¹æ®Šçš„URLç¼–ç 
                                if '%22' in href:
                                    href = href.replace('%22', '"')
                                full_url = self.base_url + href if href.startswith('/') else href
                                temp_breadcrumbs.append({"name": text, "url": full_url})
                        else:
                            # å½“å‰é¡µé¢ï¼ˆæ²¡æœ‰é“¾æ¥ï¼‰
                            text = item.get_text(strip=True)
                            if text and text not in [b['name'] for b in temp_breadcrumbs]:
                                temp_breadcrumbs.append({"name": text, "url": ""})

                    # iFixitçš„é¢åŒ…å±‘å¯¼èˆªæ˜¯å€’åºçš„ï¼Œéœ€è¦åå‘å¤„ç†
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åå‘ï¼šå¦‚æœæœ€åä¸€ä¸ªé¡¹ç›®åŒ…å«"è®¾å¤‡"æˆ–"Device"ï¼Œåˆ™éœ€è¦åå‘
                    if temp_breadcrumbs:
                        last_item = temp_breadcrumbs[-1]['name'].lower()
                        first_item = temp_breadcrumbs[0]['name'].lower()

                        # å¦‚æœæœ€åä¸€ä¸ªæ˜¯"è®¾å¤‡"æˆ–"device"ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªä¸æ˜¯"è®¾å¤‡"ï¼Œåˆ™åå‘
                        if ('è®¾å¤‡' in last_item or 'device' in last_item or
                            ('è®¾å¤‡' not in first_item and 'device' not in first_item)):
                            temp_breadcrumbs = list(reversed(temp_breadcrumbs))
                            print(f"   ğŸ”„ é¢åŒ…å±‘å¯¼èˆªå·²åå‘å¤„ç†")

                    breadcrumbs = temp_breadcrumbs
                    if breadcrumbs:
                        print(f"   âœ… ä¼ ç»Ÿé¢åŒ…å±‘æå–æˆåŠŸ: {' > '.join([b['name'] for b in breadcrumbs])}")
                        return breadcrumbs
                else:
                    # åå¤‡æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    links = breadcrumb_nav.find_all('a', href=True)
                    temp_breadcrumbs = []
                    for link in links:
                        href = link.get('href')
                        text = link.get_text(strip=True)
                        if href and text:
                            if '%22' in href:
                                href = href.replace('%22', '"')
                            full_url = self.base_url + href if href.startswith('/') else href
                            temp_breadcrumbs.append({"name": text, "url": full_url})

                    # æ£€æŸ¥é¡ºåº
                    if temp_breadcrumbs and temp_breadcrumbs[0]['name'].lower() != 'device':
                        temp_breadcrumbs = list(reversed(temp_breadcrumbs))

                    breadcrumbs = temp_breadcrumbs
                    if breadcrumbs:
                        print(f"   âœ… é“¾æ¥é¢åŒ…å±‘æå–æˆåŠŸ: {' > '.join([b['name'] for b in breadcrumbs])}")
                        return breadcrumbs

        return None

    def _clean_directory_name(self, name):
        """æ¸…ç†ç›®å½•åç§°ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»Ÿå…¼å®¹æ€§"""
        if not name:
            return "Unknown"

        # æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦ï¼Œä½†ä¿ç•™å¼•å·
        safe_name = str(name)
        safe_name = safe_name.replace("/", "_")
        safe_name = safe_name.replace("\\", "_")
        safe_name = safe_name.replace(":", "_")
        safe_name = safe_name.replace("*", "_")
        safe_name = safe_name.replace("?", "_")
        safe_name = safe_name.replace("<", "_")
        safe_name = safe_name.replace(">", "_")
        safe_name = safe_name.replace("|", "_")

        # ä¸æ›¿æ¢å¼•å·ï¼Œä¿æŒåŸæ ·ï¼ˆmacOSå’ŒLinuxæ”¯æŒå¼•å·åœ¨æ–‡ä»¶åä¸­ï¼‰
        # safe_name = safe_name.replace('"', '_')
        # safe_name = safe_name.replace("'", "_")

        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼ï¼Œä½†ä¿æŒå…¶ä»–å­—ç¬¦
        safe_name = safe_name.strip()
        # ä¸è¦ç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Œä¿æŒåŸå§‹æ ¼å¼
        # safe_name = "_".join([part for part in safe_name.split() if part])

        # ç¡®ä¿ä¸ä¸ºç©º
        if not safe_name:
            safe_name = "Unknown"

        return safe_name

    def _find_existing_path(self, breadcrumbs):
        """åœ¨å·²æœ‰ç›®å½•ç»“æ„ä¸­æŸ¥æ‰¾åŒ¹é…çš„è·¯å¾„"""
        if not breadcrumbs:
            return Path(self.storage_root) / "Device"

        # ä»Deviceå¼€å§‹æ„å»ºè·¯å¾„
        current_path = Path(self.storage_root) / "Device"

        # è·³è¿‡"Device"é¢åŒ…å±‘ï¼Œä»å®é™…åˆ†ç±»å¼€å§‹
        device_index = -1
        for i, crumb in enumerate(breadcrumbs):
            if crumb['name'].lower() in ['device', 'è®¾å¤‡']:
                device_index = i
                break

        if device_index >= 0:
            relevant_crumbs = breadcrumbs[device_index + 1:]
        else:
            relevant_crumbs = breadcrumbs

        # é€çº§æ£€æŸ¥å’Œåˆ›å»ºè·¯å¾„ï¼Œä½¿ç”¨çœŸå®çš„URLè·¯å¾„æ˜ å°„
        for crumb in relevant_crumbs:
            crumb_name = crumb['name']
            crumb_url = crumb.get('url', '')

            # æ ¹æ®URLç¡®å®šæ­£ç¡®çš„ç›®å½•åç§°
            if crumb_url:
                # ä»URLæå–è·¯å¾„æ®µ
                if '/Device/' in crumb_url:
                    url_segment = crumb_url.split('/Device/')[-1]
                    if url_segment:
                        # åŠ¨æ€å¤„ç†URLæ®µï¼Œä¸ä½¿ç”¨ç¡¬ç¼–ç æ˜ å°„
                        # å¤„ç†URLç¼–ç å’Œç‰¹æ®Šå­—ç¬¦
                        import urllib.parse
                        safe_name = urllib.parse.unquote(url_segment)
                        safe_name = safe_name.replace("%22", '"')
                        # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
                        safe_name = self._clean_directory_name(safe_name)
                    else:
                        # åå¤‡æ–¹æ¡ˆï¼šæ¸…ç†æ˜¾ç¤ºåç§°
                        safe_name = self._clean_directory_name(crumb_name)
                else:
                    # åå¤‡æ–¹æ¡ˆï¼šæ¸…ç†æ˜¾ç¤ºåç§°
                    safe_name = self._clean_directory_name(crumb_name)
            else:
                # æ²¡æœ‰URLï¼Œæ¸…ç†æ˜¾ç¤ºåç§°
                safe_name = self._clean_directory_name(crumb_name)

            potential_path = current_path / safe_name

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒ¹é…çš„ç›®å½•
            existing_match = self._find_matching_directory(current_path, safe_name)
            if existing_match:
                current_path = existing_match
                print(f"   âœ… æ‰¾åˆ°å·²æœ‰ç›®å½•: {existing_match}")
            else:
                current_path = potential_path
                print(f"   ğŸ“ å°†åˆ›å»ºæ–°ç›®å½•: {potential_path}")

        return current_path

    def _build_full_save_path(self, target_url, base_dir):
        """æ ¹æ®ç›®æ ‡URLæ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„"""
        if not target_url or '/Device/' not in target_url:
            return base_dir

        # æå–è®¾å¤‡è·¯å¾„
        device_path = target_url.split('/Device/')[-1]
        if '?' in device_path:
            device_path = device_path.split('?')[0]
        device_path = device_path.rstrip('/')

        if not device_path:
            return base_dir

        import urllib.parse
        device_path = urllib.parse.unquote(device_path)

        # æ„å»ºå®Œæ•´è·¯å¾„
        path_parts = device_path.split('/')
        full_path = Path(self.storage_root) / "Device"

        for part in path_parts:
            if part:
                # æ¸…ç†è·¯å¾„éƒ¨åˆ†
                safe_part = part.replace("/", "_").replace("\\", "_").replace(":", "_")
                safe_part = safe_part.replace('"', '"').replace("'", "_").replace("?", "_")
                safe_part = safe_part.replace("<", "_").replace(">", "_").replace("|", "_")
                safe_part = safe_part.replace("*", "_").strip()
                full_path = full_path / safe_part

        return full_path



    def _troubleshooting_belongs_to_current_page(self, troubleshooting_list, current_url):
        """æ£€æŸ¥troubleshootingæ˜¯å¦çœŸçš„å±äºå½“å‰é¡µé¢"""
        if not troubleshooting_list or not current_url:
            return False

        # æå–å½“å‰é¡µé¢çš„è®¾å¤‡è·¯å¾„
        if '/Device/' not in current_url:
            return False

        current_device_path = current_url.split('/Device/')[-1]
        if '?' in current_device_path:
            current_device_path = current_device_path.split('?')[0]
        current_device_path = current_device_path.rstrip('/')

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å…·ä½“çš„è®¾å¤‡é¡µé¢ï¼ˆåŒ…å«å…·ä½“å‹å·ä¿¡æ¯ï¼‰
        if not self._is_specific_device_page(current_device_path):
            return False

        # å¯¹äºå…·ä½“çš„è®¾å¤‡é¡µé¢ï¼Œå¦‚æœæœ‰ troubleshooting æ•°æ®ï¼Œå°±è®¤ä¸ºå±äºå½“å‰é¡µé¢
        # è¿™æ˜¯å› ä¸ºè¿™äº›æ•°æ®æ˜¯ä»å½“å‰é¡µé¢æå–çš„ï¼Œå³ä½¿æ˜¯é€šç”¨çš„ troubleshooting
        if troubleshooting_list:
            return True

        # ä»¥ä¸‹æ˜¯åŸæ¥çš„ä¸¥æ ¼æ£€æŸ¥é€»è¾‘ï¼Œä½œä¸ºå¤‡ç”¨
        # æ£€æŸ¥troubleshootingçš„URLæ˜¯å¦ä¸å½“å‰é¡µé¢ç›¸å…³
        for ts in troubleshooting_list:
            ts_url = ts.get('url', '')
            ts_title = ts.get('title', '')

            if not ts_url and not ts_title:
                continue

            # æ–¹æ³•1: æ£€æŸ¥troubleshooting URLæ˜¯å¦åŒ…å«å½“å‰è®¾å¤‡è·¯å¾„
            if ts_url and current_device_path in ts_url:
                print(f"   âœ… Troubleshooting URLåŒ¹é…å½“å‰è®¾å¤‡è·¯å¾„")
                return True

            # æ–¹æ³•2: æ£€æŸ¥troubleshootingæ ‡é¢˜æ˜¯å¦åŒ…å«å½“å‰è®¾å¤‡ä¿¡æ¯
            if ts_title:
                # ä»å½“å‰è®¾å¤‡è·¯å¾„æå–å…³é”®ä¿¡æ¯
                device_keywords = self._extract_device_keywords(current_device_path)
                title_lower = ts_title.lower()

                # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨æ ‡é¢˜ä¸­
                matches = 0
                for keyword in device_keywords:
                    if keyword.lower() in title_lower:
                        matches += 1

                # å¦‚æœå¤§éƒ¨åˆ†å…³é”®è¯éƒ½åŒ¹é…ï¼Œè®¤ä¸ºå±äºå½“å‰é¡µé¢
                if matches >= len(device_keywords) * 0.6:  # 60%çš„å…³é”®è¯åŒ¹é…
                    print(f"   âœ… Troubleshootingæ ‡é¢˜åŒ¹é…å½“å‰è®¾å¤‡: {matches}/{len(device_keywords)} å…³é”®è¯åŒ¹é…")
                    return True

        print(f"   âŒ Troubleshootingä¸å±äºå½“å‰é¡µé¢")
        return False

    def _is_specific_device_page(self, device_path):
        """æ£€æŸ¥æ˜¯å¦æ˜¯å…·ä½“çš„è®¾å¤‡é¡µé¢ï¼ˆåŒ…å«å…·ä½“å‹å·ä¿¡æ¯ï¼‰"""
        if not device_path:
            return False

        # é€šç”¨ç±»åˆ«é¡µé¢ï¼ˆä¸æ˜¯å…·ä½“è®¾å¤‡ï¼‰
        generic_categories = [
            'Mac', 'Mac_Laptop', 'Mac_Desktop', 'Mac_Hardware',
            'iPhone', 'iPad', 'iPod', 'Apple_Watch',
            'MacBook', 'MacBook_Pro', 'MacBook_Air',
            'iMac', 'Mac_Pro', 'Mac_mini'
        ]

        # å¦‚æœè·¯å¾„å°±æ˜¯è¿™äº›é€šç”¨ç±»åˆ«ä¹‹ä¸€ï¼Œä¸æ˜¯å…·ä½“è®¾å¤‡
        if device_path in generic_categories:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„å‹å·ä¿¡æ¯
        specific_indicators = [
            # å‹å·ç¼–å·
            'A1', 'A2', 'A3',  # Appleå‹å·ç¼–å·
            'Models_', 'Model_',
            # å…·ä½“è§„æ ¼
            '13"', '15"', '17"', '21.5"', '27"',
            'Unibody', 'Retina', 'Touch_Bar',
            # å¹´ä»½
            '2020', '2021', '2022', '2023', '2024',
            # ä»£æ•°
            '1st_Gen', '2nd_Gen', '3rd_Gen', '4th_Gen', '5th_Gen',
            'Generation'
        ]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æŒ‡æ ‡
        for indicator in specific_indicators:
            if indicator in device_path:
                return True

        # æ£€æŸ¥è·¯å¾„æ·±åº¦ï¼ˆå…·ä½“è®¾å¤‡é€šå¸¸è·¯å¾„æ›´æ·±ï¼‰
        path_depth = len(device_path.split('/'))
        if path_depth >= 2:  # è‡³å°‘ä¸¤çº§æ·±åº¦ï¼Œå¦‚ MacBook_Pro/MacBook_Pro_17"
            return True

        return False

    def _extract_device_keywords(self, device_path):
        """ä»è®¾å¤‡è·¯å¾„æå–å…³é”®è¯"""
        if not device_path:
            return []

        keywords = []

        # åˆ†å‰²è·¯å¾„
        parts = device_path.replace('_', ' ').split('/')
        for part in parts:
            # è¿›ä¸€æ­¥åˆ†å‰²æ¯ä¸ªéƒ¨åˆ†
            sub_parts = part.split()
            keywords.extend(sub_parts)

        # æ¸…ç†å’Œè¿‡æ»¤å…³é”®è¯
        filtered_keywords = []
        for keyword in keywords:
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            clean_keyword = keyword.replace('"', '').replace("'", '').strip()
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯å’Œå¸¸è§è¯
            if len(clean_keyword) >= 2 and clean_keyword.lower() not in ['and', 'or', 'the', 'of', 'in']:
                filtered_keywords.append(clean_keyword)

        return filtered_keywords

    def _is_troubleshooting_processed_in_parent_path(self, current_url):
        """æ£€æŸ¥å½“å‰URLçš„çˆ¶è·¯å¾„æ˜¯å¦å·²ç»å¤„ç†è¿‡troubleshooting"""
        if not current_url or '/Device/' not in current_url:
            return False

        current_path = current_url.split('/Device/')[-1].split('?')[0].rstrip('/')

        # æ£€æŸ¥æ‰€æœ‰å·²å¤„ç†çš„è·¯å¾„ï¼Œçœ‹æ˜¯å¦æœ‰çˆ¶è·¯å¾„
        for processed_path in self.processed_troubleshooting_paths:
            if current_path.startswith(processed_path + '/') or current_path == processed_path:
                return True
        return False

    def _should_skip_troubleshooting_for_subcategory(self, subcat_node, parent_processed_troubleshooting=False):
        """åˆ¤æ–­å­ç±»åˆ«æ˜¯å¦åº”è¯¥è·³è¿‡troubleshootingæå–"""
        if not subcat_node:
            return True

        # å¦‚æœçˆ¶çº§å·²ç»å¤„ç†è¿‡troubleshootingï¼Œåˆ™è·³è¿‡
        if parent_processed_troubleshooting:
            return True

        url = subcat_node.get('url', '')
        if not url or '/Device/' not in url:
            return True

        # æ£€æŸ¥æ˜¯å¦åœ¨çˆ¶è·¯å¾„ä¸­å·²ç»å¤„ç†è¿‡
        if self._is_troubleshooting_processed_in_parent_path(url):
            device_path = url.split('/Device/')[-1].split('?')[0].rstrip('/')
            print(f"      â­ï¸  è·³è¿‡troubleshootingï¼ˆå·²åœ¨çˆ¶è·¯å¾„å¤„ç†ï¼‰: {device_path}")
            return True

        return False

    def _find_matching_directory(self, parent_path, target_name):
        """åœ¨çˆ¶ç›®å½•ä¸­æŸ¥æ‰¾åŒ¹é…çš„å­ç›®å½•"""
        if not parent_path.exists():
            return None

        target_name_lower = target_name.lower()

        for item in parent_path.iterdir():
            if item.is_dir():
                item_name_lower = item.name.lower()
                # æ£€æŸ¥å®Œå…¨åŒ¹é…æˆ–ç›¸ä¼¼åŒ¹é…
                if (item_name_lower == target_name_lower or
                    target_name_lower in item_name_lower or
                    item_name_lower in target_name_lower):
                    return item

        return None

    def _build_path_from_tree_structure(self, tree_data):
        """ä»æ ‘ç»“æ„ä¸­æ„å»ºæ­£ç¡®çš„æ–‡ä»¶å¤¹è·¯å¾„"""
        if not tree_data:
            return None

        # æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹ï¼ˆåŒ…å«æŒ‡å—å’Œæ•…éšœæ’é™¤çš„èŠ‚ç‚¹ï¼‰
        target_node = self._find_target_node_in_tree(tree_data)
        if not target_node:
            return None

        # æ„å»ºä»æ ¹åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è·¯å¾„
        path_segments = self._build_path_segments_to_node(tree_data, target_node)
        if not path_segments:
            return None

        # æ„å»ºæ–‡ä»¶ç³»ç»Ÿè·¯å¾„
        root_dir = Path(self.storage_root)
        for segment in path_segments:
            safe_name = self._clean_directory_name(segment)
            root_dir = root_dir / safe_name

        print(f"   ğŸ—‚ï¸  ä»æ ‘ç»“æ„æ„å»ºè·¯å¾„: {' > '.join(path_segments)}")
        print(f"   ğŸ“ æ–‡ä»¶ç³»ç»Ÿè·¯å¾„: {root_dir}")

        return root_dir

    def _find_target_node_in_tree(self, node):
        """åœ¨æ ‘ä¸­æŸ¥æ‰¾åŒ…å«æŒ‡å—å’Œæ•…éšœæ’é™¤çš„ç›®æ ‡èŠ‚ç‚¹"""
        if not node or not isinstance(node, dict):
            return None

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦åŒ…å«æŒ‡å—æˆ–æ•…éšœæ’é™¤
        if (node.get('guides') and len(node.get('guides', [])) > 0) or \
           (node.get('troubleshooting') and len(node.get('troubleshooting', [])) > 0):
            return node

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        children = node.get('children', [])
        for child in children:
            result = self._find_target_node_in_tree(child)
            if result:
                return result

        return None

    def _build_path_segments_to_node(self, tree_root, target_node, current_path=None):
        """æ„å»ºä»æ ¹èŠ‚ç‚¹åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è·¯å¾„æ®µ"""
        if current_path is None:
            current_path = []

        if not tree_root or not isinstance(tree_root, dict):
            return None

        # æ·»åŠ å½“å‰èŠ‚ç‚¹åç§°åˆ°è·¯å¾„
        node_name = tree_root.get('name', '')
        if node_name:
            current_path.append(node_name)

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹
        if tree_root == target_node:
            return current_path.copy()

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        children = tree_root.get('children', [])
        for child in children:
            result = self._build_path_segments_to_node(child, target_node, current_path.copy())
            if result:
                return result

        return None

    def save_combined_result(self, tree_data, target_name=None):
        """ä¿å­˜æ•´åˆç»“æœåˆ°çœŸå®çš„è®¾å¤‡è·¯å¾„ç»“æ„"""
        print("   ğŸ“ åˆ›å»ºç›®å½•ç»“æ„...")

        # ä»æ ‘ç»“æ„ä¸­æ„å»ºæ­£ç¡®çš„è·¯å¾„
        root_dir = self._build_path_from_tree_structure(tree_data)

        # å¦‚æœæ²¡æœ‰ä»æ ‘ç»“æ„æ„å»ºè·¯å¾„ï¼Œä½¿ç”¨åå¤‡æ–¹æ¡ˆ
        if not root_dir:
            target_url = getattr(self, 'target_url', None)
            if target_url:
                # éªŒè¯è·¯å¾„ç»“æ„
                breadcrumbs = self._validate_path_structure(target_url)
                if breadcrumbs:
                    # åœ¨å·²æœ‰ç»“æ„ä¸­æŸ¥æ‰¾æ­£ç¡®è·¯å¾„
                    root_dir = self._find_existing_path(breadcrumbs)
                else:
                    # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨URLè§£æ
                    if '/Device/' in target_url:
                        device_path = target_url.split('/Device/')[-1]
                        if '?' in device_path:
                            device_path = device_path.split('?')[0]
                        device_path = device_path.rstrip('/')

                        if device_path:
                            import urllib.parse
                            device_path = urllib.parse.unquote(device_path)

                            # æ„å»ºè·¯å¾„ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
                            path_parts = device_path.split('/')
                            root_dir = Path(self.storage_root) / "Device"
                            for part in path_parts:
                                safe_part = part.replace("/", "_").replace("\\", "_").replace(":", "_")
                                safe_part = safe_part.replace('"', '_').replace("'", "_")
                                root_dir = root_dir / safe_part
                        else:
                            # å¦‚æœæ˜¯æ ¹Deviceé¡µé¢ï¼Œä½¿ç”¨Deviceä½œä¸ºæ ¹ç›®å½•
                            root_dir = Path(self.storage_root) / "Device"
                    else:
                        root_dir = Path(self.storage_root) / "Device"
            elif target_name:
                # å¦‚æœæ²¡æœ‰URLï¼Œå°è¯•ä»target_nameæ„å»º
                if target_name.startswith('http'):
                    # éªŒè¯URLçš„è·¯å¾„ç»“æ„
                    breadcrumbs = self._validate_path_structure(target_name)
                    if breadcrumbs:
                        root_dir = self._find_existing_path(breadcrumbs)
                    else:
                        root_dir = self._get_target_root_dir(target_name)
                else:
                    # å‡è®¾æ˜¯è®¾å¤‡åç§°ï¼Œæ”¾åœ¨Deviceç›®å½•ä¸‹
                    safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                    safe_name = safe_name.replace('"', '_').replace("'", "_")
                    root_dir = Path(self.storage_root) / "Device" / safe_name
            else:
                # ä»æ ‘æ•°æ®ä¸­æå–è·¯å¾„ä¿¡æ¯
                root_name = tree_data.get("name", "Unknown")
                if " > " in root_name:
                    root_name = root_name.split(" > ")[-1]
                safe_name = root_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                safe_name = safe_name.replace('"', '_').replace("'", "_")
                root_dir = Path(self.storage_root) / "Device" / safe_name

        # ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶ä¿å­˜å†…å®¹
        if root_dir:
            root_dir.mkdir(parents=True, exist_ok=True)
            print(f"   ğŸ“‚ ç›®æ ‡è·¯å¾„: {root_dir}")

            # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹å¹¶ç›´æ¥ä¿å­˜å…¶å†…å®¹ï¼Œè€Œä¸æ˜¯ä¿å­˜æ•´ä¸ªæ ‘ç»“æ„
            print("   ğŸ’¾ ä¿å­˜å†…å®¹å’Œä¸‹è½½åª’ä½“æ–‡ä»¶...")
            print(f"   ğŸ” å¼€å§‹ä¿å­˜åˆ°: {root_dir}")
            print(f"   ğŸ¯ ç›®æ ‡URL: {getattr(self, 'target_url', 'None')}")

            # æ‰¾åˆ°æ‰€æœ‰åŒ…å«å®é™…å†…å®¹çš„èŠ‚ç‚¹
            content_nodes = self._find_all_content_nodes_in_tree(tree_data)
            if content_nodes:
                print(f"   ğŸ¯ æ‰¾åˆ° {len(content_nodes)} ä¸ªå†…å®¹èŠ‚ç‚¹")

                # ç»Ÿè®¡æ€»çš„guideså’Œtroubleshootingæ•°é‡
                total_guides = sum(len(node.get('guides', [])) for node in content_nodes)
                total_troubleshooting = sum(len(node.get('troubleshooting', [])) for node in content_nodes)
                print(f"   ğŸ“– æ€»æŒ‡å—æ•°é‡: {total_guides}")
                print(f"   ğŸ”§ æ€»æ•…éšœæ’é™¤æ•°é‡: {total_troubleshooting}")

                # ä¿å­˜æ¯ä¸ªå†…å®¹èŠ‚ç‚¹
                for i, node in enumerate(content_nodes):
                    node_name = node.get('name', f'node_{i+1}')
                    guides_count = len(node.get('guides', []))
                    troubleshooting_count = len(node.get('troubleshooting', []))
                    print(f"   ğŸ“¦ [{i+1}/{len(content_nodes)}] ä¿å­˜èŠ‚ç‚¹: {node_name}")
                    print(f"       ğŸ“– æŒ‡å—: {guides_count} ä¸ª, ğŸ”§ æ•…éšœæ’é™¤: {troubleshooting_count} ä¸ª")

                    if i == 0:
                        # ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¿å­˜åˆ°æ ¹ç›®å½•
                        self._save_node_content(node, root_dir)
                    else:
                        # å…¶ä»–èŠ‚ç‚¹ä¿å­˜åˆ°å­ç›®å½•
                        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_").replace('"', '')
                        node_dir = root_dir / safe_name
                        self._save_node_content(node, node_dir)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹ï¼Œä¿å­˜æ•´ä¸ªæ ‘ç»“æ„ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
                self._save_tree_structure(tree_data, root_dir)

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        print(f"\nğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        self._validate_data_completeness(tree_data, root_dir)

        print(f"\nğŸ“ æ•´åˆç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {root_dir}")
        return str(root_dir)

    def _save_tree_structure(self, tree_data, base_dir, current_path_parts=None):
        """ä¿å­˜æ•´ä¸ªæ ‘ç»“æ„åˆ°æŒ‡å®šç›®å½•ï¼Œä¿®å¤è·¯å¾„é‡å¤é—®é¢˜"""
        if not tree_data or not isinstance(tree_data, dict):
            print(f"   âš ï¸  æ— æ•ˆçš„æ ‘æ•°æ®: {type(tree_data)}")
            return

        # åˆå§‹åŒ–è·¯å¾„éƒ¨åˆ†åˆ—è¡¨
        if current_path_parts is None:
            current_path_parts = []

        node_name = tree_data.get('name', 'Unknown')
        current_url = tree_data.get('url', '')

        print(f"   ğŸ” å¤„ç†èŠ‚ç‚¹: {node_name}")
        print(f"      èŠ‚ç‚¹URL: {current_url}")
        node_type = 'äº§å“' if tree_data.get('guides') or tree_data.get('troubleshooting') else 'åˆ†ç±»'
        print(f"      æŒ‡å—æ•°é‡: {len(tree_data.get('guides', []))}")
        print(f"      æ•…éšœæ’é™¤æ•°é‡: {len(tree_data.get('troubleshooting', []))}")

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å®é™…å†…å®¹éœ€è¦ä¿å­˜
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0
        has_content = has_guides or has_troubleshooting

        if has_content:
            print(f"   âœ… å‘ç°å†…å®¹èŠ‚ç‚¹: {node_name}")
            print(f"      æŒ‡å—æ•°é‡: {len(tree_data.get('guides', []))}")
            print(f"      æ•…éšœæ’é™¤æ•°é‡: {len(tree_data.get('troubleshooting', []))}")
            print(f"      ä¿å­˜è·¯å¾„: {base_dir}")

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            base_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜èŠ‚ç‚¹å†…å®¹åˆ°å½“å‰ç›®å½•ï¼Œä¸å†åˆ›å»ºé‡å¤çš„å­ç›®å½•
            print(f"   ğŸ’¾ å¼€å§‹ä¿å­˜èŠ‚ç‚¹å†…å®¹...")
            self._save_node_content(tree_data, base_dir)
            print(f"   âœ… èŠ‚ç‚¹å†…å®¹ä¿å­˜å®Œæˆ")

        # å¤„ç†å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            print(f"   ğŸŒ³ å¤„ç† {len(tree_data['children'])} ä¸ªå­èŠ‚ç‚¹...")
            for child in tree_data['children']:
                child_name = child.get('name', 'Unknown')

                # æ¸…ç†å­èŠ‚ç‚¹åç§°ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦
                safe_name = self._clean_directory_name(child_name)
                child_dir = base_dir / safe_name

                # æ£€æŸ¥å­èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
                child_has_content = (child.get('guides') and len(child.get('guides', [])) > 0) or \
                                  (child.get('troubleshooting') and len(child.get('troubleshooting', [])) > 0)

                # æ£€æŸ¥å­èŠ‚ç‚¹æ˜¯å¦æœ‰çœŸæ­£çš„å­èŠ‚ç‚¹ï¼ˆéç©ºçš„childrenæ•°ç»„ï¼‰
                child_has_children = child.get('children') and len(child.get('children', [])) > 0

                # ä¸ºæ‰€æœ‰å­èŠ‚ç‚¹åˆ›å»ºç›®å½•ï¼ŒåŒ…æ‹¬å¶å­èŠ‚ç‚¹ï¼ˆå³ä½¿æ²¡æœ‰å†…å®¹ï¼‰
                # è¿™ç¡®ä¿äº†æ ‘ç»“æ„çš„å®Œæ•´æ€§ï¼Œå³ä½¿æŸäº›å¶å­èŠ‚ç‚¹æš‚æ—¶æ²¡æœ‰guidesæˆ–troubleshooting
                print(f"      å­èŠ‚ç‚¹ '{child_name}': å†…å®¹={child_has_content}, å­èŠ‚ç‚¹={child_has_children}, URL={bool(child.get('url'))}")

                if child_has_content or child_has_children or child.get('url'):
                    try:
                        if not child_dir.exists():
                            child_dir.mkdir(parents=True, exist_ok=True)
                            print(f"   ğŸ“ åˆ›å»ºå­èŠ‚ç‚¹ç›®å½•: {child_dir}")
                            # è®°å½•ç›®å½•åç§°å¤„ç†è¿‡ç¨‹ï¼Œå¸®åŠ©è°ƒè¯•
                            print(f"      åŸå§‹åç§°: '{child_name}'")
                            print(f"      æ¸…ç†ååç§°: '{safe_name}'")
                        else:
                            print(f"   ğŸ“ ä½¿ç”¨å·²å­˜åœ¨çš„å­èŠ‚ç‚¹ç›®å½•: {child_dir}")

                        # é€’å½’å¤„ç†å­èŠ‚ç‚¹ï¼Œä½¿ç”¨å­èŠ‚ç‚¹çš„ç›®å½•
                        self._save_tree_structure(child, child_dir, current_path_parts + [node_name])
                    except Exception as e:
                        # å¢å¼ºé”™è¯¯å¤„ç†ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯ä½†ä¸ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
                        print(f"   âŒ åˆ›å»ºæˆ–å¤„ç†ç›®å½•å¤±è´¥: {child_dir}")
                        print(f"      é”™è¯¯ä¿¡æ¯: {str(e)}")
                        print(f"      å­èŠ‚ç‚¹åç§°: '{child_name}'")
                        print(f"      æ¸…ç†ååç§°: '{safe_name}'")
                        # å°è¯•ä½¿ç”¨å¤‡ç”¨åç§°
                        try:
                            backup_name = f"Category_{hash(child_name) % 10000:04d}"
                            backup_dir = base_dir / backup_name
                            print(f"      å°è¯•ä½¿ç”¨å¤‡ç”¨åç§°: '{backup_name}'")
                            backup_dir.mkdir(parents=True, exist_ok=True)
                            self._save_tree_structure(child, backup_dir, current_path_parts + [node_name])
                        except Exception as backup_error:
                            print(f"      å¤‡ç”¨åç§°ä¹Ÿå¤±è´¥: {str(backup_error)}")
                else:
                    # åªæœ‰å½“å­èŠ‚ç‚¹å®Œå…¨æ²¡æœ‰ä»»ä½•æœ‰ç”¨ä¿¡æ¯æ—¶æ‰è·³è¿‡
                    print(f"   â­ï¸  è·³è¿‡æ— æ•ˆå­èŠ‚ç‚¹: {child_name} (æ— å†…å®¹ã€æ— å­èŠ‚ç‚¹ã€æ— URL)")

    def _clean_directory_name(self, name):
        """æ¸…ç†ç›®å½•åç§°ï¼Œç§»é™¤æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦"""
        if not name:
            return "Unknown"

        # æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
        safe_name = name.replace("/", "_").replace("\\", "_").replace(":", "_")
        safe_name = safe_name.replace('"', '_').replace("'", "_").replace("?", "_")
        safe_name = safe_name.replace("<", "_").replace(">", "_").replace("|", "_")
        safe_name = safe_name.replace("*", "_").replace("(", "_").replace(")", "_")
        safe_name = safe_name.strip()

        # ç§»é™¤å¤šä½™çš„ä¸‹åˆ’çº¿
        while "__" in safe_name:
            safe_name = safe_name.replace("__", "_")

        return safe_name.strip("_") or "Unknown"

    def _validate_data_completeness(self, tree_data, root_dir):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œæ£€æŸ¥æ‰€æœ‰ç±»åˆ«æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„ç›®å½•"""
        missing_categories = []
        total_categories = 0
        existing_directories = 0

        def check_node(node, current_path=""):
            nonlocal missing_categories, total_categories, existing_directories

            if not node or not isinstance(node, dict):
                return

            node_name = node.get('name', 'Unknown')
            node_url = node.get('url', '')

            # æ£€æŸ¥æ˜¯å¦æœ‰å­èŠ‚ç‚¹
            children = node.get('children', [])
            if children:
                total_categories += len(children)

                for child in children:
                    child_name = child.get('name', 'Unknown')
                    safe_name = self._clean_directory_name(child_name)

                    # æ„å»ºé¢„æœŸçš„ç›®å½•è·¯å¾„
                    if current_path:
                        expected_dir = root_dir / current_path / safe_name
                    else:
                        expected_dir = root_dir / safe_name

                    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                    if expected_dir.exists():
                        existing_directories += 1
                        print(f"   âœ… ç›®å½•å­˜åœ¨: {safe_name}")
                    else:
                        missing_categories.append({
                            'name': child_name,
                            'safe_name': safe_name,
                            'url': child.get('url', ''),
                            'expected_path': str(expected_dir),
                            'has_children': bool(child.get('children')),
                            'has_content': bool(child.get('guides') or child.get('troubleshooting'))
                        })
                        print(f"   âŒ ç›®å½•ç¼ºå¤±: {safe_name} (åŸå: {child_name})")

                    # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
                    if current_path:
                        check_node(child, f"{current_path}/{safe_name}")
                    else:
                        check_node(child, safe_name)

        print(f"   æ­£åœ¨æ£€æŸ¥æ ‘ç»“æ„å®Œæ•´æ€§...")
        check_node(tree_data)

        # è¾“å‡ºéªŒè¯ç»“æœ
        print(f"\nğŸ“Š æ•°æ®å®Œæ•´æ€§éªŒè¯ç»“æœ:")
        print(f"   æ€»ç±»åˆ«æ•°: {total_categories}")
        print(f"   å·²åˆ›å»ºç›®å½•: {existing_directories}")
        print(f"   ç¼ºå¤±ç›®å½•: {len(missing_categories)}")

        if missing_categories:
            print(f"\nâŒ å‘ç° {len(missing_categories)} ä¸ªç¼ºå¤±çš„ç±»åˆ«ç›®å½•:")
            for missing in missing_categories:
                print(f"   - ç±»åˆ«: '{missing['name']}'")
                print(f"     æ¸…ç†ååç§°: '{missing['safe_name']}'")
                print(f"     é¢„æœŸè·¯å¾„: {missing['expected_path']}")
                print(f"     æœ‰å­èŠ‚ç‚¹: {missing['has_children']}")
                print(f"     æœ‰å†…å®¹: {missing['has_content']}")
                if missing['url']:
                    print(f"     URL: {missing['url']}")
                print()
        else:
            print(f"   âœ… æ‰€æœ‰ç±»åˆ«ç›®å½•éƒ½å·²æ­£ç¡®åˆ›å»º")

        return len(missing_categories) == 0

    def _save_node_content(self, node_data, node_dir):
        """ä¿å­˜å•ä¸ªèŠ‚ç‚¹çš„å†…å®¹ï¼ˆguideså’Œtroubleshootingï¼‰"""
        if not node_data or not isinstance(node_data, dict):
            return

        # åˆ›å»ºèŠ‚ç‚¹ç›®å½•
        node_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
        info_data = {k: v for k, v in node_data.items()
                    if k not in ['children', 'guides', 'troubleshooting']}

        if info_data:
            info_file = node_dir / "info.json"
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(info_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                # ä¸ºæ¯ä¸ªguideåˆ›å»ºå•ç‹¬çš„ç›®å½•ï¼Œåª’ä½“æ–‡ä»¶å’Œguideæ–‡ä»¶åœ¨åŒä¸€å±‚çº§
                guide_dir = guides_dir / f"guide_{i+1}"
                guide_dir.mkdir(exist_ok=True)

                # å¤„ç†åª’ä½“æ–‡ä»¶ï¼Œå­˜å‚¨åœ¨guideç›®å½•ä¸‹
                self._process_media_urls(guide_data, guide_dir)

                # ä¿å­˜guideæ–‡ä»¶åˆ°guideç›®å½•
                guide_file = guide_dir / "guide.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = node_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)

            for i, ts in enumerate(node_data['troubleshooting']):
                ts_data = ts.copy()
                # ä¸ºæ¯ä¸ªtroubleshootingåˆ›å»ºå•ç‹¬çš„ç›®å½•ï¼Œåª’ä½“æ–‡ä»¶å’Œtsæ–‡ä»¶åœ¨åŒä¸€å±‚çº§
                ts_item_dir = ts_dir / f"troubleshooting_{i+1}"
                ts_item_dir.mkdir(exist_ok=True)

                # å¤„ç†åª’ä½“æ–‡ä»¶ï¼Œå­˜å‚¨åœ¨troubleshootingç›®å½•ä¸‹
                self._process_media_urls(ts_data, ts_item_dir)

                # ä¿å­˜troubleshootingæ–‡ä»¶åˆ°troubleshootingç›®å½•
                ts_file = ts_item_dir / "troubleshooting.json"
                with open(ts_file, 'w', encoding='utf-8') as f:
                    json.dump(ts_data, f, ensure_ascii=False, indent=2)

            # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
            if self.cache_manager:
                current_url = node_data.get('url', '')
                if current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)
                    print(f"   ğŸ’¾ å·²ç”Ÿæˆtroubleshooting_cache.jsonæ–‡ä»¶: {node_dir / 'troubleshooting_cache.json'}")

        # æ›´æ–°ç¼“å­˜ç´¢å¼•
        self._update_cache_for_node(node_data, node_dir)

    def _find_target_node_in_tree(self, tree_data):
        """åœ¨æ ‘ç»“æ„ä¸­æ‰¾åˆ°åŒ…å«å®é™…å†…å®¹çš„ç›®æ ‡èŠ‚ç‚¹"""
        if not tree_data or not isinstance(tree_data, dict):
            return None

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0

        if has_guides or has_troubleshooting:
            return tree_data

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            for child in tree_data['children']:
                result = self._find_target_node_in_tree(child)
                if result:
                    return result

        return None

    def _find_all_content_nodes_in_tree(self, tree_data):
        """åœ¨æ ‘ç»“æ„ä¸­æ‰¾åˆ°æ‰€æœ‰åŒ…å«å®é™…å†…å®¹çš„èŠ‚ç‚¹"""
        content_nodes = []
        if not tree_data or not isinstance(tree_data, dict):
            return content_nodes

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0

        if has_guides or has_troubleshooting:
            content_nodes.append(tree_data)

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            for child in tree_data['children']:
                child_nodes = self._find_all_content_nodes_in_tree(child)
                content_nodes.extend(child_nodes)

        return content_nodes

    def _update_cache_for_node(self, node_data, node_dir):
        """ä¸ºèŠ‚ç‚¹æ›´æ–°ç¼“å­˜ç´¢å¼•ï¼Œæ­£ç¡®ç»Ÿè®¡åª’ä½“æ–‡ä»¶æ•°é‡"""
        if not self.cache_manager or not node_data:
            return

        url = node_data.get('url', '')
        if not url:
            return

        try:
            # ç»Ÿè®¡å†…å®¹æ•°é‡
            guides_count = len(node_data.get('guides', []))
            troubleshooting_count = len(node_data.get('troubleshooting', []))

            # ç»Ÿè®¡åª’ä½“æ–‡ä»¶æ•°é‡ - é€’å½’ç»Ÿè®¡æ‰€æœ‰å­ç›®å½•ä¸­çš„åª’ä½“æ–‡ä»¶
            media_count = 0

            # ç»Ÿè®¡guidesç›®å½•ä¸‹çš„åª’ä½“æ–‡ä»¶
            guides_dir = node_dir / "guides"
            if guides_dir.exists():
                for guide_subdir in guides_dir.iterdir():
                    if guide_subdir.is_dir():
                        media_subdir = guide_subdir / "media"
                        if media_subdir.exists():
                            media_count += len(list(media_subdir.glob("*")))

            # ç»Ÿè®¡troubleshootingç›®å½•ä¸‹çš„åª’ä½“æ–‡ä»¶
            ts_dir = node_dir / "troubleshooting"
            if ts_dir.exists():
                for ts_subdir in ts_dir.iterdir():
                    if ts_subdir.is_dir():
                        media_subdir = ts_subdir / "media"
                        if media_subdir.exists():
                            media_count += len(list(media_subdir.glob("*")))

            # ç»Ÿè®¡æ ¹ç›®å½•ä¸‹çš„åª’ä½“æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            root_media_dir = node_dir / "media"
            if root_media_dir.exists():
                media_count += len(list(root_media_dir.glob("*")))

            print(f"   ğŸ“Š ç»Ÿè®¡ç»“æœ - æŒ‡å—: {guides_count}, æ•…éšœæ’é™¤: {troubleshooting_count}, åª’ä½“æ–‡ä»¶: {media_count}")

            # æ·»åŠ åˆ°ç¼“å­˜
            self.cache_manager.add_to_cache(
                url, node_dir,
                guides_count=guides_count,
                troubleshooting_count=troubleshooting_count,
                media_count=media_count
            )

        except Exception as e:
            self.logger.error(f"æ›´æ–°ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def _save_target_content_to_root(self, tree_data, root_dir):
        """å°†ç›®æ ‡äº§å“å†…å®¹ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•ï¼Œä¼˜åŒ–ç›®å½•ç»“æ„"""
        if not tree_data or not isinstance(tree_data, dict):
            return

        # å¤åˆ¶æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = tree_data.copy()
        self._process_media_urls(node_data, root_dir)

        # ä¿å­˜ä¸»è¦ä¿¡æ¯åˆ°æ ¹ç›®å½•çš„info.json
        info_data = {k: v for k, v in node_data.items()
                    if k not in ['children', 'guides', 'troubleshooting']}

        info_file = root_dir / "info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides - ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•çš„guidesæ–‡ä»¶å¤¹
        if 'guides' in node_data and node_data['guides']:
            guides_dir = root_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - åªæœ‰å½“å‰èŠ‚ç‚¹æœ‰troubleshootingæ•°æ®æ—¶æ‰ä¿å­˜
        # troubleshootingåº”è¯¥ä¿å­˜åœ¨å®ƒæ‰€å±çš„å…·ä½“é¡µé¢ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯ä¸Šçº§ç›®å½•
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            # æ£€æŸ¥è¿™äº›troubleshootingæ˜¯å¦çœŸçš„å±äºå½“å‰é¡µé¢
            current_url = node_data.get('url', '')
            if current_url and self._troubleshooting_belongs_to_current_page(node_data['troubleshooting'], current_url):
                ts_dir = root_dir / "troubleshooting"
                ts_dir.mkdir(exist_ok=True)
                # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
                ts_media_dir = ts_dir / "media"
                ts_media_dir.mkdir(exist_ok=True)

                for i, ts in enumerate(node_data['troubleshooting']):
                    ts_data = ts.copy()
                    self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                    ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        json.dump(ts_data, f, ensure_ascii=False, indent=2)

                print(f"   ğŸ’¾ ä¿å­˜äº† {len(node_data['troubleshooting'])} ä¸ªtroubleshootingåˆ°: {ts_dir}")

                # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
                if self.cache_manager and current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)
            else:
                print(f"   âš ï¸  è·³è¿‡ä¿å­˜troubleshootingï¼Œå› ä¸ºå®ƒä»¬ä¸å±äºå½“å‰é¡µé¢: {current_url}")

        # å¤„ç†å­ç±»åˆ« - ä¿å­˜åˆ°subcategoriesæ–‡ä»¶å¤¹
        if 'children' in node_data and node_data['children']:
            subcategories_dir = root_dir / "subcategories"
            subcategories_dir.mkdir(exist_ok=True)
            for child in node_data['children']:
                self._save_subcategory_to_filesystem(child, subcategories_dir)

    def _save_subcategory_to_filesystem(self, node, subcategories_dir):
        """ä¿å­˜å­ç±»åˆ«åˆ°subcategoriesç›®å½•"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")

        # åˆ›å»ºå­ç±»åˆ«ç›®å½•
        subcat_dir = subcategories_dir / safe_name
        subcat_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, subcat_dir)

        # ä¿å­˜å­ç±»åˆ«ä¿¡æ¯
        info_file = subcat_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†å­ç±»åˆ«çš„guideså’Œtroubleshooting
        if 'guides' in node_data and node_data['guides']:
            guides_dir = subcat_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - åªæœ‰å½“å‰å­ç±»åˆ«æœ‰troubleshootingæ•°æ®æ—¶æ‰ä¿å­˜
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            current_url = node_data.get('url', '')
            if current_url and self._troubleshooting_belongs_to_current_page(node_data['troubleshooting'], current_url):
                ts_dir = subcat_dir / "troubleshooting"
                ts_dir.mkdir(exist_ok=True)
                # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
                ts_media_dir = ts_dir / "media"
                ts_media_dir.mkdir(exist_ok=True)

                for i, ts in enumerate(node_data['troubleshooting']):
                    ts_data = ts.copy()
                    self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                    ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        json.dump(ts_data, f, ensure_ascii=False, indent=2)

                # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
                if self.cache_manager and current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)

                print(f"   ğŸ’¾ ä¿å­˜äº† {len(node_data['troubleshooting'])} ä¸ªtroubleshootingåˆ°å­ç±»åˆ«: {subcat_dir}")
            else:
                print(f"   âš ï¸  è·³è¿‡ä¿å­˜troubleshootingåˆ°å­ç±»åˆ«ï¼Œå› ä¸ºå®ƒä»¬ä¸å±äºå½“å‰é¡µé¢: {current_url}")

        # é€’å½’å¤„ç†æ›´æ·±å±‚çš„å­ç±»åˆ«
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_subcategory_to_filesystem(child, subcat_dir)

    def _save_node_to_filesystem(self, node, base_dir, path_prefix):
        """é€’å½’ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶ç³»ç»Ÿï¼Œç¡®ä¿è·¯å¾„ç¬¦åˆçœŸå®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        # æ›´å…¨é¢çš„åç§°æ¸…ç†
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")
        safe_name = safe_name.replace('"', '_').replace("'", "_").replace("?", "_")
        safe_name = safe_name.replace("<", "_").replace(">", "_").replace("|", "_")
        safe_name = safe_name.replace("*", "_").strip()

        # æ„å»ºå½“å‰èŠ‚ç‚¹è·¯å¾„
        if path_prefix:
            current_path = path_prefix + "/" + safe_name
            node_dir = base_dir / current_path
        else:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒ¹é…çš„ç›®å½•
            existing_dir = self._find_matching_directory(base_dir, safe_name)
            if existing_dir:
                node_dir = existing_dir
                print(f"   âœ… ä½¿ç”¨å·²æœ‰ç›®å½•: {node_dir}")
            else:
                node_dir = base_dir / safe_name

        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not node_dir.exists():
            node_dir.mkdir(parents=True, exist_ok=True)
            print(f"   ğŸ“ åˆ›å»ºæ–°ç›®å½•: {node_dir}")

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, node_dir)

        # ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°JSONæ–‡ä»¶
        info_file = node_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - åªæœ‰å±äºå½“å‰èŠ‚ç‚¹çš„troubleshootingæ‰ä¿å­˜
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            current_url = node_data.get('url', '')
            if current_url and self._troubleshooting_belongs_to_current_page(node_data['troubleshooting'], current_url):
                ts_dir = node_dir / "troubleshooting"
                ts_dir.mkdir(exist_ok=True)
                # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
                ts_media_dir = ts_dir / "media"
                ts_media_dir.mkdir(exist_ok=True)

                for i, ts in enumerate(node_data['troubleshooting']):
                    ts_data = ts.copy()
                    self._process_media_urls(ts_data, ts_dir)
                    ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        json.dump(ts_data, f, ensure_ascii=False, indent=2)

                # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
                if self.cache_manager and current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)

                print(f"   ğŸ’¾ ä¿å­˜äº† {len(node_data['troubleshooting'])} ä¸ªtroubleshootingåˆ°èŠ‚ç‚¹: {node_dir}")
            else:
                print(f"   âš ï¸  è·³è¿‡ä¿å­˜troubleshootingåˆ°èŠ‚ç‚¹ï¼Œå› ä¸ºå®ƒä»¬ä¸å±äºå½“å‰é¡µé¢: {current_url}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_node_to_filesystem(child, base_dir, current_path)
        
    def print_combined_tree_structure(self, node, level=0):
        """
        æ‰“å°æ•´åˆåçš„æ ‘å½¢ç»“æ„ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦
        """
        if not node:
            return

        indent = "  " * level
        name = node.get('name', 'Unknown')

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„å†…å®¹
        guides_count = len(node.get('guides', []))
        troubleshooting_count = len(node.get('troubleshooting', []))
        children_count = len(node.get('children', []))

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if 'title' in node and 'steps' in node:
            # ç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹
            steps_count = len(node.get('steps', []))
            print(f"{indent}ğŸ“– {name} [æŒ‡å— - {steps_count}æ­¥éª¤]")
        elif 'causes' in node:
            # ç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹
            causes_count = len(node.get('causes', []))
            print(f"{indent}ğŸ”§ {name} [æ•…éšœæ’é™¤ - {causes_count}åŸå› ]")
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“± {name} [äº§å“{content_str}]")
        elif children_count > 0 or guides_count > 0 or troubleshooting_count > 0:
            # åˆ†ç±»èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“ {name} [åˆ†ç±»{content_str}]")
        else:
            # å…¶ä»–èŠ‚ç‚¹
            print(f"{indent}â“ {name} [æœªçŸ¥ç±»å‹]")

        # æ˜¾ç¤ºguideså†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if guides_count > 0:
            for guide in node.get('guides', []):
                guide_name = guide.get('title', 'Unknown Guide')
                steps_count = len(guide.get('steps', []))
                print(f"{indent}  ğŸ“– {guide_name} [{steps_count}æ­¥éª¤]")

        # æ˜¾ç¤ºtroubleshootingå†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if troubleshooting_count > 0:
            for ts in node.get('troubleshooting', []):
                ts_name = ts.get('title', 'Unknown Troubleshooting')
                causes_count = len(ts.get('causes', []))
                print(f"{indent}  ğŸ”§ {ts_name} [{causes_count}åŸå› ]")

        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if children_count > 0:
            for child in node['children']:
                self.print_combined_tree_structure(child, level + 1)

    def extract_dynamic_sections(self, soup):
        """åŠ¨æ€æå–é¡µé¢ä¸Šçš„çœŸå®å­—æ®µåç§°å’Œå†…å®¹ï¼ŒåŸºäºå®é™…é¡µé¢ç»“æ„ï¼Œç¡®ä¿å­—æ®µåˆ†ç¦»å’Œæ— é‡å¤"""
        sections = {}
        seen_content = set()  # ç”¨äºå»é‡
        processed_elements = set()  # è®°å½•å·²å¤„ç†çš„å…ƒç´ ï¼Œé¿å…é‡å¤å¤„ç†

        try:
            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸï¼Œé¿å…å¯¼èˆªå’Œé¡µè„š
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda x: x and 'content' in str(x).lower())
            if not main_content:
                main_content = soup

            # é¦–å…ˆé€šè¿‡IDå’Œæ ‡é¢˜æ–‡æœ¬ç²¾ç¡®å®šä½å„ä¸ªç‹¬ç«‹éƒ¨åˆ†
            section_mappings = [
                ('introduction', ['#introduction']),
                ('first_steps', ['#Section_First_Steps']),
                ('triage', ['#Section_Triage'])
            ]

            for field_name, selectors in section_mappings:
                found = False
                for selector in selectors:
                    section_element = main_content.select_one(selector)
                    if section_element and id(section_element) not in processed_elements:
                        content = self.extract_section_content_precisely(section_element, main_content, field_name)
                        if content and content.strip():
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸å·²æœ‰å†…å®¹é‡å¤
                            if not self.is_content_duplicate(content, seen_content):
                                sections[field_name] = content.strip()
                                seen_content.add(content.strip())
                                processed_elements.add(id(section_element))
                                print(f"ç²¾ç¡®æå–å­—æ®µ: {field_name} ({len(content)} å­—ç¬¦)")
                                found = True
                                break

                # å¦‚æœé€šè¿‡IDæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡æ ‡é¢˜æ–‡æœ¬æŸ¥æ‰¾
                if not found:
                    target_texts = {
                        'introduction': ['introduction'],
                        'first_steps': ['first steps', 'before undertaking'],
                        'triage': ['triage']
                    }

                    if field_name in target_texts:
                        for heading in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                            heading_text = heading.get_text().strip().lower()
                            if any(target in heading_text for target in target_texts[field_name]):
                                if id(heading) not in processed_elements:
                                    content = self.extract_section_content_precisely(heading, main_content, field_name)
                                    if content and content.strip():
                                        if not self.is_content_duplicate(content, seen_content):
                                            sections[field_name] = content.strip()
                                            seen_content.add(content.strip())
                                            processed_elements.add(id(heading))
                                            print(f"é€šè¿‡æ ‡é¢˜æå–å­—æ®µ: {field_name} ({len(content)} å­—ç¬¦)")
                                            found = True
                                            break

        except Exception as e:
            print(f"åŠ¨æ€æå–sectionsæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

        return sections

    def extract_section_content_precisely(self, element, main_content, field_name):
        """ç²¾ç¡®æå–sectionå†…å®¹"""
        try:
            content_parts = []

            # ä»å…ƒç´ åé¢å¼€å§‹æå–å†…å®¹
            next_elem = element.find_next_sibling()
            while next_elem and next_elem.name not in ['h1', 'h2', 'h3']:
                if next_elem.name in ['p', 'div', 'ul', 'ol']:
                    text = next_elem.get_text().strip()
                    if text and len(text) > 10 and not self.is_commercial_text(text):
                        content_parts.append(text)
                next_elem = next_elem.find_next_sibling()

            return '\n\n'.join(content_parts) if content_parts else ""
        except Exception:
            return ""

    def is_content_duplicate(self, content, seen_content):
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        content_clean = content.strip()
        return content_clean in seen_content

    def extract_causes_sections_with_media(self, soup):
        """æå–Causeséƒ¨åˆ†å†…å®¹ï¼Œå¹¶ä¸ºæ¯ä¸ªcauseæå–å¯¹åº”çš„å›¾ç‰‡å’Œè§†é¢‘"""
        causes = []
        seen_numbers = set()  # é˜²æ­¢é‡å¤

        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¸¦ç¼–å·çš„é“¾æ¥ï¼Œè¿™äº›é€šå¸¸æ˜¯causes
            cause_links = soup.find_all('a', href=lambda x: x and '#Section_' in x)

            for link in cause_links:
                href = link.get('href', '')
                if '#Section_' in href:
                    # æå–ç¼–å·å’Œæ ‡é¢˜
                    link_text = link.get_text().strip()

                    # è·³è¿‡éç¼–å·çš„é“¾æ¥
                    if not any(char.isdigit() for char in link_text):
                        continue

                    # æå–ç¼–å·
                    number_match = re.search(r'^(\d+)', link_text)
                    if number_match:
                        number = number_match.group(1)

                        # é˜²æ­¢é‡å¤
                        if number in seen_numbers:
                            continue
                        seen_numbers.add(number)

                        title = re.sub(r'^\d+\s*', '', link_text).strip()

                        # æŸ¥æ‰¾å¯¹åº”çš„å†…å®¹éƒ¨åˆ†
                        section_id = href.split('#')[-1]
                        content_section = soup.find(id=section_id)

                        content = ""
                        images = []
                        videos = []

                        if content_section:
                            content = self.extract_section_content(content_section)
                            # ä»è¯¥sectionä¸­æå–å›¾ç‰‡å’Œè§†é¢‘
                            images = self._extract_troubleshooting_images_from_section(content_section)
                            videos = self.extract_videos_from_section(content_section)

                        cause_data = {
                            "number": number,
                            "title": title,
                            "content": content
                        }

                        # åªæœ‰å½“æœ‰å›¾ç‰‡æ—¶æ‰æ·»åŠ imageså­—æ®µ
                        if images:
                            cause_data["images"] = images

                        # åªæœ‰å½“æœ‰è§†é¢‘æ—¶æ‰æ·»åŠ videoså­—æ®µ
                        if videos:
                            cause_data["videos"] = videos

                        causes.append(cause_data)

                        if self.verbose:
                            print(f"æå–Cause {number}: {title} ({len(content)} å­—ç¬¦, {len(images)} å›¾ç‰‡, {len(videos)} è§†é¢‘)")

            return causes
        except Exception as e:
            print(f"æå–Causesæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def extract_section_content(self, section):
        """ä»sectionä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            content_parts = []
            p_elements = section.find_all('p')
            for p in p_elements:
                p_text = p.get_text().strip()
                if self.is_commercial_text(p_text):
                    continue
                if p_text and len(p_text) > 10:
                    content_parts.append(p_text)
            return '\n\n'.join(content_parts)
        except Exception:
            return ""

    def extract_videos_from_section(self, section):
        """ä»sectionä¸­æå–è§†é¢‘"""
        videos = []
        try:
            video_elements = section.find_all(['video', 'iframe'])
            for video in video_elements:
                try:
                    from bs4 import Tag
                    if isinstance(video, Tag):
                        video_src = video.get('src') or video.get('data-src')
                        if video_src and isinstance(video_src, str):
                            if 'youtube.com' in video_src or 'youtu.be' in video_src:
                                video_data = {
                                    "url": video_src,
                                    "title": video.get('title', 'Video')
                                }
                                videos.append(video_data)
                except Exception:
                    continue
        except Exception:
            pass
        return videos

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("=" * 60)
    print("iFixité«˜æ€§èƒ½çˆ¬è™«å·¥å…· - ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python auto_crawler.py [URLæˆ–è®¾å¤‡å] [é€‰é¡¹...]")
    print("\nåŸºæœ¬ç”¨æ³•:")
    print("  python auto_crawler.py iMac_M_Series")
    print("  python auto_crawler.py https://www.ifixit.com/Device/Television")
    print("  python auto_crawler.py Television")
    print("\nğŸš€ æ€§èƒ½è°ƒä¼˜é€‰é¡¹:")
    print("  --workers N            è®¾ç½®å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤8ï¼Œæ¨è8-16ï¼‰")
    print("  --max-connections N    è®¾ç½®æœ€å¤§è¿æ¥æ•°ï¼ˆé»˜è®¤80ï¼‰")
    print("  --max-retries N        è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰")
    print("  --timeout N            è®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30ï¼‰")
    print("  --delay N              è®¾ç½®è¯·æ±‚é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤0.5ï¼‰")
    print("  --burst-mode           å¯ç”¨çˆ†å‘æ¨¡å¼ï¼ˆæœ€å¤§æ€§èƒ½ï¼Œå¯èƒ½è¢«é™åˆ¶ï¼‰")
    print("  --conservative         å¯ç”¨ä¿å®ˆæ¨¡å¼ï¼ˆé™ä½å¹¶å‘ï¼Œæ›´ç¨³å®šï¼‰")
    print("\nğŸŒ ä»£ç†å’Œç½‘ç»œé€‰é¡¹:")
    print("  --no-proxy             å…³é—­éš§é“ä»£ç†ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    print("  --proxy-switch N       ä»£ç†åˆ‡æ¢é¢‘ç‡ï¼ˆè¯·æ±‚æ•°ï¼Œé»˜è®¤1=æ¯æ¬¡åˆ‡æ¢ï¼‰")
    print("  --user-agent TEXT      è‡ªå®šä¹‰User-Agent")
    print("\nğŸ’¾ ç¼“å­˜å’Œæ•°æ®é€‰é¡¹:")
    print("  --no-cache             ç¦ç”¨ç¼“å­˜æ£€æŸ¥ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    print("  --force-refresh        å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
    print("  --cache-ttl N          ç¼“å­˜é…ç½®å‚æ•°ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼Œå®é™…ä¸ºé•¿æœŸä¿å­˜ï¼‰")
    print("\nğŸ”„ æ–­ç‚¹ç»­çˆ¬é€‰é¡¹:")
    print("  --no-resume            ç¦ç”¨æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    print("  --reset-progress       é‡ç½®æ ‘æ„å»ºè¿›åº¦ï¼ˆæ¸…é™¤æ–­ç‚¹è®°å½•ï¼‰")
    print("  --reset-only           ä»…é‡ç½®è¿›åº¦åé€€å‡ºï¼ˆä¸å¼€å§‹çˆ¬å–ï¼‰")
    print("  --show-progress        æ˜¾ç¤ºå½“å‰æ ‘æ„å»ºè¿›åº¦")
    print("  --progress-only        ä»…æ˜¾ç¤ºè¿›åº¦åé€€å‡ºï¼ˆä¸å¼€å§‹çˆ¬å–ï¼‰")
    print("\nğŸ“¹ åª’ä½“å¤„ç†é€‰é¡¹:")
    print("  --download-videos      å¯ç”¨è§†é¢‘æ–‡ä»¶ä¸‹è½½ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰")
    print("  --max-video-size N     è®¾ç½®è§†é¢‘æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼Œé»˜è®¤50ï¼‰")
    print("  --skip-images          è·³è¿‡å›¾ç‰‡ä¸‹è½½ï¼ˆä»…ä¿å­˜URLï¼‰")
    print("\nğŸ”§ è°ƒè¯•é€‰é¡¹:")
    print("  --verbose              å¯ç”¨è¯¦ç»†è¾“å‡º")
    print("  --debug                å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆæ›´è¯¦ç»†çš„æ—¥å¿—ï¼‰")
    print("  --stats                æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    print("\nğŸ“Š é¢„è®¾é…ç½®:")
    print("  --fast                 å¿«é€Ÿæ¨¡å¼ï¼š--workers 12 --burst-mode")
    print("  --stable               ç¨³å®šæ¨¡å¼ï¼š--workers 4 --conservative --delay 1")
    print("  --enterprise           ä¼ä¸šæ¨¡å¼ï¼š--workers 16 --max-connections 160")
    print("\nç¤ºä¾‹:")
    print("  # é»˜è®¤é«˜æ€§èƒ½æ¨¡å¼ï¼ˆ8çº¿ç¨‹+ä»£ç†æ± ï¼‰")
    print("  python auto_crawler.py iMac_M_Series")
    print("")
    print("  # å¿«é€Ÿæ¨¡å¼ï¼ˆ12çº¿ç¨‹+çˆ†å‘æ¨¡å¼ï¼‰")
    print("  python auto_crawler.py iMac_M_Series --fast")
    print("")
    print("  # ä¼ä¸šçº§é«˜å¹¶å‘æ¨¡å¼")
    print("  python auto_crawler.py Television --enterprise --download-videos")
    print("")
    print("  # ç¨³å®šæ¨¡å¼ï¼ˆé€‚åˆç½‘ç»œä¸ç¨³å®šç¯å¢ƒï¼‰")
    print("  python auto_crawler.py iPhone --stable --verbose")
    print("")
    print("  # è‡ªå®šä¹‰é«˜æ€§èƒ½é…ç½®")
    print("  python auto_crawler.py MacBook --workers 20 --max-connections 200 --delay 0.2")
    print("")
    print("  # æ–­ç‚¹ç»­çˆ¬ç›¸å…³æ“ä½œ")
    print("  python auto_crawler.py Television --show-progress  # æŸ¥çœ‹è¿›åº¦")
    print("  python auto_crawler.py Television --reset-progress # é‡ç½®è¿›åº¦")
    print("  python auto_crawler.py Television                  # è‡ªåŠ¨ä»æ–­ç‚¹æ¢å¤")
    print("\nğŸ¯ é»˜è®¤é…ç½®è¯´æ˜:")
    print("- ğŸ”¥ é«˜æ€§èƒ½ï¼šé»˜è®¤8çº¿ç¨‹å¹¶å‘ + éš§é“ä»£ç†æ± ")
    print("- ğŸŒ æ™ºèƒ½ä»£ç†ï¼šHTTPéš§é“ä»£ç†æ± ï¼Œæ¯æ¬¡è¯·æ±‚è‡ªåŠ¨åˆ‡æ¢IP")
    print("- ğŸ’¾ æ™ºèƒ½ç¼“å­˜ï¼šè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„å†…å®¹ï¼Œé•¿æœŸä¿å­˜åˆ°æœ¬åœ°")
    print("- ğŸ”„ æ–­ç‚¹ç»­çˆ¬ï¼šè‡ªåŠ¨è®°å½•æ ‘æ„å»ºè¿›åº¦ï¼Œæ”¯æŒä¸­æ–­æ¢å¤")
    print("- ğŸ”„ é”™è¯¯é‡è¯•ï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿ç­–ç•¥")
    print("- ğŸ“ åª’ä½“ä¸‹è½½ï¼šè‡ªåŠ¨ä¸‹è½½å¹¶æœ¬åœ°åŒ–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶")
    print("- ğŸ“Š å®æ—¶ç»Ÿè®¡ï¼šæ˜¾ç¤ºçˆ¬å–è¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡")
    print("=" * 60)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = sys.argv[1:]

    # æ£€æŸ¥å¸®åŠ©å‚æ•°
    if not args or args[0].lower() in ['--help', '-h', 'help']:
        print_usage()
        return

    # è·å–ç›®æ ‡URL/åç§°
    input_text = args[0]

    # ğŸš€ é»˜è®¤é«˜æ€§èƒ½é…ç½®
    verbose = '--verbose' in args or '--debug' in args
    debug_mode = '--debug' in args
    show_stats = '--stats' in args
    use_proxy = '--no-proxy' not in args  # é»˜è®¤å¯ç”¨ä»£ç†
    use_cache = '--no-cache' not in args  # é»˜è®¤å¯ç”¨ç¼“å­˜
    force_refresh = '--force-refresh' in args
    skip_images = '--skip-images' in args

    # ğŸ¯ é¢„è®¾é…ç½®å¤„ç†
    if '--fast' in args:
        # å¿«é€Ÿæ¨¡å¼ï¼šé«˜å¹¶å‘ + çˆ†å‘æ¨¡å¼
        default_workers = 12
        burst_mode = True
        conservative_mode = False
        default_delay = 0.3
    elif '--stable' in args:
        # ç¨³å®šæ¨¡å¼ï¼šä½å¹¶å‘ + ä¿å®ˆç­–ç•¥
        default_workers = 4
        burst_mode = False
        conservative_mode = True
        default_delay = 1.0
    elif '--enterprise' in args:
        # ä¼ä¸šæ¨¡å¼ï¼šè¶…é«˜å¹¶å‘
        default_workers = 16
        burst_mode = True
        conservative_mode = False
        default_delay = 0.2
    else:
        # é»˜è®¤é«˜æ€§èƒ½æ¨¡å¼
        default_workers = 8
        burst_mode = '--burst-mode' in args
        conservative_mode = '--conservative' in args
        default_delay = 0.5

    # è§£ææ€§èƒ½å‚æ•°
    max_workers = default_workers
    if '--workers' in args:
        try:
            workers_idx = args.index('--workers')
            if workers_idx + 1 < len(args):
                max_workers = int(args[workers_idx + 1])
        except (ValueError, IndexError):
            print(f"è­¦å‘Š: workerså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{default_workers}")

    # è§£æè¿æ¥æ•°å‚æ•°
    max_connections = max_workers * 10  # é»˜è®¤ä¸ºçº¿ç¨‹æ•°çš„10å€
    if '--max-connections' in args:
        try:
            conn_idx = args.index('--max-connections')
            if conn_idx + 1 < len(args):
                max_connections = int(args[conn_idx + 1])
        except (ValueError, IndexError):
            print(f"è­¦å‘Š: max-connectionså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{max_connections}")

    # è§£æé‡è¯•å‚æ•°
    max_retries = 1  # æé€Ÿæ¨¡å¼ï¼šæœ€å°‘é‡è¯•æ¬¡æ•°
    if '--max-retries' in args:
        try:
            retries_idx = args.index('--max-retries')
            if retries_idx + 1 < len(args):
                max_retries = int(args[retries_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-retrieså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼3")

    # è§£æè¶…æ—¶å‚æ•°
    timeout = 30
    if '--timeout' in args:
        try:
            timeout_idx = args.index('--timeout')
            if timeout_idx + 1 < len(args):
                timeout = int(args[timeout_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: timeoutå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼30ç§’")

    # è§£æè¯·æ±‚é—´éš”å‚æ•°
    request_delay = default_delay
    if '--delay' in args:
        try:
            delay_idx = args.index('--delay')
            if delay_idx + 1 < len(args):
                request_delay = float(args[delay_idx + 1])
        except (ValueError, IndexError):
            print(f"è­¦å‘Š: delayå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{default_delay}ç§’")

    # è§£æä»£ç†åˆ‡æ¢é¢‘ç‡
    proxy_switch_freq = 1  # é»˜è®¤æ¯æ¬¡è¯·æ±‚éƒ½åˆ‡æ¢IP
    if '--proxy-switch' in args:
        try:
            switch_idx = args.index('--proxy-switch')
            if switch_idx + 1 < len(args):
                proxy_switch_freq = int(args[switch_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: proxy-switchå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼1ï¼ˆæ¯æ¬¡è¯·æ±‚åˆ‡æ¢ï¼‰")

    # è§£æç¼“å­˜TTL
    cache_ttl = 24
    if '--cache-ttl' in args:
        try:
            ttl_idx = args.index('--cache-ttl')
            if ttl_idx + 1 < len(args):
                cache_ttl = int(args[ttl_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: cache-ttlå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼24å°æ—¶")

    # è§£æè§†é¢‘ä¸‹è½½å‚æ•°
    download_videos = '--download-videos' in args
    max_video_size_mb = 50
    if '--max-video-size' in args:
        try:
            size_idx = args.index('--max-video-size')
            if size_idx + 1 < len(args):
                max_video_size_mb = int(args[size_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-video-sizeå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼50MB")

    # ğŸ”„ è§£ææ–­ç‚¹ç»­çˆ¬å‚æ•°
    enable_resume = '--no-resume' not in args  # é»˜è®¤å¯ç”¨æ–­ç‚¹ç»­çˆ¬
    reset_progress = '--reset-progress' in args
    show_progress = '--show-progress' in args

    # å¤„ç†è¿›åº¦é‡ç½®
    if reset_progress:
        print("ğŸ”„ é‡ç½®æ ‘æ„å»ºè¿›åº¦...")
        from tree_building_progress import TreeBuildingProgressManager

        # ç¡®å®šç›®æ ‡URL
        url = input_text if input_text.startswith('http') else f"https://www.ifixit.com/Device/{input_text}"
        progress_manager = TreeBuildingProgressManager(url)
        progress_manager.reset_progress()
        print("âœ… è¿›åº¦å·²é‡ç½®")

        if '--reset-only' in args:
            return

    # æ˜¾ç¤ºè¿›åº¦æŠ¥å‘Š
    if show_progress:
        print("ğŸ“Š æ˜¾ç¤ºæ ‘æ„å»ºè¿›åº¦...")
        from tree_building_progress import TreeBuildingProgressManager

        # ç¡®å®šç›®æ ‡URL
        url = input_text if input_text.startswith('http') else f"https://www.ifixit.com/Device/{input_text}"
        progress_manager = TreeBuildingProgressManager(url)
        progress_manager.display_progress_report()

        if '--progress-only' in args:
            return

    # è§£æè‡ªå®šä¹‰User-Agent
    custom_user_agent = None
    if '--user-agent' in args:
        try:
            ua_idx = args.index('--user-agent')
            if ua_idx + 1 < len(args):
                custom_user_agent = args[ua_idx + 1]
        except (ValueError, IndexError):
            print("è­¦å‘Š: user-agentå‚æ•°æ— æ•ˆ")

    if not input_text:
        print_usage()
        return

    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)

    if url:
        # åˆ›å»ºæ•´åˆçˆ¬è™«å®ä¾‹ï¼ˆç”¨äºæ£€æµ‹ï¼‰
        temp_crawler = CombinedIFixitCrawler(verbose=False, use_proxy=False)

        # æ£€æŸ¥ç›®æ ‡æ˜¯å¦å·²ç»å®Œæ•´çˆ¬å–
        is_complete, target_dir = temp_crawler._check_target_completeness(input_text)

        if is_complete and not force_refresh and target_dir:
            print("\n" + "=" * 60)
            print(f"ğŸ¯ æ£€æµ‹åˆ°ç›®æ ‡å·²å®Œæˆçˆ¬å–: {name}")
            print(f"ğŸ“ ç›®å½•ä½ç½®: {target_dir}")
            print("=" * 60)

            # æ£€æŸ¥ç›®å½•å†…å®¹
            info_file = target_dir / "info.json"
            guides_dir = target_dir / "guides"
            troubleshooting_dir = target_dir / "troubleshooting"
            media_dir = target_dir / "media"

            guides_count = len(list(guides_dir.glob("*.json"))) if guides_dir.exists() else 0
            ts_count = len(list(troubleshooting_dir.glob("*.json"))) if troubleshooting_dir.exists() else 0
            media_count = len(list(media_dir.glob("*"))) if media_dir.exists() else 0

            print(f"ğŸ“Š å·²æœ‰å†…å®¹ç»Ÿè®¡:")
            print(f"   åŸºæœ¬ä¿¡æ¯: {'âœ…' if info_file.exists() else 'âŒ'}")
            print(f"   æŒ‡å—æ•°é‡: {guides_count}")
            print(f"   æ•…éšœæ’é™¤æ•°é‡: {ts_count}")
            print(f"   åª’ä½“æ–‡ä»¶æ•°é‡: {media_count}")

            print(f"\nğŸ’¡ é€‰é¡¹:")
            print(f"   1. è·³è¿‡çˆ¬å–ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
            print(f"   2. é‡æ–°çˆ¬å–ï¼ˆè¦†ç›–ç°æœ‰æ•°æ®ï¼‰")
            print(f"   3. é€€å‡ºç¨‹åº")

            while True:
                choice = input("\nè¯·é€‰æ‹© (1/2/3): ").strip()
                if choice == "1":
                    print(f"âœ… è·³è¿‡çˆ¬å–ï¼Œç°æœ‰æ•°æ®ä½ç½®: {target_dir}")
                    return
                elif choice == "2":
                    print("ğŸ”„ å°†é‡æ–°çˆ¬å–å¹¶è¦†ç›–ç°æœ‰æ•°æ®...")
                    force_refresh = True
                    break
                elif choice == "3":
                    print("ğŸ‘‹ ç¨‹åºé€€å‡º")
                    return
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")

        # ğŸš€ æ˜¾ç¤ºé«˜æ€§èƒ½é…ç½®ä¿¡æ¯
        print("\n" + "=" * 80)
        print(f"ğŸ¯ å¼€å§‹é«˜æ€§èƒ½çˆ¬å–: {name}")
        print("=" * 80)
        print(f"ğŸ”¥ æ€§èƒ½é…ç½®:")
        print(f"   å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
        print(f"   æœ€å¤§è¿æ¥æ•°: {max_connections}")
        print(f"   è¯·æ±‚é—´éš”: {request_delay}ç§’")
        print(f"   è¶…æ—¶æ—¶é—´: {timeout}ç§’")
        print(f"   æœ€å¤§é‡è¯•: {max_retries}æ¬¡")

        mode_desc = []
        if burst_mode:
            mode_desc.append("ğŸš€çˆ†å‘æ¨¡å¼")
        if conservative_mode:
            mode_desc.append("ğŸ›¡ï¸ä¿å®ˆæ¨¡å¼")
        if mode_desc:
            print(f"   è¿è¡Œæ¨¡å¼: {' + '.join(mode_desc)}")

        print(f"ğŸŒ ç½‘ç»œé…ç½®:")
        print(f"   éš§é“ä»£ç†: {'âœ…å¯ç”¨' if use_proxy else 'âŒç¦ç”¨'}")
        if use_proxy:
            if proxy_switch_freq == 1:
                print(f"   ä»£ç†åˆ‡æ¢: æ¯æ¬¡è¯·æ±‚åˆ‡æ¢IP")
            else:
                print(f"   ä»£ç†åˆ‡æ¢: æ¯{proxy_switch_freq}æ¬¡è¯·æ±‚")
        print(f"   æ™ºèƒ½ç¼“å­˜: {'âœ…å¯ç”¨' if use_cache else 'âŒç¦ç”¨'}")
        if use_cache:
            print(f"   ç¼“å­˜ç­–ç•¥: é•¿æœŸä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶")

        print(f"ğŸ“ åª’ä½“å¤„ç†:")
        print(f"   å›¾ç‰‡ä¸‹è½½: {'âŒè·³è¿‡' if skip_images else 'âœ…å¯ç”¨'}")
        print(f"   è§†é¢‘ä¸‹è½½: {'âœ…å¯ç”¨' if download_videos else 'âŒç¦ç”¨'}")
        if download_videos:
            print(f"   è§†é¢‘å¤§å°é™åˆ¶: {max_video_size_mb}MB")

        print(f"ğŸ”§ å…¶ä»–é€‰é¡¹:")
        print(f"   è¯¦ç»†è¾“å‡º: {'âœ…å¯ç”¨' if verbose else 'âŒç¦ç”¨'}")
        print(f"   è°ƒè¯•æ¨¡å¼: {'âœ…å¯ç”¨' if debug_mode else 'âŒç¦ç”¨'}")
        print(f"   ç»Ÿè®¡ä¿¡æ¯: {'âœ…å¯ç”¨' if show_stats else 'âŒç¦ç”¨'}")
        print(f"   å¼ºåˆ¶åˆ·æ–°: {'âœ…å¯ç”¨' if force_refresh else 'âŒç¦ç”¨'}")

        print("=" * 80)
        print("ğŸš€ æ‰§è¡Œé˜¶æ®µ:")
        print("   é˜¶æ®µ1: æ„å»ºå®Œæ•´æ ‘å½¢ç»“æ„")
        print("   é˜¶æ®µ2: é«˜å¹¶å‘æå–è¯¦ç»†å†…å®¹")
        print("   é˜¶æ®µ3: å¹¶è¡Œä¸‹è½½åª’ä½“æ–‡ä»¶")
        print("=" * 80)

        # ğŸš€ åˆ›å»ºé«˜æ€§èƒ½æ•´åˆçˆ¬è™«
        crawler = CombinedIFixitCrawler(
            verbose=verbose,
            use_proxy=use_proxy,
            use_cache=use_cache,
            force_refresh=force_refresh,
            max_workers=max_workers,
            max_retries=max_retries,
            download_videos=download_videos,
            max_video_size_mb=max_video_size_mb,
            max_connections=max_connections,
            timeout=timeout,
            request_delay=request_delay,
            proxy_switch_freq=proxy_switch_freq,
            cache_ttl=cache_ttl,
            custom_user_agent=custom_user_agent,
            burst_mode=burst_mode,
            conservative_mode=conservative_mode,
            skip_images=skip_images,
            debug_mode=debug_mode,
            show_stats=show_stats
        )

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œæ•´åˆçˆ¬å–
        try:
            combined_data = crawler.crawl_combined_tree(url, name)

            if combined_data:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = crawler.count_detailed_nodes(combined_data)
                elapsed_time = time.time() - start_time

                # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
                print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
                print("=" * 60)
                crawler.print_combined_tree_structure(combined_data)
                print("=" * 60)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\næ•´åˆçˆ¬å–ç»Ÿè®¡:")
                print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
                print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
                print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
                print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
                print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
                print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

                # ä¿å­˜ç»“æœ
                print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶å¤¹...")
                filename = crawler.save_combined_result(combined_data, target_name=input_text)

                print(f"\nâœ… æ•´åˆçˆ¬å–å®Œæˆ!")
                print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {filename}")

                # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
                crawler._print_performance_stats()

                # ä¿å­˜ç¼“å­˜ç´¢å¼•
                if crawler.cache_manager:
                    crawler.cache_manager.save_cache_index()
                    cache_stats = crawler.cache_manager.get_cache_stats()
                    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
                    print(f"- æ€»å¤„ç†URL: {cache_stats['total_urls']}")
                    print(f"- ç¼“å­˜å‘½ä¸­: {cache_stats['cached_urls']}")
                    print(f"- æ–°å¢å¤„ç†: {cache_stats['new_urls']}")
                    if cache_stats['total_urls'] > 0:
                        hit_rate = (cache_stats['cached_urls'] / cache_stats['total_urls']) * 100
                        print(f"- ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")

                # æä¾›ä½¿ç”¨å»ºè®®
                print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
                print(f"- æŸ¥çœ‹æ–‡ä»¶å¤¹ç»“æ„äº†è§£å®Œæ•´çš„æ•°æ®å±‚çº§")
                print(f"- æ¯ä¸ªç›®å½•åŒ…å«info.jsonã€guides/ã€troubleshooting/å­ç›®å½•")
                print(f"- æ‰€æœ‰åª’ä½“æ–‡ä»¶å·²ä¸‹è½½åˆ°media/ç›®å½•ï¼ŒURLå·²æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„")
                print(f"- ä¸‹æ¬¡çˆ¬å–ç›¸åŒå†…å®¹å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œå¤§å¹…æå‡é€Ÿåº¦")

            else:
                print("\nâœ— æ•´åˆçˆ¬å–å¤±è´¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
            # å³ä½¿ä¸­æ–­ä¹Ÿä¿å­˜ç¼“å­˜ç´¢å¼•
            if crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
        except Exception as e:
            print(f"\nâœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ç¼“å­˜ç´¢å¼•
            if crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
            if verbose:
                import traceback
                traceback.print_exc()
        finally:
            # ç¡®ä¿ç¼“å­˜ç´¢å¼•è¢«ä¿å­˜
            if 'crawler' in locals() and crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
    else:
        print_usage()

def test_multi_proxy_concurrent():
    """æµ‹è¯•å¤šä»£ç†å¹¶å‘åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•å¤šä»£ç†å¹¶å‘åŠŸèƒ½")
    print("=" * 60)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œå¯ç”¨å¤šä»£ç†å¹¶å‘
    crawler = CombinedIFixitCrawler(
        verbose=True,
        use_proxy=True,
        use_cache=False,
        max_workers=5  # ä½¿ç”¨5ä¸ªå¹¶å‘çº¿ç¨‹
    )

    # æ˜¾ç¤ºä»£ç†æ± ä¿¡æ¯
    if crawler.proxy_manager:
        stats = crawler.proxy_manager.get_stats()
        print(f"ğŸ“Š ä»£ç†æ± ç»Ÿè®¡:")
        print(f"   æ± å¤§å°: {stats['pool_size']}")
        print(f"   æ´»è·ƒä»£ç†: {stats['active_proxies']}")
        print(f"   ä»£ç†è¯¦æƒ…: {len(stats['proxy_details'])} ä¸ªä»£ç†")

        # æµ‹è¯•å¤šä¸ªä»£ç†åˆ†é…
        print(f"\nğŸ”„ æµ‹è¯•ä»£ç†åˆ†é…:")
        for i in range(8):
            proxy = crawler.proxy_manager.get_proxy(i)
            if proxy:
                print(f"   çº¿ç¨‹ {i} -> ä»£ç† #{proxy.get('proxy_id', 'N/A')}")

    return crawler

def test_tunnel_proxy():
    """æµ‹è¯•HTTPéš§é“ä»£ç†åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•HTTPéš§é“ä»£ç†åŠŸèƒ½")
    print("=" * 60)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œå¯ç”¨ä»£ç†
    crawler = CombinedIFixitCrawler(
        verbose=True,
        use_proxy=True,
        use_cache=False,
        max_workers=1
    )

    # æµ‹è¯•ä»£ç†è·å–
    print("\n1. æµ‹è¯•éš§é“ä»£ç†é…ç½®...")
    if crawler.proxy_manager:
        proxy = crawler.proxy_manager.get_proxy()
        if proxy:
            print("âœ… éš§é“ä»£ç†é…ç½®æˆåŠŸ")
            print(f"éš§é“åœ°å€: {crawler.proxy_manager.tunnel_host}:{crawler.proxy_manager.tunnel_port}")
            print(f"ç”¨æˆ·å: {crawler.proxy_manager.username}")
            stats = crawler.proxy_manager.get_stats()
            print(f"ä»£ç†ç»Ÿè®¡: {stats}")
        else:
            print("âŒ éš§é“ä»£ç†é…ç½®å¤±è´¥")
            return
    else:
        print("âŒ ä»£ç†ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return

    # æµ‹è¯•å®é™…ç½‘ç»œè¯·æ±‚
    print("\n2. æµ‹è¯•å®é™…ç½‘ç»œè¯·æ±‚...")
    test_url = "https://www.ifixit.com/Device"
    try:
        soup = crawler.get_soup(test_url)
        if soup:
            title = soup.find('title')
            print(f"âœ… ç½‘ç»œè¯·æ±‚æˆåŠŸ")
            print(f"é¡µé¢æ ‡é¢˜: {title.get_text() if title else 'No title'}")
        else:
            print("âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œæœªè·å–åˆ°é¡µé¢å†…å®¹")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")

    # æµ‹è¯•å¤šæ¬¡è¯·æ±‚ï¼ˆéªŒè¯IPè‡ªåŠ¨åˆ‡æ¢ï¼‰
    print("\n3. æµ‹è¯•å¤šæ¬¡è¯·æ±‚ï¼ˆéªŒè¯IPè‡ªåŠ¨åˆ‡æ¢ï¼‰...")
    test_urls = [
        "https://dev.kdlapi.com/testproxy",  # ä»£ç†æµ‹è¯•URL
        "https://httpbin.org/ip",            # æ˜¾ç¤ºIPçš„æµ‹è¯•URL
        "https://www.ifixit.com/Device"      # iFixité¡µé¢
    ]

    for i, url in enumerate(test_urls, 1):
        try:
            print(f"  è¯·æ±‚ {i}: {url}")
            response = requests.get(url,
                                  proxies=crawler.proxy_manager.get_proxy(),
                                  timeout=10,
                                  headers={'User-Agent': 'Mozilla/5.0'})
            if response.status_code == 200:
                print(f"    âœ… æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")
                if 'testproxy' in url or 'httpbin' in url:
                    print(f"    å“åº”: {response.text[:100]}...")
            else:
                print(f"    âš ï¸  çŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            print(f"    âŒ è¯·æ±‚å¤±è´¥: {str(e)[:100]}")

    print("\n" + "=" * 60)
    print("ğŸ éš§é“ä»£ç†æµ‹è¯•å®Œæˆ")
    print("=" * 60)


async def test_async_performance():
    """æµ‹è¯•å¼‚æ­¥ç‰ˆæœ¬çš„æ€§èƒ½"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•å¼‚æ­¥çˆ¬è™«æ€§èƒ½")
    print("=" * 60)

    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://www.ifixit.com/Device/Mac_Laptop",
        "https://www.ifixit.com/Device/iPhone",
        "https://www.ifixit.com/Device/iPad"
    ]

    for i, test_url in enumerate(test_urls, 1):
        print(f"\nğŸ” æµ‹è¯• {i}/{len(test_urls)}: {test_url}")
        print("-" * 40)

        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = CombinedIFixitCrawler(
            verbose=False,
            use_proxy=False,  # å…ˆä¸ä½¿ç”¨ä»£ç†æµ‹è¯•
            use_cache=False,
            max_workers=5
        )

        try:
            # æµ‹è¯•å¼‚æ­¥ç‰ˆæœ¬
            print("âš¡ å¼‚æ­¥ç‰ˆæœ¬æµ‹è¯•...")
            start_time = time.time()

            result = await crawler.crawl_combined_tree_async(test_url)

            async_time = time.time() - start_time

            if result:
                stats = crawler.count_detailed_nodes(result)
                print(f"âœ… å¼‚æ­¥ç‰ˆæœ¬å®Œæˆ")
                print(f"   è€—æ—¶: {async_time:.2f} ç§’")
                print(f"   èŠ‚ç‚¹æ•°: {sum(stats.values())} ä¸ª")
                print(f"   æŒ‡å—: {stats.get('guides', 0)} ä¸ª")
                print(f"   æ•…éšœæ’é™¤: {stats.get('troubleshooting', 0)} ä¸ª")
            else:
                print("âŒ å¼‚æ­¥ç‰ˆæœ¬å¤±è´¥")

            # æµ‹è¯•åŒæ­¥ç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”
            print("ğŸŒ åŒæ­¥ç‰ˆæœ¬æµ‹è¯•...")
            start_time = time.time()

            sync_result = crawler.crawl_combined_tree(test_url)

            sync_time = time.time() - start_time

            if sync_result:
                sync_stats = crawler.count_detailed_nodes(sync_result)
                print(f"âœ… åŒæ­¥ç‰ˆæœ¬å®Œæˆ")
                print(f"   è€—æ—¶: {sync_time:.2f} ç§’")
                print(f"   èŠ‚ç‚¹æ•°: {sum(sync_stats.values())} ä¸ª")
                print(f"   æŒ‡å—: {sync_stats.get('guides', 0)} ä¸ª")
                print(f"   æ•…éšœæ’é™¤: {sync_stats.get('troubleshooting', 0)} ä¸ª")

                # æ€§èƒ½å¯¹æ¯”
                if async_time > 0 and sync_time > 0:
                    speedup = sync_time / async_time
                    print(f"ğŸš€ æ€§èƒ½æå‡: {speedup:.2f}x")
                    if speedup > 1:
                        print(f"   å¼‚æ­¥ç‰ˆæœ¬å¿« {(speedup-1)*100:.1f}%")
                    else:
                        print(f"   åŒæ­¥ç‰ˆæœ¬å¿« {(1/speedup-1)*100:.1f}%")
            else:
                print("âŒ åŒæ­¥ç‰ˆæœ¬å¤±è´¥")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")

    print("\n" + "=" * 60)
    print("ğŸ å¼‚æ­¥æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print("=" * 60)


async def run_async_crawler(input_text):
    """å¼‚æ­¥è¿è¡Œçˆ¬è™«çš„ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¼‚æ­¥çˆ¬è™«æ¨¡å¼")

    # è§£æè¾“å…¥
    url, name = parse_input(input_text)
    if not url:
        print("âŒ æ— æ³•è§£æè¾“å…¥çš„URLæˆ–è®¾å¤‡å")
        return

    print(f"ğŸ¯ ç›®æ ‡: {name}")
    print(f"ğŸ”— URL: {url}")

    # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
    crawler = CombinedIFixitCrawler(
        verbose=True,
        use_proxy=True,
        use_cache=True,
        max_workers=10  # å¼‚æ­¥ç‰ˆæœ¬å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    )

    start_time = time.time()

    try:
        # æ‰§è¡Œå¼‚æ­¥çˆ¬å–
        combined_data = await crawler.crawl_combined_tree_async(url, name)

        if combined_data:
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = crawler.count_detailed_nodes(combined_data)
            elapsed_time = time.time() - start_time

            # æ˜¾ç¤ºç»“æœ
            print("ğŸ‰ å¼‚æ­¥çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š æŒ‡å—: {stats['guides']} | æ•…éšœæ’é™¤: {stats['troubleshooting']} | äº§å“: {stats['products']} | åˆ†ç±»: {stats['categories']}")
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.1f} ç§’")

            # ä¿å­˜ç»“æœ
            filename = crawler.save_combined_result(combined_data, target_name=input_text)
            print(f"ğŸ“ æ•°æ®å·²ä¿å­˜: {filename}")

            # åœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            if hasattr(crawler, 'verbose') and crawler.verbose:
                crawler._print_performance_stats()

        else:
            print("âŒ å¼‚æ­¥çˆ¬å–å¤±è´¥")

    except Exception as e:
        print(f"âŒ å¼‚æ­¥çˆ¬å–å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "--test-proxy":
        test_tunnel_proxy()
        sys.exit(0)

    # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "--test-async":
        asyncio.run(test_async_performance())
        sys.exit(0)

    # é»˜è®¤ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆé«˜æ€§èƒ½é…ç½®ï¼‰
    main()


async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•° - é»˜è®¤ä½¿ç”¨å¼‚æ­¥å¹¶å‘+ä»£ç†æ± æ¨¡å¼"""
    import sys

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = sys.argv[1:]

    # é»˜è®¤é…ç½®
    verbose = False
    use_proxy = True  # é»˜è®¤å¯ç”¨ä»£ç†
    use_cache = True
    force_refresh = False
    max_workers = 20  # å¼‚æ­¥æ¨¡å¼é»˜è®¤æ›´é«˜å¹¶å‘ï¼Œæå‡åˆ°20
    max_retries = 3
    download_videos = True
    max_video_size_mb = 100

    # è§£æå‚æ•°
    input_text = None
    i = 0
    while i < len(args):
        arg = args[i]
        if arg == "--help" or arg == "-h":
            print_help()
            return
        elif arg == "--verbose":
            verbose = True
        elif arg == "--no-proxy":
            use_proxy = False
        elif arg == "--no-cache":
            use_cache = False
        elif arg == "--force-refresh":
            force_refresh = True
        elif arg == "--no-videos":
            download_videos = False
        elif arg == "--max-video-size":
            if i + 1 < len(args):
                try:
                    max_video_size_mb = int(args[i + 1])
                    i += 1
                except ValueError:
                    print(f"âŒ æ— æ•ˆçš„è§†é¢‘å¤§å°é™åˆ¶: {args[i + 1]}")
                    return
        elif arg == "--max-workers":
            if i + 1 < len(args):
                try:
                    max_workers = int(args[i + 1])
                    i += 1
                except ValueError:
                    print(f"âŒ æ— æ•ˆçš„å¹¶å‘æ•°: {args[i + 1]}")
                    return
        elif arg == "--max-retries":
            if i + 1 < len(args):
                try:
                    max_retries = int(args[i + 1])
                    i += 1
                except ValueError:
                    print(f"âŒ æ— æ•ˆçš„é‡è¯•æ¬¡æ•°: {args[i + 1]}")
                    return
        elif not arg.startswith("--"):
            if input_text is None:
                input_text = arg
        i += 1

    # æ£€æŸ¥è¾“å…¥
    if not input_text:
        print("âŒ è¯·æä¾›URLæˆ–è®¾å¤‡å")
        print("ğŸ’¡ ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        return

    # è§£æè¾“å…¥
    url, name = parse_input(input_text)
    if not url:
        print("âŒ æ— æ³•è§£æè¾“å…¥çš„URLæˆ–è®¾å¤‡å")
        return

    print("ğŸš€ å¯åŠ¨å¼‚æ­¥å¹¶å‘çˆ¬è™« (é»˜è®¤æ¨¡å¼)")
    print(f"ğŸ¯ ç›®æ ‡: {name}")
    print(f"ğŸ”— URL: {url}")
    print(f"âš¡ å¹¶å‘æ•°: {max_workers}")
    print(f"ğŸ”„ ä»£ç†: {'å¯ç”¨' if use_proxy else 'ç¦ç”¨'}")
    print(f"ğŸ’¾ ç¼“å­˜: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'}")

    if download_videos:
        print(f"ğŸ¬ è§†é¢‘ä¸‹è½½: å¯ç”¨ (é™åˆ¶: {max_video_size_mb}MB)")
    else:
        print("ğŸ¬ è§†é¢‘ä¸‹è½½: ç¦ç”¨")

    # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
    crawler = CombinedIFixitCrawler(
        verbose=verbose,
        use_proxy=use_proxy,
        use_cache=use_cache,
        force_refresh=force_refresh,
        max_workers=max_workers,
        max_retries=max_retries,
        download_videos=download_videos,
        max_video_size_mb=max_video_size_mb,
        proxy_switch_freq=1,  # å¼‚æ­¥æ¨¡å¼ä¹Ÿä½¿ç”¨æ¯æ¬¡è¯·æ±‚åˆ‡æ¢IP
        enable_resume=enable_resume  # ä¼ é€’æ–­ç‚¹ç»­çˆ¬å‚æ•°
    )

    start_time = time.time()

    try:
        # æ‰§è¡Œå¼‚æ­¥çˆ¬å–
        combined_data = await crawler.crawl_combined_tree_async(url, name)

        if combined_data:
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = crawler.count_detailed_nodes(combined_data)
            elapsed_time = time.time() - start_time

            # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
            print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
            print("=" * 60)
            crawler.print_combined_tree_structure(combined_data)
            print("=" * 60)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"\nå¼‚æ­¥çˆ¬å–ç»Ÿè®¡:")
            print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
            print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
            print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
            print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
            print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
            print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

            # ä¿å­˜ç»“æœ
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶å¤¹...")
            filename = crawler.save_combined_result(combined_data, target_name=input_text)

            print(f"\nâœ… å¼‚æ­¥çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {filename}")

            # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            crawler._print_performance_stats()

        else:
            print("âŒ å¼‚æ­¥çˆ¬å–å¤±è´¥")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        print("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ --sync å‚æ•°åˆ‡æ¢åˆ°åŒæ­¥æ¨¡å¼")
    except Exception as e:
        print(f"âŒ å¼‚æ­¥çˆ¬å–å¼‚å¸¸: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        print("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ --sync å‚æ•°åˆ‡æ¢åˆ°åŒæ­¥æ¨¡å¼")
