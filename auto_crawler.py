#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•´åˆçˆ¬è™«å·¥å…· - ç»“åˆæ ‘å½¢ç»“æ„å’Œè¯¦ç»†å†…å®¹æå– + ä»£ç†æ±  + æœ¬åœ°æ–‡ä»¶å­˜å‚¨
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
æ”¯æŒHTTPéš§é“ä»£ç†æ± ã€æœ¬åœ°æ–‡ä»¶å¤¹å­˜å‚¨ã€åª’ä½“æ–‡ä»¶ä¸‹è½½ç­‰åŠŸèƒ½
ç”¨æ³•: python auto_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python auto_crawler.py https://www.ifixit.com/Device/Television
      python auto_crawler.py Television
"""

import os
import sys
import json
import time
import random
import re
import requests
import httpx
import asyncio
import aiofiles
from urllib.parse import urlparse, urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
import hashlib
import mimetypes
import threading
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import queue

# å¯¼å…¥ä¸¤ä¸ªåŸºç¡€çˆ¬è™«
from enhanced_crawler import EnhancedIFixitCrawler
from tree_crawler import TreeCrawler


def safe_str(obj):
    """å®‰å…¨çš„å­—ç¬¦ä¸²è½¬æ¢ï¼Œé¿å…æ ¼å¼åŒ–é”™è¯¯"""
    try:
        if obj is None:
            return "None"
        if hasattr(obj, '__str__'):
            result = str(obj)
            # ç¡®ä¿è¿”å›çš„å­—ç¬¦ä¸²ä¸ä¸ºç©º
            return result if result else "Empty string"
        else:
            return repr(obj)
    except Exception:
        return "Unknown error"


def safe_json_dump(data, file_handle, **kwargs):
    """å®‰å…¨çš„JSONåºåˆ—åŒ–å‡½æ•°ï¼Œè‡ªåŠ¨å¤„ç†PosixPathç­‰ä¸å¯åºåˆ—åŒ–å¯¹è±¡"""
    def convert_paths(obj):
        """é€’å½’è½¬æ¢PosixPathå¯¹è±¡ä¸ºå­—ç¬¦ä¸²"""
        if hasattr(obj, '__fspath__'):  # æ£€æŸ¥æ˜¯å¦æ˜¯è·¯å¾„å¯¹è±¡
            return str(obj)
        elif isinstance(obj, dict):
            return {k: convert_paths(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_paths(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(convert_paths(item) for item in obj)
        else:
            return obj

    try:
        # è½¬æ¢æ•°æ®ä¸­çš„æ‰€æœ‰è·¯å¾„å¯¹è±¡
        safe_data = convert_paths(data)
        json.dump(safe_data, file_handle, **kwargs)
    except Exception as e:
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ›´ä¸¥æ ¼çš„è½¬æ¢
        try:
            safe_data = json.loads(json.dumps(data, default=str))
            json.dump(safe_data, file_handle, **kwargs)
        except Exception as e2:
            # æœ€åçš„å›é€€æ–¹æ¡ˆï¼šè®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­ç¨‹åº
            print(f"âš ï¸ JSONåºåˆ—åŒ–å¤±è´¥ï¼Œè·³è¿‡æ­¤æ–‡ä»¶: {e2}")
            # å†™å…¥ä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯ä¿¡æ¯
            error_data = {
                "error": "JSON serialization failed",
                "original_error": str(e2),
                "timestamp": datetime.now().isoformat()
            }
            json.dump(error_data, file_handle, **kwargs)


class TunnelProxyManager:
    """éš§é“ä»£ç†ç®¡ç†å™¨ - ä½¿ç”¨HTTPéš§é“ä»£ç†æ± ï¼Œæ”¯æŒå¤šä»£ç†å¹¶å‘"""

    def __init__(self, tunnel_host="b353.kdltpspro.com", tunnel_port=15818,
                 username="t15237111788411", password="htjqu9dr", max_reset_cycles=3,
                 pool_size=10):
        """
        åˆå§‹åŒ–éš§é“ä»£ç†ç®¡ç†å™¨

        Args:
            tunnel_host: éš§é“åŸŸå
            tunnel_port: éš§é“ç«¯å£
            username: ç”¨æˆ·å
            password: å¯†ç 
            max_reset_cycles: æœ€å¤§é‡ç½®å‘¨æœŸæ•°
            pool_size: ä»£ç†æ± å¤§å°ï¼ˆå¹¶å‘è¿æ¥æ•°ï¼‰
        """
        self.tunnel_host = tunnel_host
        self.tunnel_port = tunnel_port
        self.username = username
        self.password = password
        self.pool_size = pool_size

        self.proxy_pool = []  # ä»£ç†æ± 
        self.current_index = 0  # å½“å‰ä»£ç†ç´¢å¼•
        self.proxy_switch_count = 0  # è¯·æ±‚è®¡æ•°å™¨
        self.proxy_lock = threading.Lock()

        # æ·»åŠ é‡ç½®å¾ªç¯é™åˆ¶
        self.max_reset_cycles = max_reset_cycles
        self.reset_cycle_count = 0
        self.failed_count = 0  # å¤±è´¥è®¡æ•°

        # åˆå§‹åŒ–ä»£ç†æ± 
        self._init_proxy_pool()

    def _init_proxy_pool(self):
        """åˆå§‹åŒ–ä»£ç†æ±  - åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„ä»£ç†è¿æ¥"""
        try:
            # æ„å»ºåŸºç¡€ä»£ç†URL
            base_proxy_url = f"http://{self.username}:{self.password}@{self.tunnel_host}:{self.tunnel_port}"

            # åˆ›å»ºä»£ç†æ±  - æ¯ä¸ªä»£ç†éƒ½æ˜¯ç‹¬ç«‹çš„è¿æ¥
            for i in range(self.pool_size):
                proxy_config = {
                    'http': base_proxy_url,
                    'https': base_proxy_url,
                    'id': i,
                    'active': True,
                    'failed_count': 0
                }
                self.proxy_pool.append(proxy_config)

            print(f"ğŸ“‹ å¤šä»£ç†æ± åˆå§‹åŒ–æˆåŠŸ")
            print(f"ğŸŒ éš§é“åœ°å€: {self.tunnel_host}:{self.tunnel_port}")
            print(f"ğŸ‘¤ ç”¨æˆ·å: {self.username}")
            print(f"ğŸ”„ ä»£ç†æ± å¤§å°: {self.pool_size} ä¸ªå¹¶å‘è¿æ¥")

            # å¯åŠ¨ä»£ç†é¢„çƒ­ï¼ˆåå°ä»»åŠ¡ï¼‰
            try:
                import asyncio
                asyncio.create_task(self._warmup_proxies())
            except RuntimeError:
                # å¦‚æœæ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œè·³è¿‡é¢„çƒ­
                pass

        except Exception as e:
            print(f"âŒ ä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
            self.proxy_pool = []

    def get_proxy(self, thread_id=None):
        """è·å–ä»£ç†é…ç½® - æ™ºèƒ½ä»£ç†é€‰æ‹©ï¼Œä¼˜å…ˆä½¿ç”¨ç¨³å®šä»£ç†"""
        with self.proxy_lock:
            if not self.proxy_pool:
                print("âŒ ä»£ç†æ± æœªåˆå§‹åŒ–")
                return None

            # å¦‚æœæŒ‡å®šäº†çº¿ç¨‹IDï¼Œå°è¯•ä¸ºè¯¥çº¿ç¨‹åˆ†é…å›ºå®šä»£ç†
            if thread_id is not None:
                proxy_index = thread_id % len(self.proxy_pool)
                proxy = self.proxy_pool[proxy_index]
                if proxy['active']:
                    return {
                        'http': proxy['http'],
                        'https': proxy['https'],
                        'proxy_id': proxy['id']
                    }

            # æ™ºèƒ½é€‰æ‹©ä»£ç†ï¼šä¼˜å…ˆé€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„æ´»è·ƒä»£ç†
            active_proxies = [p for p in self.proxy_pool if p['active']]
            if not active_proxies:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„ä»£ç†")
                return None

            # æŒ‰å¤±è´¥æ¬¡æ•°æ’åºï¼Œä¼˜å…ˆä½¿ç”¨æœ€ç¨³å®šçš„ä»£ç†
            active_proxies.sort(key=lambda x: x['failed_count'])

            # é€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„ä»£ç†
            proxy = active_proxies[0]

            # å¦‚æœæœ€å¥½çš„ä»£ç†å¤±è´¥æ¬¡æ•°ä¹Ÿå¾ˆé«˜ï¼Œè½®è¯¢ä½¿ç”¨
            if proxy['failed_count'] > 3:
                proxy = active_proxies[self.current_index % len(active_proxies)]
                self.current_index = (self.current_index + 1) % len(active_proxies)

            return {
                'http': proxy['http'],
                'https': proxy['https'],
                'proxy_id': proxy['id']
            }

    def mark_proxy_failed(self, proxy, reason=""):
        """æ ‡è®°ä»£ç†å¤±è´¥"""
        with self.proxy_lock:
            if proxy and 'proxy_id' in proxy:
                proxy_id = proxy['proxy_id']
                if proxy_id < len(self.proxy_pool):
                    self.proxy_pool[proxy_id]['failed_count'] += 1
                    failed_count = self.proxy_pool[proxy_id]['failed_count']

                    # æé«˜å¤±è´¥é˜ˆå€¼ï¼Œå‡å°‘é¢‘ç¹åˆ‡æ¢
                    if failed_count >= 5:  # æé«˜å¤±è´¥é˜ˆå€¼åˆ°5æ¬¡
                        self.proxy_pool[proxy_id]['active'] = False

                        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¯ç”¨ä»£ç†
                        active_count = sum(1 for p in self.proxy_pool if p['active'])
                        if active_count <= 2:  # ä¿ç•™è‡³å°‘2ä¸ªä»£ç†å¯ç”¨
                            # é‡æ–°æ¿€æ´»å¤±è´¥æ¬¡æ•°æœ€å°‘çš„ä»£ç†
                            min_failed = min(p['failed_count'] for p in self.proxy_pool)
                            for p in self.proxy_pool:
                                if p['failed_count'] == min_failed:
                                    p['active'] = True
                                    p['failed_count'] = max(0, p['failed_count'] - 2)  # å‡å°‘å¤±è´¥è®¡æ•°
            else:
                self.failed_count += 1
                # é™é»˜å¤„ç†å¤±è´¥ï¼Œå‡å°‘æ—¥å¿—è¾“å‡º

    def get_stats(self):
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        active_count = sum(1 for p in self.proxy_pool if p['active'])
        total_failed = sum(p['failed_count'] for p in self.proxy_pool)

        return {
            'tunnel_host': self.tunnel_host,
            'tunnel_port': self.tunnel_port,
            'username': self.username,
            'pool_size': self.pool_size,
            'active_proxies': active_count,
            'total_failed': total_failed,
            'proxy_details': [
                {
                    'id': p['id'],
                    'active': p['active'],
                    'failed_count': p['failed_count']
                } for p in self.proxy_pool
            ]
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        with self.proxy_lock:
            for proxy in self.proxy_pool:
                proxy['failed_count'] = 0
                proxy['active'] = True
            self.failed_count = 0

    async def test_proxy_speed(self, proxy_url, test_url="https://httpbin.org/ip"):
        """æµ‹è¯•å•ä¸ªä»£ç†çš„å“åº”é€Ÿåº¦"""
        try:
            import time
            start_time = time.time()

            timeout_config = httpx.Timeout(connect=1.0, read=2.0)
            async with httpx.AsyncClient(timeout=timeout_config, proxy=proxy_url) as client:
                response = await client.get(test_url)
                end_time = time.time()

                if response.status_code == 200:
                    return end_time - start_time
                else:
                    return float('inf')
        except Exception:
            return float('inf')

    async def _warmup_proxies(self):
        """ä»£ç†é¢„çƒ­ - åå°æµ‹è¯•ä»£ç†å¯ç”¨æ€§"""
        try:
            # æµ‹è¯•å‰3ä¸ªä»£ç†
            proxy_url = f"http://{self.username}:{self.password}@{self.tunnel_host}:{self.tunnel_port}"

            # å¹¶è¡Œæµ‹è¯•
            tasks = []
            for i in range(min(3, len(self.proxy_pool))):
                task = self.test_proxy_speed(proxy_url, "https://httpbin.org/ip")
                tasks.append(task)

            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # æ ‡è®°ä¸å¯ç”¨çš„ä»£ç†
                for i, result in enumerate(results):
                    if isinstance(result, Exception) or result == float('inf'):
                        if i < len(self.proxy_pool):
                            self.proxy_pool[i]['active'] = False
                            self.proxy_pool[i]['failed_count'] += 1

                print(f"ğŸ”¥ ä»£ç†é¢„çƒ­å®Œæˆï¼Œæµ‹è¯•äº† {len(tasks)} ä¸ªä»£ç†")

        except Exception as e:
            # é¢„çƒ­å¤±è´¥ä¸å½±å“ä¸»ç¨‹åº
            pass


class AsyncHttpClientManager:
    """å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨ - åŸºäºhttpxçš„é«˜æ€§èƒ½å¼‚æ­¥è¯·æ±‚"""

    def __init__(self, proxy_manager=None, max_connections=500, max_keepalive_connections=100,
                 timeout=3.0, max_retries=2):
        """
        åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ›´é«˜å¹¶å‘ï¼‰

        Args:
            proxy_manager: ä»£ç†ç®¡ç†å™¨å®ä¾‹
            max_connections: æœ€å¤§è¿æ¥æ•°ï¼ˆæå‡åˆ°200ä»¥æ”¯æŒæ›´é«˜å¹¶å‘ï¼‰
            max_keepalive_connections: æœ€å¤§ä¿æŒè¿æ¥æ•°ï¼ˆæå‡åˆ°50ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.proxy_manager = proxy_manager
        self.max_connections = max_connections
        self.max_keepalive_connections = max_keepalive_connections
        self.timeout = timeout
        self.max_retries = max_retries

        # è¯·æ±‚å¤´é…ç½® - å¼ºåˆ¶è‹±æ–‡ç‰ˆæœ¬
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=1.0",  # å¼ºåˆ¶è‹±æ–‡ï¼Œæé«˜ä¼˜å…ˆçº§
            "Accept-Encoding": "gzip, deflate, br",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
            "Cookie": "locale=en"  # æ·»åŠ è‹±æ–‡locale cookie
        }

        # åª’ä½“æ–‡ä»¶ä¸“ç”¨è¯·æ±‚å¤´ - å¼ºåˆ¶è‹±æ–‡ç‰ˆæœ¬
        self.media_headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=1.0",  # å¼ºåˆ¶è‹±æ–‡ï¼Œæé«˜ä¼˜å…ˆçº§
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://www.ifixit.com/",
            "Origin": "https://www.ifixit.com",
            "Sec-Fetch-Dest": "image",
            "Sec-Fetch-Mode": "no-cors",
            "Sec-Fetch-Site": "same-site",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Cookie": "locale=en"  # æ·»åŠ è‹±æ–‡locale cookie
        }

        self._client = None
        self._semaphore = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._init_client()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self._close_client()

    async def _init_client(self):
        """åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
        try:
            if self._client:
                await self._client.aclose()
                self._client = None
                
            self._semaphore = asyncio.Semaphore(self.max_connections)

            # ä¼˜åŒ–è¶…æ—¶é…ç½® - å¢åŠ è¶…æ—¶æ—¶é—´å‡å°‘é¢‘ç¹åˆ‡æ¢
            timeout_config = httpx.Timeout(
                connect=5.0,    # è¿æ¥è¶…æ—¶5ç§’
                read=10.0,      # è¯»å–è¶…æ—¶10ç§’
                write=5.0,      # å†™å…¥è¶…æ—¶5ç§’
                pool=3.0        # è¿æ¥æ± è·å–è¶…æ—¶3ç§’
            )

            self._client = httpx.AsyncClient(
                timeout=timeout_config,
                follow_redirects=True,
                limits=httpx.Limits(
                    max_connections=self.max_connections,
                    max_keepalive_connections=self.max_keepalive_connections,
                    keepalive_expiry=5.0  # è¿æ¥ä¿æŒæ—¶é—´5ç§’ï¼Œå¿«é€Ÿé‡Šæ”¾å¤±æ•ˆè¿æ¥
                ),
                headers=self.headers
            )
            return True
        except Exception as e:
            print(f"åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            return False

    async def _close_client(self):
        """å…³é—­å¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
        if self._client:
            await self._client.aclose()
            self._client = None

    async def get(self, url, headers=None, **kwargs):
        """å¼‚æ­¥GETè¯·æ±‚ - ä¿®å¤äº‹ä»¶å¾ªç¯å…³é—­é—®é¢˜"""
        try:
            # æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
            try:
                loop = asyncio.get_running_loop()
                if loop.is_closed():
                    return None
            except RuntimeError:
                return None

            if not self._client or not self._semaphore:
                await self._init_client()

            # å†æ¬¡æ£€æŸ¥ç¡®ä¿_clientå’Œ_semaphoreå·²ç»åˆå§‹åŒ–
            if not self._client or not self._semaphore:
                return None

            async with self._semaphore:
                request_headers = self.headers.copy()
                if headers:
                    request_headers.update(headers)

                return await self._client.get(url, headers=request_headers, **kwargs)
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e) if e is not None else "Unknown error"
            # ç‰¹æ®Šå¤„ç†äº‹ä»¶å¾ªç¯å…³é—­çš„æƒ…å†µ
            if "Event loop is closed" in error_msg:
                return None
            print(f"å¼‚æ­¥HTTPè¯·æ±‚å¤±è´¥: {error_msg}")
            return None

    async def get_with_retry(self, url, headers=None, max_retries=None, **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„å¼‚æ­¥GETè¯·æ±‚ - ä¿®å¤äº‹ä»¶å¾ªç¯å…³é—­é—®é¢˜"""
        if max_retries is None:
            max_retries = self.max_retries

        last_error = None

        for attempt in range(max_retries + 1):
            try:
                # æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
                try:
                    loop = asyncio.get_running_loop()
                    if loop.is_closed():
                        return None
                except RuntimeError:
                    return None

                response = await self.get(url, headers=headers, **kwargs)

                # ç‰¹æ®Šå¤„ç†404é”™è¯¯ - ä¸é‡è¯•ï¼Œç›´æ¥è¿”å›å“åº”
                if response and response.status_code == 404:
                    print(f"âš ï¸ é¡µé¢ä¸å­˜åœ¨ (404): {url}")
                    return response

                if response:
                    response.raise_for_status()
                    return response
                else:
                    return None

            except (httpx.ProxyError, httpx.ConnectTimeout, httpx.ConnectError, httpx.ReadTimeout, httpx.TimeoutException) as e:
                # ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œå‡å°‘é¢‘ç¹åˆ‡æ¢
                if self.proxy_manager and attempt < max_retries:
                    # åªåœ¨ç¬¬ä¸€æ¬¡é‡è¯•æ—¶åˆ‡æ¢ä»£ç†
                    if attempt == 0:
                        import time
                        current_time = time.strftime("%H:%M:%S")
                        error_str = safe_str(e)
                        print(f"[{current_time}] ğŸ”„ ç½‘ç»œé”™è¯¯ï¼Œåˆ‡æ¢ä»£ç†...")

                        self.proxy_manager.mark_proxy_failed(None, f"ç½‘ç»œé”™è¯¯: {error_str[:50]}")

                        # ç«‹å³é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯ä»¥ä½¿ç”¨æ–°ä»£ç†
                        await self._close_client()
                        await self._init_client()

                    # é€’å¢å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„é‡è¯•
                    delay = 0.5 + attempt * 0.5  # 0.5ç§’, 1ç§’, 1.5ç§’...
                    await asyncio.sleep(delay)

                last_error = e

            except Exception as e:
                last_error = e

                # ç‰¹æ®Šå¤„ç†äº‹ä»¶å¾ªç¯å…³é—­çš„æƒ…å†µ
                error_str = safe_str(e)
                if "Event loop is closed" in error_str:
                    return None

                if attempt < max_retries:
                    # éç½‘ç»œé”™è¯¯çš„å¿«é€Ÿé‡è¯•
                    delay = 0.1 + attempt * 0.1  # æœ€å¤§å»¶è¿Ÿ0.3ç§’
                    await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œä½†ä¸è¦æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯è¿”å›Noneè®©è°ƒç”¨è€…å¤„ç†
        return None

    async def download_file(self, url, local_path, headers=None):
        """å¼‚æ­¥ä¸‹è½½æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæå‡ä¸‹è½½é€Ÿåº¦ï¼‰"""
        if not headers:
            headers = self.media_headers

        try:
            # æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
            try:
                loop = asyncio.get_running_loop()
                if loop.is_closed():
                    return None
            except RuntimeError:
                return None

            response = await self.get_with_retry(url, headers=headers)
            if response is None:
                return None

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # å¼‚æ­¥å†™å…¥æ–‡ä»¶ï¼Œä¼˜åŒ–chunkå¤§å°ä»¥æå‡æ€§èƒ½
            async with aiofiles.open(local_path, 'wb') as f:
                async for chunk in response.aiter_bytes(chunk_size=16384):  # å¢åŠ chunkå¤§å°
                    await f.write(chunk)

            return local_path

        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯å¤„ç†
            error_msg = str(e) if e is not None else "Unknown error"
            if "Event loop is closed" in error_msg:
                return None
            return None

    async def download_files_batch(self, download_tasks, max_concurrent=20):
        """æ‰¹é‡å¼‚æ­¥ä¸‹è½½æ–‡ä»¶ï¼Œä¼˜åŒ–æ€§èƒ½"""
        semaphore = asyncio.Semaphore(max_concurrent)

        async def download_with_semaphore(url, local_path, headers=None):
            async with semaphore:
                try:
                    # æ£€æŸ¥äº‹ä»¶å¾ªç¯æ˜¯å¦ä»ç„¶æ´»è·ƒ
                    try:
                        loop = asyncio.get_running_loop()
                        if loop.is_closed():
                            return None
                    except RuntimeError:
                        # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
                        return None

                    return await self.download_file(url, local_path, headers)
                except Exception as e:
                    # ç‰¹æ®Šå¤„ç†äº‹ä»¶å¾ªç¯å…³é—­çš„æƒ…å†µ
                    if "Event loop is closed" in str(e):
                        return None
                    return None

        # åˆ›å»ºæ‰€æœ‰ä¸‹è½½ä»»åŠ¡
        tasks = [download_with_semaphore(url, path, headers)
                for url, path, headers in download_tasks]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä¸‹è½½
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r is not None and not isinstance(r, Exception))
        return success_count, len(tasks)


class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - ä¼˜åŒ–çˆ¬è™«é‡å¤æ‰§è¡Œæ•ˆç‡"""

    def __init__(self, storage_root, logger=None, force_refresh=False):
        self.storage_root = Path(storage_root)
        self.logger = logger or logging.getLogger(__name__)
        self.cache_index_file = self.storage_root / "cache_index.json"
        self.cache_index = {}
        self.force_refresh = force_refresh  # æ·»åŠ å¼ºåˆ¶åˆ·æ–°æ ‡å¿—
        self.stats = {
            'total_urls': 0,
            'cached_urls': 0,
            'new_urls': 0,
            'invalid_cache': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        self.load_cache_index()

    def load_cache_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            if self.cache_index_file.exists():
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.cache_index = data.get('entries', {})
                    self.logger.info(f"å·²åŠ è½½ç¼“å­˜ç´¢å¼•ï¼ŒåŒ…å« {len(self.cache_index)} ä¸ªæ¡ç›®")
            else:
                self.cache_index = {}
                self.logger.info("ç¼“å­˜ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ç´¢å¼•")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
            self.cache_index = {}

    def save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            self.storage_root.mkdir(parents=True, exist_ok=True)
            cache_data = {
                'version': '1.0',
                'last_updated': datetime.now(timezone.utc).isoformat(),
                'entries': self.cache_index
            }
            with open(self.cache_index_file, 'w', encoding='utf-8') as f:
                safe_json_dump(cache_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ç¼“å­˜ç´¢å¼•å·²ä¿å­˜ï¼ŒåŒ…å« {len(self.cache_index)} ä¸ªæ¡ç›®")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def get_url_hash(self, url):
        """ç”ŸæˆURLçš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

    def is_url_cached_and_valid(self, url, local_path):
        """æ£€æŸ¥URLæ˜¯å¦å·²ç¼“å­˜ä¸”æ•°æ®æœ‰æ•ˆ"""
        url_hash = self.get_url_hash(url)
        self.stats['total_urls'] += 1

        self.logger.info(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
        self.logger.info(f"   URLå“ˆå¸Œ: {url_hash}")
        self.logger.info(f"   æœ¬åœ°è·¯å¾„: {local_path}")

        # æ£€æŸ¥ç¼“å­˜ç´¢å¼•ä¸­æ˜¯å¦å­˜åœ¨
        if url_hash not in self.cache_index:
            self.logger.info(f"   âŒ ç¼“å­˜ç´¢å¼•ä¸­ä¸å­˜åœ¨æ­¤URL")
            self.stats['new_urls'] += 1
            self.stats['cache_misses'] += 1
            return False

        cache_entry = self.cache_index[url_hash]
        self.logger.info(f"   ğŸ“‹ æ‰¾åˆ°ç¼“å­˜æ¡ç›®: {cache_entry.get('processed_time', 'N/A')}")

        # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not local_path.exists():
            self.logger.info(f"   âŒ æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_path}")
            self.stats['invalid_cache'] += 1
            self.stats['cache_misses'] += 1
            return False

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if not self._validate_cached_data(local_path, cache_entry):
            self.logger.info(f"   âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            self.stats['invalid_cache'] += 1
            self.stats['cache_misses'] += 1
            return False

        self.logger.info(f"   âœ… ç¼“å­˜æœ‰æ•ˆï¼Œå‘½ä¸­!")
        self.stats['cached_urls'] += 1
        self.stats['cache_hits'] += 1
        return True

    def _validate_cached_data(self, local_path, cache_entry):
        """éªŒè¯ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§ - æ™ºèƒ½é€‚é…ä¸åŒçš„æ–‡ä»¶ç»“æ„"""
        try:
            self.logger.info(f"   ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")

            # æ£€æŸ¥info.jsonæ–‡ä»¶
            info_file = local_path / "info.json"
            if not info_file.exists():
                self.logger.info(f"   âŒ info.jsonæ–‡ä»¶ä¸å­˜åœ¨")
                return False

            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            # æ£€æŸ¥troubleshootingç¼“å­˜çš„ä¸€è‡´æ€§
            url = cache_entry.get('url', '')
            # troubleshootingç¼“å­˜ç°åœ¨å®Œå…¨ç‹¬ç«‹ç®¡ç†ï¼Œä¸ä¾èµ–cache_indexæ£€æŸ¥

            structure = cache_entry.get('structure', {})
            self.logger.info(f"   ğŸ“Š é¢„æœŸç»“æ„: {structure}")

            # æ™ºèƒ½éªŒè¯ï¼šå¦‚æœæ²¡æœ‰ç»“æ„ä¿¡æ¯ï¼Œä»å®é™…æ–‡ä»¶æ¨æ–­
            if not structure:
                structure = self._infer_structure_from_files(local_path)
                self.logger.info(f"   ğŸ” æ¨æ–­çš„ç»“æ„: {structure}")

            # éªŒè¯guidesç›®å½•å’Œæ–‡ä»¶
            if structure.get('has_guides', False):
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"   âŒ guidesç›®å½•ä¸å­˜åœ¨")
                    return False

                # æ£€æŸ¥guideå­ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ–°çš„ç›®å½•ç»“æ„ï¼šguide_1/guide.jsonï¼‰
                guide_subdirs = list(guides_dir.glob("guide_*"))
                expected_count = structure.get('guides_count', 0)

                # éªŒè¯æ¯ä¸ªguideå­ç›®å½•éƒ½æœ‰guide.jsonæ–‡ä»¶
                valid_guides = 0
                for guide_subdir in guide_subdirs:
                    guide_file = guide_subdir / "guide.json"
                    if guide_file.exists():
                        valid_guides += 1

                if valid_guides != expected_count:
                    self.logger.info(f"   âŒ æŒ‡å—æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {valid_guides}")
                    return False
                else:
                    self.logger.info(f"   âœ… æŒ‡å—æ–‡ä»¶éªŒè¯é€šè¿‡: {valid_guides} ä¸ª")

            # éªŒè¯troubleshootingç›®å½•å’Œæ–‡ä»¶
            if structure.get('has_troubleshooting', False):
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"   âŒ troubleshootingç›®å½•ä¸å­˜åœ¨")
                    return False

                # æ£€æŸ¥troubleshootingå­ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ–°çš„ç›®å½•ç»“æ„ï¼štroubleshooting_1/troubleshooting.jsonï¼‰
                ts_subdirs = list(ts_dir.glob("troubleshooting_*"))
                expected_count = structure.get('troubleshooting_count', 0)

                # éªŒè¯æ¯ä¸ªtroubleshootingå­ç›®å½•éƒ½æœ‰troubleshooting.jsonæ–‡ä»¶
                valid_troubleshooting = 0
                for ts_subdir in ts_subdirs:
                    ts_file = ts_subdir / "troubleshooting.json"
                    if ts_file.exists():
                        valid_troubleshooting += 1

                if valid_troubleshooting != expected_count:
                    self.logger.info(f"   âŒ æ•…éšœæ’é™¤æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {valid_troubleshooting}")
                    return False
                else:
                    self.logger.info(f"   âœ… æ•…éšœæ’é™¤æ–‡ä»¶éªŒè¯é€šè¿‡: {valid_troubleshooting} ä¸ª")

            # éªŒè¯åª’ä½“æ–‡ä»¶
            if structure.get('has_media', False):
                media_dir = local_path / "media"
                if media_dir.exists():
                    media_files = list(media_dir.glob("*"))
                    if len(media_files) == 0 and structure.get('media_count', 0) > 0:
                        self.logger.info(f"   âŒ åª’ä½“æ–‡ä»¶ç¼ºå¤±")
                        return False
                    else:
                        self.logger.info(f"   âœ… åª’ä½“æ–‡ä»¶éªŒè¯é€šè¿‡: {len(media_files)} ä¸ª")

            self.logger.info(f"   âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            self.logger.error(f"   âŒ éªŒè¯ç¼“å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def _infer_structure_from_files(self, local_path):
        """ä»å®é™…æ–‡ä»¶æ¨æ–­ç¼“å­˜ç»“æ„"""
        structure = {
            'has_guides': False,
            'guides_count': 0,
            'has_troubleshooting': False,
            'troubleshooting_count': 0,
            'has_media': False,
            'media_count': 0
        }

        try:
            # æ£€æŸ¥guidesç›®å½•
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                # æ£€æŸ¥æ–°æ ¼å¼ï¼šguide_*/guide.json
                guide_subdirs = list(guides_dir.glob("guide_*"))
                valid_guides = sum(1 for subdir in guide_subdirs if (subdir / "guide.json").exists())

                # å¦‚æœæ²¡æœ‰æ–°æ ¼å¼ï¼Œæ£€æŸ¥æ—§æ ¼å¼ï¼š*.json
                if valid_guides == 0:
                    guide_files = list(guides_dir.glob("*.json"))
                    valid_guides = len(guide_files)

                if valid_guides > 0:
                    structure['has_guides'] = True
                    structure['guides_count'] = valid_guides

            # æ£€æŸ¥troubleshootingç›®å½•
            ts_dir = local_path / "troubleshooting"
            if ts_dir.exists():
                # æ£€æŸ¥æ–°æ ¼å¼ï¼štroubleshooting_*/troubleshooting.json
                ts_subdirs = list(ts_dir.glob("troubleshooting_*"))
                valid_ts = sum(1 for subdir in ts_subdirs if (subdir / "troubleshooting.json").exists())

                # å¦‚æœæ²¡æœ‰æ–°æ ¼å¼ï¼Œæ£€æŸ¥æ—§æ ¼å¼ï¼š*.json
                if valid_ts == 0:
                    ts_files = list(ts_dir.glob("*.json"))
                    valid_ts = len(ts_files)

                if valid_ts > 0:
                    structure['has_troubleshooting'] = True
                    structure['troubleshooting_count'] = valid_ts

            # æ£€æŸ¥mediaç›®å½•
            media_dir = local_path / "media"
            if media_dir.exists():
                media_files = list(media_dir.glob("*"))
                if media_files:
                    structure['has_media'] = True
                    structure['media_count'] = len(media_files)

        except Exception as e:
            self.logger.error(f"æ¨æ–­æ–‡ä»¶ç»“æ„å¤±è´¥: {e}")

        return structure

    def add_to_cache(self, url, local_path, guides_count=0, troubleshooting_count=0, media_count=0):
        """æ·»åŠ URLåˆ°ç¼“å­˜ç´¢å¼•"""
        url_hash = self.get_url_hash(url)

        # åˆ†ææœ¬åœ°æ•°æ®ç»“æ„
        structure = self._analyze_local_structure(local_path, guides_count, troubleshooting_count, media_count)

        cache_entry = {
            'url': url,
            'local_path': str(local_path.relative_to(self.storage_root)),
            'processed_time': datetime.now(timezone.utc).isoformat(),
            'content_hash': self._generate_content_hash(local_path),
            'structure': structure,
            'status': 'complete'
        }

        self.cache_index[url_hash] = cache_entry
        self.logger.info(f"å·²æ·»åŠ åˆ°ç¼“å­˜: {url}")

    def _analyze_local_structure(self, local_path, guides_count=0, troubleshooting_count=0, media_count=0):
        """åˆ†ææœ¬åœ°æ•°æ®ç»“æ„ - ä¸å†åŒ…å«troubleshootingä¿¡æ¯ï¼Œç”±ç‹¬ç«‹ç¼“å­˜ç®¡ç†"""
        structure = {
            'has_guides': False,
            'guides_count': 0,
            'has_media': False,
            'media_count': 0
        }

        try:
            # æ£€æŸ¥guidesç›®å½•
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                guide_files = list(guides_dir.glob("*.json"))
                structure['has_guides'] = len(guide_files) > 0
                structure['guides_count'] = len(guide_files)
            elif guides_count > 0:
                structure['has_guides'] = True
                structure['guides_count'] = guides_count

            # troubleshootingä¿¡æ¯ä¸å†è®°å½•åœ¨cache_indexä¸­ï¼Œç”±troubleshooting_cache.jsonç‹¬ç«‹ç®¡ç†

            # æ£€æŸ¥mediaç›®å½•
            media_dir = local_path / "media"
            if media_dir.exists():
                media_files = list(media_dir.glob("*"))
                structure['has_media'] = len(media_files) > 0
                structure['media_count'] = len(media_files)
            elif media_count > 0:
                structure['has_media'] = True
                structure['media_count'] = media_count

        except Exception as e:
            self.logger.error(f"åˆ†ææœ¬åœ°ç»“æ„æ—¶å‡ºé”™: {e}")

        return structure

    def _find_actual_device_path(self, device_url):
        """æŸ¥æ‰¾è®¾å¤‡URLå¯¹åº”çš„å®é™…å­˜åœ¨è·¯å¾„"""
        try:
            # ä»URLæå–è®¾å¤‡åç§°
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_name = unquote(device_path)

            # åœ¨Deviceç›®å½•ä¸‹æœç´¢åŒ…å«è®¾å¤‡åç§°çš„ç›®å½•
            device_root = self.storage_root / "Device"
            if not device_root.exists():
                return None

            # ç”Ÿæˆå¯èƒ½çš„è®¾å¤‡åç§°å˜ä½“
            name_variants = [
                device_name,
                device_name.replace('%22', '"'),
                device_name.replace('"', '%22'),
                device_path,  # åŸå§‹ç¼–ç ç‰ˆæœ¬
                unquote(device_path)  # è§£ç ç‰ˆæœ¬
            ]

            # ä¼˜å…ˆæŸ¥æ‰¾ç›´æ¥åŒ¹é…çš„ç›®å½•
            for variant in name_variants:
                direct_path = device_root / variant
                if direct_path.exists():
                    self.logger.info(f"æ‰¾åˆ°ç›´æ¥åŒ¹é…è·¯å¾„: {direct_path}")
                    return direct_path

            # é€’å½’æœç´¢åŒ¹é…çš„ç›®å½•
            self.logger.info(f"å¼€å§‹é€’å½’æœç´¢è®¾å¤‡è·¯å¾„ï¼ŒæŸ¥æ‰¾: {name_variants}")

            for path in device_root.rglob("*"):
                if path.is_dir():
                    # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ¹é…ä»»ä½•å˜ä½“
                    for variant in name_variants:
                        if path.name == variant:
                            self.logger.info(f"æ‰¾åˆ°åç§°åŒ¹é…è·¯å¾„: {path}")
                            return path

                    # æ£€æŸ¥æ˜¯å¦æœ‰troubleshooting_cache.jsonæ–‡ä»¶
                    ts_cache_file = path / "troubleshooting_cache.json"
                    if ts_cache_file.exists():
                        # éªŒè¯è¿™æ˜¯å¦æ˜¯æ­£ç¡®çš„è®¾å¤‡ç›®å½•
                        path_str = str(path)
                        for variant in name_variants:
                            if variant in path_str:
                                self.logger.info(f"é€šè¿‡troubleshooting_cache.jsonæ‰¾åˆ°åŒ¹é…è·¯å¾„: {path}")
                                return path

            self.logger.warning(f"æœªæ‰¾åˆ°è®¾å¤‡è·¯å¾„ï¼Œæœç´¢çš„å˜ä½“: {name_variants}")
            return None

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾å®é™…è®¾å¤‡è·¯å¾„å¤±è´¥: {e}")
            return None

    def _generate_content_hash(self, local_path):
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼"""
        try:
            info_file = local_path / "info.json"
            if info_file.exists():
                with open(info_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œæ—¶å‡ºé”™: {e}")
        return ""

    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def is_troubleshooting_section_cached(self, cache_key, device_url):
        """æ£€æŸ¥troubleshootingéƒ¨åˆ†æ˜¯å¦å·²ç¼“å­˜"""
        try:
            # ä½¿ç”¨ä¸save_troubleshooting_cacheç›¸åŒçš„è·¯å¾„é€»è¾‘
            local_path = self._get_troubleshooting_cache_path(cache_key, device_url)
            ts_cache_file = local_path / "troubleshooting_cache.json"

            if not ts_cache_file.exists():
                self.logger.info(f"ğŸ” Troubleshootingç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {ts_cache_file}")
                return False

            # éªŒè¯ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§
            with open(ts_cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            # æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if not isinstance(cached_data, list):
                self.logger.info(f"ğŸ” Troubleshootingç¼“å­˜æ•°æ®æ ¼å¼æ— æ•ˆ")
                return False

            self.logger.info(f"âœ… Troubleshootingéƒ¨åˆ†ç¼“å­˜æœ‰æ•ˆ: {len(cached_data)} ä¸ªé¡¹ç›®")
            return True

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥troubleshootingç¼“å­˜å¤±è´¥: {e}")
            return False

    def _get_troubleshooting_cache_path(self, cache_key, device_url):
        """è·å–troubleshootingç¼“å­˜çš„è·¯å¾„ - ä¸Troubleshootingæ–‡ä»¶å¤¹åœ¨åŒä¸€å±‚çº§"""
        try:
            # troubleshooting_cache.jsonåº”è¯¥ä¸troubleshootingæ–‡ä»¶å¤¹åœ¨åŒä¸€å±‚çº§
            if '/Device/' in device_url:
                # é¦–å…ˆå°è¯•æŸ¥æ‰¾å®é™…å­˜åœ¨troubleshootingæ–‡ä»¶å¤¹çš„è·¯å¾„
                actual_path = self._find_troubleshooting_directory_path(device_url)
                if actual_path:
                    self.logger.info(f"âœ… æ‰¾åˆ°è®¾å¤‡ç›®å½•ç”¨äºtroubleshootingç¼“å­˜: {actual_path}")
                    return actual_path

                # å¦‚æœæ²¡æ‰¾åˆ°å®é™…çš„troubleshootingç›®å½•ï¼Œé¢„æµ‹å°†æ¥çš„è®¾å¤‡ç›®å½•è·¯å¾„
                predicted_path = self._predict_device_directory_path(device_url)
                if predicted_path:
                    self.logger.info(f"ğŸ”® é¢„æµ‹è®¾å¤‡ç›®å½•ç”¨äºtroubleshootingç¼“å­˜: {predicted_path}")
                    return predicted_path

                # æœ€åçš„å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å“ˆå¸Œè·¯å¾„
                self.logger.info(f"ğŸ” æ— æ³•é¢„æµ‹è®¾å¤‡ç›®å½•ï¼Œä½¿ç”¨å“ˆå¸Œè·¯å¾„")
                url_hash = self.get_url_hash(device_url)
                fallback_path = Path(self.storage_root) / "cache" / url_hash[:8]
                return fallback_path
            else:
                # éè®¾å¤‡URLä½¿ç”¨å“ˆå¸Œè·¯å¾„
                url_hash = self.get_url_hash(device_url)
                fallback_path = Path(self.storage_root) / "cache" / url_hash[:8]
                return fallback_path
        except Exception as e:
            self.logger.error(f"è·å–troubleshootingç¼“å­˜è·¯å¾„å¤±è´¥: {e}")
            # ç¡®ä¿æ€»æ˜¯è¿”å›ä¸€ä¸ªæœ‰æ•ˆè·¯å¾„
            url_hash = self.get_url_hash(device_url)
            fallback_path = Path(self.storage_root) / "cache" / url_hash[:8]
            return fallback_path

    def _clean_directory_name(self, name):
        """æ¸…ç†ç›®å½•åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        if not name:
            return "unknown"
        
        # æ›¿æ¢éæ³•å­—ç¬¦
        safe_name = name.replace("/", "_").replace("\\", "_").replace(":", "_")
        safe_name = safe_name.replace('"', '_').replace("'", "_").replace("?", "_")
        safe_name = safe_name.replace("*", "_").replace("|", "_").replace("<", "_").replace(">", "_")
        
        # ç¡®ä¿åç§°ä¸ä¸ºç©º
        if not safe_name or safe_name.isspace():
            return "unknown"
            
        return safe_name

    def _build_device_path_from_url(self, device_url):
        """åŸºäºURLæ„å»ºè®¾å¤‡è·¯å¾„ - ä¸å®é™…æ–‡ä»¶ä¿å­˜é€»è¾‘ä¿æŒä¸€è‡´"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„è·¯å¾„ä¿¡æ¯
            device_hash = self.get_url_hash(device_url)
            if device_hash in self.cache_index:
                cache_entry = self.cache_index[device_hash]
                cached_path = self.storage_root / cache_entry.get('local_path', '')
                if cached_path.exists():
                    return cached_path

            # ä»URLä¸­æå–è®¾å¤‡è·¯å¾„
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)

            # å§‹ç»ˆä½¿ç”¨å®Œæ•´çš„å±‚çº§è·¯å¾„ç»“æ„
            path_parts = device_path.split('/')
            
            # æ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„
            result_path = self.storage_root / "Device"
            for part in path_parts:
                if part:  # è·³è¿‡ç©ºéƒ¨åˆ†
                    safe_part = self._clean_directory_name(part)
                    result_path = result_path / safe_part
            
            return result_path

        except Exception as e:
            self.logger.error(f"æ„å»ºè®¾å¤‡è·¯å¾„å¤±è´¥: {e}")
            # æœ€ç»ˆå›é€€åˆ°ç®€å•è·¯å¾„ï¼Œä½†ä»ç„¶å°è¯•ä¿æŒå±‚çº§ç»“æ„
            device_path = device_url.split('/Device/')[-1].split('?')[0].rstrip('/')
            path_parts = device_path.split('/')
            
            result_path = self.storage_root / "Device"
            for part in path_parts:
                if part:  # è·³è¿‡ç©ºéƒ¨åˆ†
                    safe_part = self._clean_directory_name(part)
                    result_path = result_path / safe_part
            
            return result_path

    def _find_troubleshooting_directory_path(self, device_url):
        """æŸ¥æ‰¾å®é™…å­˜åœ¨troubleshootingæ–‡ä»¶å¤¹çš„è·¯å¾„"""
        try:
            # ä»device_urlæå–è®¾å¤‡è·¯å¾„ä¿¡æ¯
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)

            # åœ¨Deviceç›®å½•ä¸‹é€’å½’æœç´¢åŒ…å«troubleshootingæ–‡ä»¶å¤¹çš„ç›®å½•
            device_root = self.storage_root / "Device"
            if not device_root.exists():
                return None

            device_name = device_path.split('/')[-1]
            self.logger.info(f"æœç´¢troubleshootingæ–‡ä»¶å¤¹ï¼Œè®¾å¤‡åç§°: {device_name}")

            # é¦–å…ˆå°è¯•ç›´æ¥æ„å»ºè·¯å¾„
            direct_path = self.storage_root / "Device" / device_path
            if direct_path.exists():
                self.logger.info(f"ç›´æ¥æ‰¾åˆ°è®¾å¤‡ç›®å½•: {direct_path}")
                # å³ä½¿troubleshootingç›®å½•ä¸å­˜åœ¨ï¼Œä¹Ÿè¿”å›è®¾å¤‡ç›®å½•ï¼Œä»¥ä¾¿åç»­åˆ›å»ºcacheæ–‡ä»¶
                return direct_path

            # é€’å½’æœç´¢troubleshootingæ–‡ä»¶å¤¹
            for path in device_root.rglob("troubleshooting"):
                if path.is_dir():
                    parent_path = path.parent

                    # æ£€æŸ¥çˆ¶ç›®å½•åæ˜¯å¦åŒ¹é…è®¾å¤‡åç§°
                    if parent_path.name.lower() == device_name.lower():
                        self.logger.info(f"é€šè¿‡è®¾å¤‡åç§°åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

                    # æ£€æŸ¥URLç¼–ç çš„ç‰ˆæœ¬
                    if device_name.replace('"', '%22') == parent_path.name or device_name.replace('%22', '"') == parent_path.name:
                        self.logger.info(f"é€šè¿‡URLç¼–ç åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

                    # æ£€æŸ¥è·¯å¾„å±‚çº§æ˜¯å¦åŒ¹é…è®¾å¤‡è·¯å¾„ç»“æ„
                    relative_path = parent_path.relative_to(device_root)
                    relative_path_str = str(relative_path)

                    # æ£€æŸ¥è®¾å¤‡è·¯å¾„æ˜¯å¦åŒ…å«åœ¨ç›¸å¯¹è·¯å¾„ä¸­ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
                    if device_path.lower() == relative_path_str.lower():
                        self.logger.info(f"é€šè¿‡å®Œæ•´è·¯å¾„åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

                    # æ£€æŸ¥ç›¸å¯¹è·¯å¾„æ˜¯å¦ä»¥è®¾å¤‡åç§°ç»“å°¾
                    if relative_path_str.lower().endswith(device_name.lower()):
                        self.logger.info(f"é€šè¿‡è·¯å¾„ç»“å°¾åŒ¹é…æ‰¾åˆ°troubleshootingç›®å½•: {parent_path}")
                        return parent_path

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°troubleshootingç›®å½•ï¼Œå°è¯•æŸ¥æ‰¾è®¾å¤‡ç›®å½•
            for path in device_root.rglob("*"):
                if path.is_dir() and path.name.lower() == device_name.lower():
                    self.logger.info(f"æ‰¾åˆ°è®¾å¤‡ç›®å½•: {path}")
                    return path

            self.logger.info(f"æœªæ‰¾åˆ°troubleshootingæ–‡ä»¶å¤¹ï¼Œè®¾å¤‡è·¯å¾„: {device_path}")
            return None

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾troubleshootingç›®å½•å¤±è´¥: {e}")
            return None

    def _predict_device_directory_path(self, device_url):
        """é¢„æµ‹è®¾å¤‡ç›®å½•çš„è·¯å¾„ï¼ŒåŸºäºURLç»“æ„å’Œç°æœ‰çš„ç›®å½•æ¨¡å¼"""
        try:
            # ä»device_urlæå–è®¾å¤‡è·¯å¾„ä¿¡æ¯
            device_path = device_url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)
            device_name = device_path.split('/')[-1]  # è·å–è®¾å¤‡åç§°

            # æ„å»ºé¢„æœŸçš„è®¾å¤‡ç›®å½•è·¯å¾„
            device_root = self.storage_root / "Device"

            # æ–¹æ³•1: æŸ¥æ‰¾ç°æœ‰çš„è®¾å¤‡ç›®å½•ï¼ˆæœ€å‡†ç¡®çš„æ–¹æ³•ï¼‰
            if device_root.exists():
                # é€’å½’æœç´¢åŒ¹é…çš„è®¾å¤‡ç›®å½•
                for existing_path in device_root.rglob("*"):
                    if existing_path.is_dir() and existing_path.name == device_name:
                        self.logger.info(f"æ‰¾åˆ°ç°æœ‰è®¾å¤‡ç›®å½•: {existing_path}")
                        return existing_path

                # æœç´¢åŒ…å«è®¾å¤‡åç§°çš„ç›®å½•ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
                for existing_path in device_root.rglob("*"):
                    if existing_path.is_dir():
                        # æ£€æŸ¥ç›®å½•åæ˜¯å¦ä¸è®¾å¤‡åç§°åŒ¹é…ï¼ˆè€ƒè™‘URLç¼–ç ï¼‰
                        if (existing_path.name.lower() == device_name.lower() or
                            existing_path.name.replace('_', ' ').lower() == device_name.replace('_', ' ').lower() or
                            existing_path.name.replace('%22', '"').lower() == device_name.replace('%22', '"').lower()):
                            self.logger.info(f"æ‰¾åˆ°åŒ¹é…çš„è®¾å¤‡ç›®å½•: {existing_path}")
                            return existing_path

            # æ–¹æ³•2: åŸºäºç°æœ‰ç›®å½•ç»“æ„é¢„æµ‹è·¯å¾„
            if device_root.exists():
                # æŸ¥æ‰¾ç›¸ä¼¼çš„ç›®å½•ç»“æ„æ¨¡å¼
                for existing_path in device_root.rglob("*"):
                    if existing_path.is_dir():
                        relative_path = existing_path.relative_to(device_root)
                        path_parts = str(relative_path).split('/')

                        # å¦‚æœæ‰¾åˆ°æ·±åº¦ç›¸ä¼¼çš„ç›®å½•ç»“æ„ï¼Œå°è¯•æ„å»ºç›¸ä¼¼è·¯å¾„
                        if len(path_parts) >= 3:  # è‡³å°‘æœ‰3å±‚æ·±åº¦çš„ç›®å½•
                            # å°è¯•åŸºäºç°æœ‰ç»“æ„æ„å»ºè·¯å¾„
                            # ä¾‹å¦‚ï¼šMac/Mac_Laptop/MacBook_Pro/MacBook_Pro_16/device_name
                            parent_structure = '/'.join(path_parts[:-1])
                            predicted_path = device_root / parent_structure / device_name

                            # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨
                            if predicted_path.parent.exists():
                                self.logger.info(f"é¢„æµ‹è·¯å¾„åŸºäºç°æœ‰ç»“æ„: {predicted_path}")
                                return predicted_path

            # æ–¹æ³•3: ç›´æ¥ä½¿ç”¨URLè·¯å¾„ç»“æ„
            predicted_path = device_root / device_path
            if predicted_path.parent.exists():  # å¦‚æœçˆ¶ç›®å½•å­˜åœ¨ï¼Œè¯´æ˜è·¯å¾„ç»“æ„åˆç†
                self.logger.info(f"é¢„æµ‹è·¯å¾„åŸºäºURLç»“æ„: {predicted_path}")
                return predicted_path

            # æ–¹æ³•4: ä½¿ç”¨ç®€å•çš„è®¾å¤‡åç§°è·¯å¾„ï¼ˆæœ€åçš„å›é€€æ–¹æ¡ˆï¼‰
            simple_path = device_root / device_name
            self.logger.info(f"é¢„æµ‹è·¯å¾„ä½¿ç”¨ç®€å•ç»“æ„: {simple_path}")
            return simple_path

        except Exception as e:
            self.logger.error(f"é¢„æµ‹è®¾å¤‡ç›®å½•è·¯å¾„å¤±è´¥: {e}")
            return None

    def load_troubleshooting_cache(self, cache_key, device_url):
        """åŠ è½½ç¼“å­˜çš„troubleshootingæ•°æ®"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„è·å–æ–¹æ³•
            local_path = self._get_troubleshooting_cache_path(cache_key, device_url)
            ts_cache_file = local_path / "troubleshooting_cache.json"

            if not ts_cache_file.exists():
                return None

            with open(ts_cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            self.logger.info(f"ğŸ“‹ åŠ è½½troubleshootingç¼“å­˜: {len(cached_data)} ä¸ªé¡¹ç›®")
            self.stats['cache_hits'] += 1
            return cached_data

        except Exception as e:
            self.logger.error(f"åŠ è½½troubleshootingç¼“å­˜å¤±è´¥: {e}")
            return None

    def save_troubleshooting_cache(self, cache_key, device_url, troubleshooting_data):
        """ä¿å­˜troubleshootingæ•°æ®åˆ°ç¼“å­˜"""
        try:
            if not troubleshooting_data:
                return

            # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„è·å–æ–¹æ³•
            local_path = self._get_troubleshooting_cache_path(cache_key, device_url)

            # ç°åœ¨_get_troubleshooting_cache_pathæ€»æ˜¯è¿”å›ä¸€ä¸ªè·¯å¾„ï¼Œä¸ä¼šè¿”å›None
            if local_path is None:
                self.logger.error(f"âŒ æ— æ³•ç¡®å®štroubleshootingç¼“å­˜è·¯å¾„")
                return

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path.mkdir(parents=True, exist_ok=True)

            ts_cache_file = local_path / "troubleshooting_cache.json"

            # ç¼“å­˜ä¿æŠ¤æœºåˆ¶ï¼šå¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ä¸”ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if not self.force_refresh and ts_cache_file.exists():
                try:
                    with open(ts_cache_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)

                    # å¦‚æœç°æœ‰æ•°æ®ä¸ä¸ºç©ºä¸”æ–°æ•°æ®ä¸ç°æœ‰æ•°æ®ç›¸åŒï¼Œåˆ™è·³è¿‡ä¿å­˜
                    if existing_data and len(existing_data) >= len(troubleshooting_data):
                        self.logger.info(f"ğŸ”’ ä¿æŠ¤ç°æœ‰troubleshootingç¼“å­˜: {len(existing_data)} ä¸ªé¡¹ç›® (è·³è¿‡è¦†ç›–)")
                        return
                except Exception as e:
                    self.logger.warning(f"è¯»å–ç°æœ‰troubleshootingç¼“å­˜å¤±è´¥ï¼Œå°†ä¿å­˜æ–°æ•°æ®: {e}")

            # ä¿å­˜troubleshootingæ•°æ®
            with open(ts_cache_file, 'w', encoding='utf-8') as f:
                safe_json_dump(troubleshooting_data, f, ensure_ascii=False, indent=2)

            # troubleshootingç¼“å­˜ç°åœ¨å®Œå…¨ç‹¬ç«‹ï¼Œä¸å†æ·»åŠ åˆ°ä¸»ç¼“å­˜ç´¢å¼•
            self.logger.info(f"ğŸ’¾ ä¿å­˜troubleshootingç¼“å­˜: {len(troubleshooting_data)} ä¸ªé¡¹ç›®")

        except Exception as e:
            self.logger.error(f"ä¿å­˜troubleshootingç¼“å­˜å¤±è´¥: {e}")

    def save_troubleshooting_cache_after_directory_creation(self, cache_key, device_url, troubleshooting_data, ts_dir):
        """åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶"""
        try:
            if not troubleshooting_data or not ts_dir:
                self.logger.warning(f"è·³è¿‡ä¿å­˜troubleshootingç¼“å­˜: æ•°æ®ä¸ºç©ºæˆ–ç›®å½•æ— æ•ˆ")
                return

            # è·å–Troubleshootingç›®å½•çš„çˆ¶ç›®å½•ï¼ˆä¸Troubleshootingæ–‡ä»¶å¤¹åŒå±‚ï¼‰
            cache_dir = ts_dir.parent
            ts_cache_file = cache_dir / "troubleshooting_cache.json"

            self.logger.info(f"ğŸ” å‡†å¤‡ä¿å­˜troubleshootingç¼“å­˜åˆ°: {ts_cache_file}")

            # ç®€åŒ–ç¼“å­˜ä¿æŠ¤æœºåˆ¶ï¼šåªåœ¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ä¸‹è·³è¿‡ç°æœ‰æ–‡ä»¶
            if not self.force_refresh and ts_cache_file.exists():
                try:
                    with open(ts_cache_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)

                    # åªæœ‰åœ¨ç°æœ‰æ•°æ®æ˜æ˜¾æ›´å¤šæ—¶æ‰è·³è¿‡
                    if existing_data and len(existing_data) > len(troubleshooting_data) * 1.5:
                        self.logger.info(f"ğŸ”’ ä¿æŠ¤ç°æœ‰troubleshootingç¼“å­˜: {len(existing_data)} ä¸ªé¡¹ç›® (è·³è¿‡è¦†ç›–)")
                        return
                    else:
                        self.logger.info(f"ğŸ”„ æ›´æ–°troubleshootingç¼“å­˜: {len(existing_data)} -> {len(troubleshooting_data)} ä¸ªé¡¹ç›®")
                except Exception as e:
                    self.logger.warning(f"è¯»å–ç°æœ‰troubleshootingç¼“å­˜å¤±è´¥ï¼Œå°†ä¿å­˜æ–°æ•°æ®: {e}")

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            cache_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜troubleshootingæ•°æ®
            with open(ts_cache_file, 'w', encoding='utf-8') as f:
                safe_json_dump(troubleshooting_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜troubleshootingç¼“å­˜: {ts_cache_file} ({len(troubleshooting_data)} ä¸ªé¡¹ç›®)")
            print(f"   ğŸ’¾ å·²ç”Ÿæˆtroubleshooting_cache.json: {ts_cache_file}")

        except Exception as e:
            self.logger.error(f"åœ¨ç›®å½•åˆ›å»ºåä¿å­˜troubleshootingç¼“å­˜å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    def _generate_content_hash_for_data(self, data):
        """ä¸ºæ•°æ®ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼"""
        try:
            content_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(content_str.encode('utf-8')).hexdigest()[:16]
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ•°æ®å“ˆå¸Œæ—¶å‡ºé”™: {e}")
            return ""

    def display_cache_report(self):
        """æ˜¾ç¤ºç¼“å­˜æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æ™ºèƒ½ç¼“å­˜æŠ¥å‘Š")
        print("="*60)
        print(f"æ€»URLæ•°é‡: {self.stats['total_urls']}")
        print(f"ç¼“å­˜å‘½ä¸­: {self.stats['cached_urls']} (è·³è¿‡å¤„ç†)")
        print(f"éœ€è¦å¤„ç†: {self.stats['new_urls']} (æ–°å¢æˆ–æ›´æ–°)")
        print(f"æ— æ•ˆç¼“å­˜: {self.stats['invalid_cache']} (éœ€è¦é‡æ–°å¤„ç†)")

        if self.stats['total_urls'] > 0:
            hit_rate = (self.stats['cached_urls'] / self.stats['total_urls']) * 100

        print("="*60)

    def clean_invalid_cache(self):
        """æ¸…ç†æ— æ•ˆçš„ç¼“å­˜æ¡ç›®"""
        invalid_keys = []

        for url_hash, cache_entry in self.cache_index.items():
            local_path = self.storage_root / cache_entry['local_path']
            if not local_path.exists() or not self._validate_cached_data(local_path, cache_entry):
                invalid_keys.append(url_hash)

        for key in invalid_keys:
            del self.cache_index[key]

        if invalid_keys:
            self.logger.info(f"å·²æ¸…ç† {len(invalid_keys)} ä¸ªæ— æ•ˆç¼“å­˜æ¡ç›®")
            self.save_cache_index()

        return len(invalid_keys)


class CombinedIFixitCrawler(EnhancedIFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", verbose=False, use_proxy=True,
                 use_cache=True, force_refresh=False, max_workers=16, max_retries=1,
                 download_videos=False, max_video_size_mb=50, max_connections=None,
                 timeout=3, request_delay=0.01, proxy_switch_freq=1, cache_ttl=24,
                 custom_user_agent=None, burst_mode=False, conservative_mode=False,
                 skip_images=False, debug_mode=False, show_stats=False, enable_resume=True):
        super().__init__(base_url, verbose)

        # ç«‹å³åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œç¡®ä¿loggerå¯ç”¨
        self._setup_logging()

        self.enable_resume = enable_resume
        self.tree_crawler = TreeCrawler(base_url, enable_resume=enable_resume, logger=self.logger, verbose=verbose)
        self.processed_nodes = set()
        self.target_url = None

        # æœ¬åœ°å­˜å‚¨é…ç½®ï¼ˆéœ€è¦åœ¨ç¼“å­˜ç®¡ç†å™¨ä¹‹å‰è®¾ç½®ï¼‰
        # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æ•°æ®ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º ifixit_data
        self.storage_root = os.getenv('IFIXIT_DATA_DIR', 'ifixit_data')
        self.media_folder = "media"

        # ğŸš€ é«˜æ€§èƒ½é…ç½®
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.max_connections = max_connections or (max_workers * 10)
        self.timeout = timeout
        self.request_delay = request_delay
        self.proxy_switch_freq = proxy_switch_freq
        self.cache_ttl = cache_ttl
        self.custom_user_agent = custom_user_agent
        self.burst_mode = burst_mode
        self.conservative_mode = conservative_mode
        self.skip_images = skip_images
        self.debug_mode = debug_mode
        self.show_stats = show_stats

        # ç¼“å­˜é…ç½®
        self.use_cache = use_cache
        self.force_refresh = force_refresh
        self.cache_manager = CacheManager(self.storage_root, self.logger, force_refresh) if use_cache else None

        # ä»£ç†æ± é…ç½®
        self.use_proxy = use_proxy
        # åˆ›å»ºä»£ç†æ± ï¼Œå¤§å°ä¸çº¿ç¨‹æ•°åŒ¹é…
        proxy_pool_size = max(max_workers, 10) if use_proxy else 0
        self.proxy_manager = TunnelProxyManager(pool_size=proxy_pool_size) if use_proxy else None
        self.failed_urls = set()  # æ·»åŠ å¤±è´¥URLé›†åˆ

        # å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨
        self.async_http_manager = None

        # è§†é¢‘å¤„ç†é…ç½®
        self.download_videos = download_videos
        self.max_video_size_mb = max_video_size_mb
        self.video_extensions = ['.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv']

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "retry_success": 0,
            "retry_failed": 0,
            "total_retries": 0,  # æ·»åŠ ç¼ºå¤±çš„total_retriesé”®
            "media_downloaded": 0,
            "media_failed": 0,
            "videos_skipped": 0,
            "videos_downloaded": 0,
            "errors": 0  # æ·»åŠ ç¼ºå¤±çš„errorsé”®
        }

        # å¹¶å‘é…ç½®
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.thread_local = threading.local()

        # é”™è¯¯å¤„ç†é…ç½®
        self.failed_log_file = "failed_urls.log"

        # Troubleshootingå¤„ç†çŠ¶æ€è·Ÿè¸ª
        self.processed_troubleshooting_paths = set()  # è·Ÿè¸ªå·²å¤„ç†troubleshootingçš„è·¯å¾„
        self._troubleshooting_from_cache = False  # æ ‡è®°troubleshootingæ•°æ®æ˜¯å¦æ¥è‡ªç¼“å­˜

        # åª’ä½“æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç¼“å­˜ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
        self._file_exists_cache = {}  # ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç»“æœ
        self._cache_max_size = 1000   # ç¼“å­˜æœ€å¤§å¤§å°

        # æ‰“å°è§†é¢‘å¤„ç†é…ç½®
        if self.download_videos:
            self.logger.info(f"âœ… è§†é¢‘ä¸‹è½½å·²å¯ç”¨ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {max_video_size_mb}MB")
        else:
            self.logger.info("âš ï¸ è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå°†ä¿ç•™åŸå§‹URL")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ­£ç¡®æ¸…ç†"""
        try:
            self.cleanup()
        except Exception:
            pass  # é™é»˜å¤„ç†ææ„å‡½æ•°ä¸­çš„é”™è¯¯

    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        try:
            # æ¸…ç†å¼‚æ­¥HTTPç®¡ç†å™¨
            if self.async_http_manager:
                try:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
                    import asyncio
                    try:
                        loop = asyncio.get_running_loop()
                        if not loop.is_closed():
                            # å¦‚æœåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œåˆ›å»ºä»»åŠ¡æ¥æ¸…ç†
                            asyncio.create_task(self._cleanup_async_resources())
                    except RuntimeError:
                        # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥æ¸…ç†
                        asyncio.run(self._cleanup_async_resources())
                except Exception:
                    pass  # é™é»˜å¤„ç†æ¸…ç†é”™è¯¯
                finally:
                    self.async_http_manager = None
        except Exception:
            pass  # é™é»˜å¤„ç†æ‰€æœ‰æ¸…ç†é”™è¯¯

    async def _cleanup_async_resources(self):
        """å¼‚æ­¥æ¸…ç†èµ„æº"""
        try:
            if self.async_http_manager:
                await self.async_http_manager._close_client()
        except Exception:
            pass  # é™é»˜å¤„ç†æ¸…ç†é”™è¯¯

    def _load_proxy_config(self):
        """åŠ è½½ä»£ç†é…ç½®"""

        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()

        # éš§é“ä»£ç†å·²åœ¨__init__ä¸­åˆå§‹åŒ–ï¼Œæ— éœ€é¢å¤–é…ç½®

        # å¤åˆ¶æ ‘å½¢çˆ¬è™«çš„é‡è¦æ–¹æ³•åˆ°å½“å‰ç±»
        self.find_exact_path = self.tree_crawler.find_exact_path
        self.extract_breadcrumbs_from_page = self.tree_crawler.extract_breadcrumbs_from_page
        self._build_tree_from_path = self.tree_crawler._build_tree_from_path
        self._find_node_by_url = self.tree_crawler._find_node_by_url
        self.ensure_instruction_url_in_leaf_nodes = self.tree_crawler.ensure_instruction_url_in_leaf_nodes

        # é‡å†™tree_crawlerçš„get_soupæ–¹æ³•ï¼Œè®©å®ƒä½¿ç”¨æˆ‘ä»¬çš„å¢å¼ºç‰ˆæœ¬
        self.tree_crawler.get_soup = self._tree_crawler_get_soup

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO if self.verbose else logging.WARNING,
            format=log_format,
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _check_cache_validity(self, url, local_path):
        """æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ - ä½¿ç”¨æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
        if not self.use_cache or self.force_refresh:
            return False

        # ä½¿ç”¨CacheManagerè¿›è¡Œæ™ºèƒ½ç¼“å­˜æ£€æŸ¥
        if self.cache_manager:
            return self.cache_manager.is_url_cached_and_valid(url, local_path)

        # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰çš„ç®€å•ç¼“å­˜æ£€æŸ¥
        return self._legacy_cache_check(url, local_path)

    def _legacy_cache_check(self, url, local_path):
        """åŸæœ‰çš„ç¼“å­˜æ£€æŸ¥é€»è¾‘ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        if not local_path.exists():
            return False

        # æ£€æŸ¥info.jsonæ˜¯å¦å­˜åœ¨
        info_file = local_path / "info.json"
        if not info_file.exists():
            self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘info.json - {url}")
            return False

        try:
            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            # æ£€æŸ¥guidesç›®å½•
            if 'guides' in info_data:
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘guidesç›®å½• - {url}")
                    return False

            # æ£€æŸ¥troubleshootingç›®å½•
            if 'troubleshooting' in info_data:
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘troubleshootingç›®å½• - {url}")
                    return False

            # æ£€æŸ¥mediaæ–‡ä»¶
            media_dir = local_path / self.media_folder
            if media_dir.exists():
                # ç®€å•æ£€æŸ¥ï¼šå¦‚æœmediaç›®å½•å­˜åœ¨ä½†ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ä¸‹è½½å¤±è´¥
                media_files = list(media_dir.glob("*"))
                if len(media_files) == 0:
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶
                    if self._should_have_media(info_data):
                        self.logger.info(f"ç¼“å­˜æ— æ•ˆ: mediaç›®å½•ä¸ºç©ºä½†åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶ - {url}")
                        return False

            self.logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡çˆ¬å–: {url}")
            return True

        except Exception as e:
            self.logger.error(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e} - {url}")
            return False

    def _should_have_media(self, data):
        """æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åº”è¯¥åŒ…å«åª’ä½“æ–‡ä»¶"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and value:
                    return True
                elif key == 'images' and isinstance(value, list) and value:
                    return True
                elif isinstance(value, (dict, list)):
                    if self._should_have_media(value):
                        return True
        elif isinstance(data, list):
            for item in data:
                if self._should_have_media(item):
                    return True
        return False

    def _log_failed_url(self, url, error, retry_count=0):
        """è®°å½•å¤±è´¥çš„URLåˆ°æ—¥å¿—æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
        safe_error = safe_str(error)
        safe_url = safe_str(url)
        log_entry = f"{timestamp} | {safe_url} | {safe_error} | retry_count: {retry_count}\n"

        try:
            with open(self.failed_log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e)
            self.logger.error(f"æ— æ³•å†™å…¥å¤±è´¥æ—¥å¿—: {error_msg}")

    def _log_failed_media(self, url, error_msg):
        """è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—"""
        try:
            failed_media_log = "failed_media.log"
            with open(failed_media_log, 'a', encoding='utf-8') as f:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                # ç¡®ä¿error_msgä¸ä¸ºNoneï¼Œé¿å…æ ¼å¼åŒ–é”™è¯¯
                safe_error_msg = safe_str(error_msg) if error_msg is not None else "Unknown error"
                safe_url = safe_str(url) if url is not None else "Unknown URL"
                log_line = f"[{timestamp}] {safe_url} - {safe_error_msg}\n"
                f.write(log_line)
        except Exception as e:
            # é¿å…åœ¨é”™è¯¯å¤„ç†ä¸­å†æ¬¡å‡ºç°æ ¼å¼åŒ–é”™è¯¯
            try:
                error_msg_str = safe_str(e)
                if self.logger:
                    self.logger.error(f"æ— æ³•å†™å…¥åª’ä½“å¤±è´¥æ—¥å¿—: {error_msg_str}")
            except:
                pass  # é™é»˜å¤„ç†ï¼Œé¿å…çº§è”é”™è¯¯

    def _exponential_backoff(self, attempt):
        """ä¼˜åŒ–é‡è¯•ç­–ç•¥ - å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§"""
        # ä¼˜åŒ–æ¨¡å¼ï¼šåˆç†å»¶è¿Ÿï¼Œå‡å°‘ä»£ç†åˆ‡æ¢
        base_delay = 0.5  # åŸºç¡€å»¶è¿Ÿ0.5ç§’
        linear_increment = 0.3  # æ¯æ¬¡é‡è¯•å¢åŠ 0.3ç§’
        max_delay = 2  # æœ€å¤§å»¶è¿Ÿ2ç§’

        delay = min(max_delay, base_delay + (attempt * linear_increment))
        time.sleep(delay)
        return delay

    def _is_temporary_error(self, error):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶æ€§é”™è¯¯"""
        temporary_errors = [
            "timeout", "connection", "network", "502", "503", "504",
            "429", "500", "ConnectionError", "Timeout", "ReadTimeout"
        ]
        error_str = str(error).lower()
        return any(temp_err in error_str for temp_err in temporary_errors)

    def _retry_with_backoff(self, func, *args, **kwargs):
        """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶"""
        last_error = None

        for attempt in range(self.max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    self.stats["retry_success"] += 1
                    self.logger.info(f"é‡è¯•æˆåŠŸ (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                return result

            except Exception as e:
                last_error = e
                self.stats["total_retries"] += 1

                if not self._is_temporary_error(e):
                    self.logger.warning(f"æ°¸ä¹…æ€§é”™è¯¯ï¼Œè·³è¿‡æ­¤é¡¹: {e}")
                    # å¯¹äºæ°¸ä¹…æ€§é”™è¯¯ï¼Œè¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    return None

                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    self.logger.warning(f"é‡è¯• {attempt+1}/{self.max_retries} (å»¶è¿Ÿ{delay:.1f}s): {e}")

                    # ç½‘ç»œé”™è¯¯æ—¶åˆ‡æ¢ä»£ç†
                    if self.use_proxy and ("network" in str(e).lower() or "proxy" in str(e).lower() or "timeout" in str(e).lower()):
                        self._switch_proxy(f"é‡è¯•æ—¶ç½‘ç»œé”™è¯¯: {str(e)}")

                    time.sleep(delay)
                else:
                    self.logger.warning(f"é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤é¡¹: {e}")

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›Noneè®©è°ƒç”¨è€…ç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡
        self.stats["retry_failed"] += 1
        if last_error:
            self.logger.warning(f"ä»»åŠ¡å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡: {str(last_error)}")
            return None
        else:
            return None





    def _get_next_proxy(self):
        """è·å–å½“å‰ä»£ç†æˆ–åˆ‡æ¢æ–°ä»£ç†"""
        if not self.use_proxy or not self.proxy_manager:
            return None

        # æ£€æŸ¥å½“å‰çº¿ç¨‹æ˜¯å¦æœ‰æŒ‡å®šçš„ä»£ç†ID
        current_thread = threading.current_thread()
        thread_proxy_id = getattr(current_thread, 'proxy_id', None)

        # å¢åŠ ä»£ç†ä½¿ç”¨è®¡æ•°
        if hasattr(self.proxy_manager, 'proxy_switch_count'):
            self.proxy_manager.proxy_switch_count += 1

            # å¯¹äºå¤šä»£ç†æ± ï¼Œæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹ä»£ç†
            if thread_proxy_id is not None:
                if self.verbose and self.proxy_manager.proxy_switch_count % 10 == 0:
                    print(f"ğŸ”„ å·²å¤„ç† {self.proxy_manager.proxy_switch_count} ä¸ªè¯·æ±‚ï¼ˆå¤šä»£ç†å¹¶å‘ï¼‰")
            elif self.proxy_switch_freq == 1:
                # æ¯æ¬¡è¯·æ±‚éƒ½åˆ‡æ¢ï¼ˆéš§é“ä»£ç†çš„é»˜è®¤è¡Œä¸ºï¼‰
                if self.verbose and self.proxy_manager.proxy_switch_count % 10 == 0:
                    print(f"ğŸ”„ å·²å¤„ç† {self.proxy_manager.proxy_switch_count} ä¸ªè¯·æ±‚ï¼ˆæ¯æ¬¡è‡ªåŠ¨åˆ‡æ¢IPï¼‰")
            elif self.proxy_manager.proxy_switch_count % self.proxy_switch_freq == 0:
                # æŒ‰æŒ‡å®šé¢‘ç‡åˆ‡æ¢
                print(f"ğŸ”„ è¾¾åˆ°åˆ‡æ¢é¢‘ç‡ ({self.proxy_switch_freq})ï¼Œåˆ‡æ¢ä»£ç†IP")

        return self.proxy_manager.get_proxy(thread_proxy_id)

    def _switch_proxy(self, reason="è¯·æ±‚å¤±è´¥"):
        """åˆ‡æ¢ä»£ç†IP"""
        import time
        # å‡å°‘æ—¥å¿—è¾“å‡ºé¢‘ç‡ï¼Œé¿å…æ—¥å¿—æ±¡æŸ“
        if hasattr(self, '_last_proxy_switch_time'):
            current_time = time.time()
            if current_time - self._last_proxy_switch_time < 10:  # 10ç§’å†…ä¸é‡å¤è¾“å‡º
                pass  # é™é»˜å¤„ç†
            else:
                self.logger.info(f"ğŸ”„ ä»£ç†åˆ‡æ¢: {reason[:50]}")
                self._last_proxy_switch_time = current_time
        else:
            self.logger.info(f"ğŸ”„ ä»£ç†åˆ‡æ¢: {reason[:50]}")
            self._last_proxy_switch_time = time.time()

        if not self.use_proxy or not self.proxy_manager:
            return False

        # å¯¹äºå¤šä»£ç†æ± ï¼Œæ ‡è®°å¤±è´¥ä½†ä¸éœ€è¦åˆ‡æ¢ï¼ˆæ¯ä¸ªçº¿ç¨‹æœ‰ç‹¬ç«‹ä»£ç†ï¼‰
        self.proxy_manager.mark_proxy_failed(None, reason)

        # è·å–æ–°ä»£ç†
        new_proxy = self.proxy_manager.get_proxy()
        return new_proxy is not None

    # éš§é“ä»£ç†ç›¸å…³æ–¹æ³•å·²é›†æˆåˆ°TunnelProxyManagerç±»ä¸­

    async def _init_async_http_manager(self):
        """åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨"""
        try:
            if not self.async_http_manager:
                # ä¼˜åŒ–ï¼šæå‡è¿æ¥æ± å¤§å°ä»¥æ”¯æŒæ›´é«˜çš„åª’ä½“ä¸‹è½½å¹¶å‘
                max_connections = max(self.max_workers * 15, 200)  # æå‡è¿æ¥æ± å¤§å°
                max_keepalive = max(self.max_workers * 3, 50)      # æå‡ä¿æŒè¿æ¥æ•°

                self.async_http_manager = AsyncHttpClientManager(
                    proxy_manager=self.proxy_manager,
                    max_connections=max_connections,
                    max_keepalive_connections=max_keepalive,
                    timeout=5.0,  # å‡å°‘è¶…æ—¶æ—¶é—´ä»8ç§’åˆ°5ç§’
                    max_retries=1  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œå¿«é€Ÿå¤±è´¥
                )
                
                # ç¡®ä¿åˆå§‹åŒ–æˆåŠŸ
                init_success = await self.async_http_manager._init_client()
                
                # éªŒè¯åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
                if not init_success or not self.async_http_manager._client:
                    self.logger.error("å¼‚æ­¥HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
                    return False
                    
                # ç¡®ä¿ä¿¡å·é‡å·²åˆå§‹åŒ–
                if not self.async_http_manager._semaphore:
                    self.logger.error("å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¿¡å·é‡åˆå§‹åŒ–å¤±è´¥")
                    return False
                    
                return True
            return True
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å¼‚æ­¥HTTPç®¡ç†å™¨å¤±è´¥: {e}")
            return False

    async def _close_async_http_manager(self):
        """å…³é—­å¼‚æ­¥HTTPå®¢æˆ·ç«¯ç®¡ç†å™¨"""
        if self.async_http_manager:
            await self.async_http_manager._close_client()
            self.async_http_manager = None

    async def get_soup_async(self, url, use_playwright=False):
        """å¼‚æ­¥ç‰ˆæœ¬çš„get_soupæ–¹æ³•"""
        if not url:
            return None

        if url in self.failed_urls:
            self.logger.warning(f"è·³è¿‡å·²çŸ¥å¤±è´¥URL: {url}")
            return None

        # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        # ç¡®ä¿ä½¿ç”¨www.ifixit.comè€Œä¸æ˜¯å…¶ä»–å­åŸŸå
        if 'ifixit.com' in url and not url.startswith('https://www.ifixit.com'):
            url = url.replace('https://', 'https://www.').replace('http://', 'https://www.')
            if not url.startswith('https://www.ifixit.com'):
                url = url.replace('ifixit.com', 'www.ifixit.com')

        # æ·»åŠ å¼ºåˆ¶è‹±æ–‡å‚æ•°
        if '?' in url:
            if 'lang=' not in url:
                url += '&lang=en'
            else:
                # æ›¿æ¢ç°æœ‰çš„langå‚æ•°
                import re
                url = re.sub(r'lang=[^&]*', 'lang=en', url)
        else:
            url += '?lang=en'

        self.stats["total_requests"] += 1

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return await self._get_soup_with_playwright_async(url)

        # å¦åˆ™ä½¿ç”¨å¼‚æ­¥httpxæ–¹æ³•
        return await self._get_soup_httpx_async(url)

    async def _get_soup_httpx_async(self, url):
        """ä½¿ç”¨httpxå¼‚æ­¥è·å–é¡µé¢å†…å®¹"""
        # åˆå§‹åŒ–å¼‚æ­¥HTTPç®¡ç†å™¨
        init_success = await self._init_async_http_manager()
        if not init_success or not self.async_http_manager:
            self.logger.error(f"åˆå§‹åŒ–å¼‚æ­¥HTTPç®¡ç†å™¨å¤±è´¥ï¼Œæ— æ³•è·å–é¡µé¢ {url}")
            self.failed_urls.add(url)
            return None

        try:
            response = await self.async_http_manager.get_with_retry(url, headers=self.headers)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response is None:
                self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: å“åº”ä¸ºç©º")
                self.failed_urls.add(url)
                return None
                
            if response and response.status_code == 404:
                self.logger.warning(f"é¡µé¢ä¸å­˜åœ¨ {url}: 404 Not Found")
                self.failed_urls.add(url)
                # å¯¹äº404é”™è¯¯ï¼ŒæŠ›å‡ºç‰¹å®šå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
                raise httpx.HTTPStatusError("404 Not Found", request=response.request, response=response)

            # å¯¹äºå…¶ä»–é”™è¯¯çŠ¶æ€ç 
            if response and response.status_code >= 400:
                self.logger.error(f"HTTPé”™è¯¯ {url}: {response.status_code}")
                self.failed_urls.add(url)
                raise httpx.HTTPStatusError(f"HTTP {response.status_code}", request=response.request, response=response)

            from bs4 import BeautifulSoup
            return BeautifulSoup(response.content, 'html.parser')

        except httpx.HTTPStatusError:
            # ç›´æ¥å‘ä¸Šä¼ é€’HTTPçŠ¶æ€é”™è¯¯ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            raise

    async def _get_soup_with_playwright_async(self, url):
        """å¼‚æ­¥ç‰ˆæœ¬çš„Playwrighté¡µé¢è·å–"""
        try:
            from playwright.async_api import async_playwright

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´ - å¼ºåˆ¶è‹±æ–‡ç‰ˆæœ¬
                await page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=1.0',  # å¼ºåˆ¶è‹±æ–‡ï¼Œæé«˜ä¼˜å…ˆçº§
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Cookie': 'locale=en'  # æ·»åŠ è‹±æ–‡locale cookie
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                await page.goto(url, wait_until='networkidle')

                # å¼ºåˆ¶è®¾ç½®è‹±æ–‡è¯­è¨€
                await page.evaluate("""
                    // è®¾ç½®localStorageä¸­çš„è¯­è¨€åå¥½
                    localStorage.setItem('locale', 'en');
                    localStorage.setItem('language', 'en');
                    localStorage.setItem('lang', 'en');

                    // è®¾ç½®cookie
                    document.cookie = 'locale=en; path=/; domain=.ifixit.com';
                    document.cookie = 'language=en; path=/; domain=.ifixit.com';
                """)

                # ç­‰å¾…ä¸€ä¸‹è®©è®¾ç½®ç”Ÿæ•ˆ
                await page.wait_for_timeout(1000)

                # è·å–é¡µé¢å†…å®¹
                content = await page.content()
                await browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            self.logger.error(f"Playwrightå¼‚æ­¥è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return None

    async def _process_tasks_async(self, tasks, guides_data, troubleshooting_data):
        """å¼‚æ­¥å¹¶å‘å¤„ç†ä»»åŠ¡åˆ—è¡¨"""
        # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
        semaphore = asyncio.Semaphore(self.max_workers)

        async def process_single_task(task_type, url):
            """å¤„ç†å•ä¸ªä»»åŠ¡çš„å¼‚æ­¥åŒ…è£…å™¨"""
            async with semaphore:
                try:
                    if task_type == 'guide':
                        return await self._process_guide_task_async(url)
                    else:  # troubleshooting
                        return await self._process_troubleshooting_task_async(url)
                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")
                    return None

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        async_tasks = [process_single_task(task_type, url) for task_type, url in tasks]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*async_tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        completed_count = 0
        total_tasks = len(tasks)

        for i, (result, (task_type, url)) in enumerate(zip(results, tasks)):
            completed_count += 1

            if isinstance(result, Exception):
                print(f"    âŒ [{completed_count}/{total_tasks}] {task_type} ä»»åŠ¡å¼‚å¸¸: {str(result)[:50]}...")
                continue

            if result:
                if task_type == 'guide':
                    guides_data.append(result)
                    title = result.get('title', '') if isinstance(result, dict) else 'æœªçŸ¥æ ‡é¢˜'
                    print(f"    âœ… [{completed_count}/{total_tasks}] æŒ‡å—: {title}")
                else:
                    troubleshooting_data.append(result)
                    title = result.get('title', '') if isinstance(result, dict) else 'æœªçŸ¥æ ‡é¢˜'
                    print(f"    âœ… [{completed_count}/{total_tasks}] æ•…éšœæ’é™¤: {title}")
            else:
                print(f"    âš ï¸  [{completed_count}/{total_tasks}] {task_type} å¤„ç†å¤±è´¥")

        return guides_data, troubleshooting_data

    async def _process_guide_task_async(self, url):
        """å¼‚æ­¥å¤„ç†æŒ‡å—ä»»åŠ¡"""
        try:
            soup = await self.get_soup_async(url)
            if soup:
                return self.extract_guide_content(url)
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥å¤„ç†æŒ‡å—å¤±è´¥ {url}: {e}")
        return None

    async def _process_troubleshooting_task_async(self, url):
        """å¼‚æ­¥å¤„ç†æ•…éšœæ’é™¤ä»»åŠ¡"""
        try:
            soup = await self.get_soup_async(url)
            if soup:
                return self.extract_troubleshooting_content(url)
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥å¤„ç†æ•…éšœæ’é™¤å¤±è´¥ {url}: {e}")
        return None

    def get_soup(self, url, use_playwright=False):
        """é‡å†™get_soupæ–¹æ³•ï¼Œæ”¯æŒä»£ç†æ± ã€é‡è¯•æœºåˆ¶å’ŒPlaywrightæ¸²æŸ“"""
        if not url:
            return None

        if url in self.failed_urls:
            self.logger.warning(f"è·³è¿‡å·²çŸ¥å¤±è´¥URL: {url}")
            return None

        # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        # ç¡®ä¿ä½¿ç”¨www.ifixit.comè€Œä¸æ˜¯å…¶ä»–å­åŸŸå
        if 'ifixit.com' in url and not url.startswith('https://www.ifixit.com'):
            url = url.replace('https://', 'https://www.').replace('http://', 'https://www.')
            if not url.startswith('https://www.ifixit.com'):
                url = url.replace('ifixit.com', 'www.ifixit.com')

        # æ·»åŠ å¼ºåˆ¶è‹±æ–‡å‚æ•°
        if '?' in url:
            if 'lang=' not in url:
                url += '&lang=en'
            else:
                # æ›¿æ¢ç°æœ‰çš„langå‚æ•°
                import re
                url = re.sub(r'lang=[^&]*', 'lang=en', url)
        else:
            url += '?lang=en'

        self.stats["total_requests"] += 1

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return self._retry_with_backoff(self._get_soup_with_playwright, url)

        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„requestsæ–¹æ³•
        return self._retry_with_backoff(self._get_soup_requests, url)

    def _get_soup_requests(self, url):
        """ä½¿ç”¨requestsè·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒæ™ºèƒ½ä»£ç†åˆ‡æ¢"""
        session = requests.Session()
        retry_strategy = Retry(
            total=0,  # å®Œå…¨ç¦ç”¨urllib3çš„é‡è¯•ï¼Œç”±ä¸Šå±‚å¤„ç†
            backoff_factor=0,  # æ— é€€é¿å»¶è¿Ÿ
            status_forcelist=[],  # ä¸é‡è¯•ä»»ä½•çŠ¶æ€ç 
            raise_on_status=False  # ä¸åœ¨çŠ¶æ€ç é”™è¯¯æ—¶ç«‹å³æŠ›å‡ºå¼‚å¸¸
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è·å–ä»£ç†
        proxies = self._get_next_proxy() if self.use_proxy else None

        try:
            response = session.get(
                url,
                headers=self.headers,
                timeout=(5, 10),  # è¿æ¥è¶…æ—¶5ç§’ï¼Œè¯»å–è¶…æ—¶10ç§’
                proxies=proxies
            )
            response.raise_for_status()

            from bs4 import BeautifulSoup
            return BeautifulSoup(response.content, 'html.parser')

        except (requests.exceptions.ProxyError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ConnectionError,
                requests.exceptions.ReadTimeout,
                requests.exceptions.Timeout) as e:
            # ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œé™é»˜å¤„ç†
            if self.use_proxy:
                # é™é»˜åˆ‡æ¢ä»£ç†å¹¶é‡è¯•ä¸€æ¬¡
                if self._switch_proxy(f"ç½‘ç»œé”™è¯¯: {str(e)[:50]}"):
                    try:
                        # ä½¿ç”¨æ–°ä»£ç†å¿«é€Ÿé‡è¯•ä¸€æ¬¡
                        new_proxies = self._get_next_proxy()
                        response = session.get(
                            url,
                            headers=self.headers,
                            timeout=(3, 8),  # è¿æ¥è¶…æ—¶3ç§’ï¼Œè¯»å–è¶…æ—¶8ç§’
                            proxies=new_proxies
                        )
                        response.raise_for_status()

                        from bs4 import BeautifulSoup
                        return BeautifulSoup(response.content, 'html.parser')
                    except Exception:
                        # é™é»˜å¤±è´¥ï¼Œè¿”å›Noneè®©ä¸Šå±‚å¤„ç†
                        return None
            # é™é»˜å¤±è´¥ï¼Œä¸è¾“å‡ºé”™è¯¯ä¿¡æ¯
            return None

    def _get_soup_with_playwright(self, url):
        """ä½¿ç”¨Playwrightè·å–JavaScriptæ¸²æŸ“åçš„é¡µé¢å†…å®¹"""
        try:
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´ - å¼ºåˆ¶è‹±æ–‡ç‰ˆæœ¬
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=1.0',  # å¼ºåˆ¶è‹±æ–‡ï¼Œæé«˜ä¼˜å…ˆçº§
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Cookie': 'locale=en'  # æ·»åŠ è‹±æ–‡locale cookie
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                page.goto(url, wait_until='networkidle')

                # å¼ºåˆ¶è®¾ç½®è‹±æ–‡è¯­è¨€
                page.evaluate("""
                    // è®¾ç½®localStorageä¸­çš„è¯­è¨€åå¥½
                    localStorage.setItem('locale', 'en');
                    localStorage.setItem('language', 'en');
                    localStorage.setItem('lang', 'en');

                    // è®¾ç½®cookie
                    document.cookie = 'locale=en; path=/; domain=.ifixit.com';
                    document.cookie = 'language=en; path=/; domain=.ifixit.com';
                """)

                # ç­‰å¾…ä¸€ä¸‹è®©è®¾ç½®ç”Ÿæ•ˆ
                page.wait_for_timeout(1000)

                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = str(e) if e is not None else "Unknown error"
            print(f"Playwrightè·å–é¡µé¢å¤±è´¥ {url}: {error_msg}")
            self.failed_urls.add(url)
            return None

    def _create_local_directory(self, path):
        """åˆ›å»ºæœ¬åœ°ç›®å½•ç»“æ„"""
        full_path = Path(self.storage_root) / path
        full_path.mkdir(parents=True, exist_ok=True)
        return full_path

    def _get_file_size_from_url(self, url):
        """è·å–URLæ–‡ä»¶çš„å¤§å°ï¼ˆMBï¼‰"""
        try:
            # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
            if '.thumbnail.medium' in url:
                url = url.replace('.thumbnail.medium', '.medium')

            # ä½¿ç”¨ä¸ä¸‹è½½ç›¸åŒçš„è¯·æ±‚å¤´
            media_headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://www.ifixit.com/",
                "Origin": "https://www.ifixit.com",
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "same-site",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }

            proxies = self._get_next_proxy() if self.use_proxy else None
            response = requests.head(url, headers=media_headers, timeout=3, proxies=proxies)
            if response and response.status_code == 200:
                content_length = response.headers.get('content-length')
                if content_length:
                    size_bytes = int(content_length)
                    size_mb = size_bytes / (1024 * 1024)
                    return size_mb
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° {url}: {e}")
        return None

    def _is_video_file(self, url):
        """æ£€æŸ¥URLæ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
        if not url:
            return False
        url_path = url.split('?')[0].lower()
        return any(url_path.endswith(ext) for ext in self.video_extensions)

    async def _download_media_file_async(self, url, local_dir, filename=None):
        """å¼‚æ­¥ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œè§†é¢‘å¤„ç†ç­–ç•¥ï¼Œæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰"""
        if not url or not url.startswith('http'):
            return None

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        # ç¡®ä¿åª’ä½“æ–‡ä»¶å¤¹åœ¨å½“å‰èŠ‚ç‚¹çš„ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯åœ¨æ ¹ç›®å½•
        local_path = local_dir / self.media_folder / filename

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸ä½¿ç”¨è·¨é¡µé¢å»é‡
        is_troubleshooting = "troubleshooting" in str(local_dir)

        if not is_troubleshooting:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰
            existing_file_path = self._find_existing_media_file(url, filename)
            if existing_file_path:
                # æ–‡ä»¶å·²å­˜åœ¨äºå…¶ä»–ä½ç½®ï¼Œè¿”å›ç›¸å¯¹äºå½“å‰ç›®å½•çš„è·¯å¾„
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                self.stats["media_downloaded"] += 1
                return existing_file_path

        # å¦‚æœæ–‡ä»¶åœ¨å½“å‰ç›®å½•å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„
        if local_path.exists():
            if self.verbose:
                self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨äºå½“å‰ç›®å½•: {filename}")
            self.stats["media_downloaded"] += 1
            # å¯¹äºtroubleshootingç›®å½•ï¼Œè¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
            if is_troubleshooting:
                return str(local_path.relative_to(local_dir))
            else:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
                try:
                    return str(local_path.relative_to(Path(self.storage_root)))
                except ValueError:
                    # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                    return str(local_path.relative_to(local_dir))

        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
        if self._is_video_file(url):
            if not self.download_videos:
                self.logger.info(f"è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰: {url}")
                self.stats["videos_skipped"] += 1
                return url

        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
        file_size_mb = await self._get_file_size_from_url_async(url)
        if file_size_mb and file_size_mb > self.max_video_size_mb:
            self.logger.warning(f"è·³è¿‡å¤§è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB > {self.max_video_size_mb}MB): {url}")
            self.stats["videos_skipped"] += 1
            return url

        self.logger.info(f"å‡†å¤‡ä¸‹è½½è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB): {url}")

        try:
            result = await self._download_media_file_impl_async(url, local_dir, filename)
            if self._is_video_file(url) and result != url:
                self.stats["videos_downloaded"] += 1
            return result
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e)
            safe_url = safe_str(url) if url is not None else "Unknown URL"
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æœ€ç»ˆå¤±è´¥ {safe_url}: {error_msg}")
            self.stats["media_failed"] += 1
            safe_error_msg = safe_str(error_msg) if error_msg is not None else "Unknown error"
            self._log_failed_url(url, f"åª’ä½“ä¸‹è½½å¤±è´¥: {safe_error_msg}")
            # è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—
            self._log_failed_media(url, error_msg)
            return url

    async def _get_file_size_from_url_async(self, url):
        """å¼‚æ­¥è·å–URLæ–‡ä»¶çš„å¤§å°ï¼ˆMBï¼‰"""
        try:
            # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
            if '.thumbnail.medium' in url:
                url = url.replace('.thumbnail.medium', '.medium')

            # ç¡®ä¿å¼‚æ­¥HTTPç®¡ç†å™¨å·²åˆå§‹åŒ–
            init_success = await self._init_async_http_manager()
            if not init_success or not self.async_http_manager:
                self.logger.error("å¼‚æ­¥HTTPç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è·å–æ–‡ä»¶å¤§å°")
                return None

            # ä½¿ç”¨è‡ªå®šä¹‰å¤´ä¿¡æ¯
            headers = self.headers.copy()
            headers.update({
                "Accept": "*/*",
                "Accept-Encoding": "gzip, deflate, br"
            })
            
            response = await self.async_http_manager.get(url, headers=headers)
            if response and response.status_code == 200:
                content_length = response.headers.get('content-length')
                if content_length:
                    size_bytes = int(content_length)
                    size_mb = size_bytes / (1024 * 1024)
                    return size_mb
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†ï¼Œé¿å…æ ¼å¼åŒ–é”™è¯¯
            error_msg = safe_str(e)

            # ç‰¹æ®Šå¤„ç†å„ç§ç½‘ç»œé”™è¯¯ï¼Œé¿å…æ—¥å¿—æ±¡æŸ“
            if "Event loop is closed" in error_msg:
                return None
            elif "nodename nor servname provided" in error_msg:
                # DNSè§£æå¤±è´¥ï¼Œé™é»˜å¤„ç†
                return None
            elif "Connection" in error_msg or "timeout" in error_msg.lower():
                # è¿æ¥é”™è¯¯ï¼Œé™é»˜å¤„ç†
                return None
            else:
                # å…¶ä»–é”™è¯¯æ‰è®°å½•æ—¥å¿—
                self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° {url}: {error_msg}")
        return None

    async def _download_media_file_impl_async(self, url, local_dir, filename):
        """å¼‚æ­¥åª’ä½“æ–‡ä»¶ä¸‹è½½çš„å…·ä½“å®ç°"""
        # ç¡®ä¿åª’ä½“æ–‡ä»¶å¤¹åœ¨å½“å‰èŠ‚ç‚¹çš„ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯åœ¨æ ¹ç›®å½•
        local_path = local_dir / self.media_folder / filename

        # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')
            if self.verbose:
                self.logger.info(f"URLæ ¼å¼ä¿®å¤: .thumbnail.medium -> .medium")

        # ç¡®ä¿å¼‚æ­¥HTTPç®¡ç†å™¨å·²åˆå§‹åŒ–
        init_success = await self._init_async_http_manager()
        if not init_success or not self.async_http_manager:
            self.logger.error("å¼‚æ­¥HTTPç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ä¸‹è½½åª’ä½“æ–‡ä»¶")
            return None

        try:
            # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
            if self.async_http_manager:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                local_path.parent.mkdir(parents=True, exist_ok=True)
                
                # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
                # è®¾ç½®åª’ä½“ä¸‹è½½çš„è¯·æ±‚å¤´
                media_headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Referer': 'https://www.ifixit.com/'
                }
                response = await self.async_http_manager.get_with_retry(url, headers=media_headers)
                if response is None:
                    return None

                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                async with aiofiles.open(local_path, 'wb') as f:
                    async for chunk in response.aiter_bytes(chunk_size=16384):
                        await f.write(chunk)
                        
                self.stats["media_downloaded"] += 1
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename}")
                else:
                    # ç®€åŒ–çš„è¿›åº¦æç¤º
                    if self.stats["media_downloaded"] % 10 == 0:
                        print(f"      ğŸ“¥ å·²ä¸‹è½½ {self.stats['media_downloaded']} ä¸ªåª’ä½“æ–‡ä»¶...")
            else:
                safe_url = safe_str(url) if url is not None else "Unknown URL"
                self.logger.error(f"å¼‚æ­¥HTTPå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¸‹è½½åª’ä½“æ–‡ä»¶: {safe_url}")
                return None
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e)
            safe_url = safe_str(url) if url is not None else "Unknown URL"
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {safe_url}, é”™è¯¯: {error_msg}")
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™è¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
        is_troubleshooting = "troubleshooting" in str(local_dir)
        if is_troubleshooting:
            return str(local_path.relative_to(local_dir))
        else:
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
            try:
                return str(local_path.relative_to(Path(self.storage_root)))
            except ValueError:
                # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                return str(local_path.relative_to(local_dir))

    async def _download_media_file(self, url, local_dir, filename=None):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¼‚æ­¥ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œè§†é¢‘å¤„ç†ç­–ç•¥ï¼Œæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰"""
        if not url or not url.startswith('http'):
            return None

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        # ç¡®ä¿åª’ä½“æ–‡ä»¶å¤¹åœ¨å½“å‰èŠ‚ç‚¹çš„ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯åœ¨æ ¹ç›®å½•
        local_path = local_dir / self.media_folder / filename

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸ä½¿ç”¨è·¨é¡µé¢å»é‡
        is_troubleshooting = "troubleshooting" in str(local_dir)

        # ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ï¼Œé¿å…é‡å¤çš„æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
        if self._check_file_exists_cached(local_path):
            if self.verbose:
                self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨äºå½“å‰ç›®å½•: {filename}")
            self.stats["media_downloaded"] += 1
            # å¯¹äºtroubleshootingç›®å½•ï¼Œè¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
            if is_troubleshooting:
                return str(local_path.relative_to(local_dir))
            else:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
                try:
                    return str(local_path.relative_to(Path(self.storage_root)))
                except ValueError:
                    # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                    return str(local_path.relative_to(local_dir))

        if not is_troubleshooting:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰
            existing_file_path = self._find_existing_media_file(url, filename)
            if existing_file_path:
                # æ–‡ä»¶å·²å­˜åœ¨äºå…¶ä»–ä½ç½®ï¼Œè¿”å›ç›¸å¯¹äºå½“å‰ç›®å½•çš„è·¯å¾„
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                self.stats["media_downloaded"] += 1
                return existing_file_path

            # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
            if self._is_video_file(url):
                if not self.download_videos:
                    self.logger.info(f"è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰: {url}")
                    self.stats["videos_skipped"] += 1
                    return url

            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size_mb = await self._get_file_size_from_url_async(url)
            if file_size_mb and file_size_mb > self.max_video_size_mb:
                self.logger.warning(f"è·³è¿‡å¤§è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB > {self.max_video_size_mb}MB): {url}")
                self.stats["videos_skipped"] += 1
                return url

            self.logger.info(f"å‡†å¤‡ä¸‹è½½è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB): {url}")

        try:
            # ä½¿ç”¨å¼‚æ­¥å®ç°
            result = await self._download_media_file_impl(url, local_dir, filename)
            if self._is_video_file(url) and result != url:
                self.stats["videos_downloaded"] += 1
            return result
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e)
            safe_url = safe_str(url) if url is not None else "Unknown URL"
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æœ€ç»ˆå¤±è´¥ {safe_url}: {error_msg}")
            self.stats["media_failed"] += 1
            safe_error_msg = safe_str(error_msg) if error_msg is not None else "Unknown error"
            self._log_failed_url(url, f"åª’ä½“ä¸‹è½½å¤±è´¥: {safe_error_msg}")
            # è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—
            self._log_failed_media(url, error_msg)
            return url

    def _find_existing_media_file(self, url, filename):
        """æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„åª’ä½“æ–‡ä»¶ï¼ˆè·¨é¡µé¢å»é‡ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        try:
            # åŸºäºURLçš„MD5å“ˆå¸Œæ¥æŸ¥æ‰¾æ–‡ä»¶
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]

            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"media_{url_hash}"
            if cache_key in self._file_exists_cache:
                return self._file_exists_cache[cache_key]

            # åœ¨æ•´ä¸ªstorage_rootç›®å½•ä¸‹æœç´¢å…·æœ‰ç›¸åŒå“ˆå¸Œçš„æ–‡ä»¶
            storage_path = Path(self.storage_root)
            if not storage_path.exists():
                self._file_exists_cache[cache_key] = None
                return None

            # æœç´¢æ‰€æœ‰mediaç›®å½•ä¸‹çš„æ–‡ä»¶
            for media_dir in storage_path.rglob("media"):
                if media_dir.is_dir():
                    # æŸ¥æ‰¾å…·æœ‰ç›¸åŒå“ˆå¸Œå‰ç¼€çš„æ–‡ä»¶
                    for existing_file in media_dir.glob(f"{url_hash}.*"):
                        if existing_file.is_file():
                            try:
                                # å°è¯•è¿”å›ç›¸å¯¹äºstorage_rootçš„è·¯å¾„
                                result = str(existing_file.relative_to(storage_path))
                                # ç¼“å­˜ç»“æœ
                                self._cache_file_exists_result(cache_key, result)
                                return result
                            except ValueError:
                                # å¦‚æœä¸èƒ½ç›¸å¯¹äºstorage_rootï¼Œåˆ™è¿”å›ç»å¯¹è·¯å¾„
                                result = str(existing_file)
                                self._cache_file_exists_result(cache_key, result)
                                return result

            # ç¼“å­˜æœªæ‰¾åˆ°çš„ç»“æœ
            self._cache_file_exists_result(cache_key, None)
            return None

        except Exception as e:
            if self.verbose:
                self.logger.warning(f"æŸ¥æ‰¾å·²å­˜åœ¨åª’ä½“æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def _cache_file_exists_result(self, cache_key, result):
        """ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ç»“æœ"""
        # å¦‚æœç¼“å­˜è¿‡å¤§ï¼Œæ¸…ç†ä¸€åŠ
        if len(self._file_exists_cache) >= self._cache_max_size:
            # æ¸…ç†ç¼“å­˜çš„å‰ä¸€åŠ
            keys_to_remove = list(self._file_exists_cache.keys())[:self._cache_max_size // 2]
            for key in keys_to_remove:
                del self._file_exists_cache[key]

        self._file_exists_cache[cache_key] = result

    def _check_file_exists_cached(self, file_path):
        """å¸¦ç¼“å­˜çš„æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥"""
        cache_key = f"exists_{str(file_path)}"

        if cache_key in self._file_exists_cache:
            return self._file_exists_cache[cache_key]

        exists = file_path.exists()
        self._cache_file_exists_result(cache_key, exists)
        return exists

    async def _download_media_file_impl(self, url, local_dir, filename):
        """åª’ä½“æ–‡ä»¶ä¸‹è½½çš„å…·ä½“å®ç°ï¼ˆå¼‚æ­¥ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç¡®ä¿åª’ä½“æ–‡ä»¶å¤¹åœ¨å½“å‰èŠ‚ç‚¹çš„ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯åœ¨æ ¹ç›®å½•
        local_path = local_dir / self.media_folder / filename
        local_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')
            if self.verbose:
                self.logger.info(f"URLæ ¼å¼ä¿®å¤: .thumbnail.medium -> .medium")

        # ç¡®ä¿å¼‚æ­¥HTTPç®¡ç†å™¨å·²åˆå§‹åŒ–
        if not self.async_http_manager:
            await self._init_async_http_manager()

        try:
            # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
            if self.async_http_manager:
                # è®¾ç½®åª’ä½“ä¸‹è½½çš„è¯·æ±‚å¤´
                media_headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Referer': 'https://www.ifixit.com/'
                }
                
                # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¸‹è½½æ–‡ä»¶
                response = await self.async_http_manager.get_with_retry(url, headers=media_headers)
                if response is None:
                    return None
                
                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                async with aiofiles.open(local_path, 'wb') as f:
                    async for chunk in response.aiter_bytes(chunk_size=16384):
                        await f.write(chunk)

                # è®¡ç®—æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„
                is_troubleshooting = "troubleshooting" in str(local_dir)
                if is_troubleshooting:
                    return str(local_path.relative_to(local_dir))
                else:
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
                    try:
                        return str(local_path.relative_to(Path(self.storage_root)))
                    except ValueError:
                        # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                        return str(local_path.relative_to(local_dir))
            else:
                safe_url = safe_str(url) if url is not None else "Unknown URL"
                self.logger.error(f"å¼‚æ­¥HTTPå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¸‹è½½åª’ä½“æ–‡ä»¶: {safe_url}")
                return None
        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e)
            safe_url = safe_str(url) if url is not None else "Unknown URL"
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {safe_url}, é”™è¯¯: {error_msg}")
            return None

    async def _process_media_urls_async(self, data, local_dir):
        """å¼‚æ­¥é€’å½’å¤„ç†æ•°æ®ä¸­çš„åª’ä½“URLï¼Œä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
        media_tasks = []

        def collect_media_urls(obj, path=""):
            """æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„åª’ä½“URL"""
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                        media_tasks.append((current_path, value, obj, key))
                    elif key == 'images' and isinstance(value, list):
                        for i, img_item in enumerate(value):
                            if isinstance(img_item, str) and img_item.startswith('http'):
                                media_tasks.append((f"{current_path}[{i}]", img_item, value, i))
                            elif isinstance(img_item, dict) and 'url' in img_item:
                                img_url = img_item['url']
                                if isinstance(img_url, str) and img_url.startswith('http'):
                                    media_tasks.append((f"{current_path}[{i}].url", img_url, img_item, 'url'))
                    elif key == 'videos' and isinstance(value, list):
                        for i, video_item in enumerate(value):
                            if isinstance(video_item, str) and video_item.startswith('http'):
                                if self.download_videos:
                                    media_tasks.append((f"{current_path}[{i}]", video_item, value, i))
                            elif isinstance(video_item, dict) and 'url' in video_item:
                                video_url = video_item['url']
                                if isinstance(video_url, str) and video_url.startswith('http'):
                                    if self.download_videos:
                                        media_tasks.append((f"{current_path}[{i}].url", video_url, video_item, 'url'))
                    elif isinstance(value, (dict, list)):
                        collect_media_urls(value, current_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    collect_media_urls(item, f"{path}[{i}]" if path else f"[{i}]")

        # æ”¶é›†æ‰€æœ‰åª’ä½“URL
        collect_media_urls(data)

        if not media_tasks:
            return

        # åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡ - ä¼˜åŒ–å¹¶å‘æ•°ä»¥æå‡æ€§èƒ½
        max_concurrent_downloads = min(20, max(self.max_workers * 2, 10))  # å¢åŠ åª’ä½“ä¸‹è½½å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent_downloads)

        async def download_single_media(path, url, container, key):
            async with semaphore:
                try:
                    # æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
                    try:
                        loop = asyncio.get_running_loop()
                        if loop.is_closed():
                            return False
                    except RuntimeError:
                        return False

                    local_path = await self._download_media_file(url, local_dir)
                    if local_path and local_path != url:
                        container[key] = local_path
                    return True
                except Exception as e:
                    # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
                    error_msg = safe_str(e)
                    # ç‰¹æ®Šå¤„ç†äº‹ä»¶å¾ªç¯å…³é—­çš„æƒ…å†µ
                    if "Event loop is closed" in error_msg:
                        return False
                    safe_url = safe_str(url) if url is not None else "Unknown URL"
                    self.logger.error(f"å¼‚æ­¥åª’ä½“ä¸‹è½½å¤±è´¥ {safe_url}: {error_msg}")
                    self.stats["media_failed"] += 1
                    return False

        # å¹¶å‘ä¸‹è½½æ‰€æœ‰åª’ä½“æ–‡ä»¶
        download_tasks = [download_single_media(path, url, container, key)
                         for path, url, container, key in media_tasks]

        if download_tasks:
            print(f"      ğŸ“¥ å¼€å§‹é«˜å¹¶å‘å¼‚æ­¥ä¸‹è½½ {len(download_tasks)} ä¸ªåª’ä½“æ–‡ä»¶ï¼ˆå¹¶å‘æ•°: {max_concurrent_downloads}ï¼‰...")
            start_time = time.time()
            results = await asyncio.gather(*download_tasks, return_exceptions=True)
            end_time = time.time()
            success_count = sum(1 for r in results if r is True)
            download_time = end_time - start_time
            print(f"      âœ… å¼‚æ­¥ä¸‹è½½å®Œæˆ: {success_count}/{len(download_tasks)} æˆåŠŸï¼Œè€—æ—¶ {download_time:.2f}ç§’")

    def _process_media_urls(self, data, local_dir):
        """é€’å½’å¤„ç†æ•°æ®ä¸­çš„åª’ä½“URLï¼Œä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„åª’ä½“URL
        media_urls = []

        def collect_urls(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                        media_urls.append((obj, key, value))
                    elif key == 'images' and isinstance(value, list):
                        for i, img_item in enumerate(value):
                            if isinstance(img_item, str) and img_item.startswith('http'):
                                media_urls.append((value, i, img_item))
                            elif isinstance(img_item, dict) and 'url' in img_item:
                                img_url = img_item['url']
                                if isinstance(img_url, str) and img_url.startswith('http'):
                                    media_urls.append((img_item, 'url', img_url))
                    elif key == 'videos' and isinstance(value, list):
                        for i, video_item in enumerate(value):
                            if isinstance(video_item, str) and video_item.startswith('http'):
                                if self.download_videos:
                                    media_urls.append((value, i, video_item))
                            elif isinstance(video_item, dict) and 'url' in video_item:
                                video_url = video_item['url']
                                if isinstance(video_url, str) and video_url.startswith('http'):
                                    if self.download_videos:
                                        media_urls.append((video_item, 'url', video_url))
                    elif isinstance(value, (dict, list)):
                        collect_urls(value)
            elif isinstance(obj, list):
                for item in obj:
                    collect_urls(item)

        collect_urls(data)

        # å¦‚æœæœ‰åª’ä½“URLéœ€è¦ä¸‹è½½ï¼Œä¼˜å…ˆä½¿ç”¨åŒæ­¥æ–¹å¼é¿å…äº‹ä»¶å¾ªç¯é—®é¢˜
        if media_urls:
            try:
                # æ£€æµ‹æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
                loop = asyncio.get_running_loop()
                if loop and not loop.is_closed():
                    # å¦‚æœå·²ç»åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œä½¿ç”¨åŒæ­¥å›é€€æ–¹æ¡ˆé¿å…åµŒå¥—äº‹ä»¶å¾ªç¯é—®é¢˜
                    if self.verbose:
                        self.logger.info("æ£€æµ‹åˆ°æ´»è·ƒå¼‚æ­¥ç¯å¢ƒï¼Œä½¿ç”¨åŒæ­¥å›é€€æ–¹æ¡ˆå¤„ç†åª’ä½“æ–‡ä»¶")
                    self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)
                else:
                    # äº‹ä»¶å¾ªç¯å·²å…³é—­ï¼Œä½¿ç”¨åŒæ­¥å¤„ç†
                    if self.verbose:
                        self.logger.info("äº‹ä»¶å¾ªç¯å·²å…³é—­ï¼Œä½¿ç”¨åŒæ­¥å¤„ç†åª’ä½“æ–‡ä»¶")
                    self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥å°è¯•åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                try:
                    if self.verbose:
                        self.logger.info("åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯è¿›è¡Œå¼‚æ­¥åª’ä½“å¤„ç†")
                    # ä½¿ç”¨æ–°çš„äº‹ä»¶å¾ªç¯å¤„ç†
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self._process_collected_media_urls_async(media_urls, local_dir))
                    finally:
                        loop.close()
                        asyncio.set_event_loop(None)
                except Exception as e:
                    # å¦‚æœå¼‚æ­¥å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†
                    self.logger.warning(f"å¼‚æ­¥åª’ä½“å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†: {safe_str(e)}")
                    self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ï¼Œç›´æ¥ä½¿ç”¨åŒæ­¥å¤„ç†
                self.logger.warning(f"äº‹ä»¶å¾ªç¯æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨åŒæ­¥å¤„ç†: {safe_str(e)}")
                self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)

    async def _process_collected_media_urls_async(self, media_urls, local_dir):
        """å¼‚æ­¥å¤„ç†æ”¶é›†åˆ°çš„åª’ä½“URL - ä¿®å¤äº‹ä»¶å¾ªç¯å…³é—­é—®é¢˜"""
        # æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
        try:
            loop = asyncio.get_running_loop()
            if loop.is_closed():
                self.logger.warning("äº‹ä»¶å¾ªç¯å·²å…³é—­ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†")
                return self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)
        except RuntimeError:
            self.logger.warning("æ— æ³•è·å–äº‹ä»¶å¾ªç¯ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†")
            return self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)

        # ç¡®ä¿å¼‚æ­¥HTTPç®¡ç†å™¨å·²åˆå§‹åŒ–
        init_success = await self._init_async_http_manager()
        if not init_success:
            self.logger.warning("å¼‚æ­¥HTTPç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥å¤„ç†")
            return self._process_media_urls_sync_fallback_from_urls(media_urls, local_dir)

        async def download_and_update(container, key, url):
            try:
                # å†æ¬¡æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
                try:
                    loop = asyncio.get_running_loop()
                    if loop.is_closed():
                        return False
                except RuntimeError:
                    return False

                local_path = await self._download_media_file(url, local_dir)
                if local_path and local_path != url:
                    # è®¡ç®—æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„
                    if hasattr(local_path, '__fspath__'):  # æ£€æŸ¥æ˜¯å¦æ˜¯è·¯å¾„å¯¹è±¡
                        local_path = Path(local_path)
                        try:
                            # å°è¯•è®¡ç®—ç›¸å¯¹äºstorage_rootçš„è·¯å¾„
                            relative_path = local_path.relative_to(Path(self.storage_root))
                            container[key] = str(relative_path)
                        except ValueError:
                            # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè®¡ç®—ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                            try:
                                relative_path = local_path.relative_to(local_dir)
                                container[key] = str(relative_path)
                            except ValueError:
                                # æœ€åå›é€€åˆ°ç»å¯¹è·¯å¾„
                                container[key] = str(local_path)
                    else:
                        container[key] = local_path
                return True
            except Exception as e:
                # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
                error_msg = safe_str(e)
                # ç‰¹æ®Šå¤„ç†äº‹ä»¶å¾ªç¯å…³é—­çš„æƒ…å†µ
                if "Event loop is closed" in error_msg:
                    return False
                safe_url = safe_str(url) if url is not None else "Unknown URL"
                self.logger.error(f"åª’ä½“ä¸‹è½½å¤±è´¥ {safe_url}: {error_msg}")
                return False

        if media_urls:
            # é™åˆ¶å¹¶å‘æ•°ï¼Œå‡å°‘å¹¶å‘ä»¥é¿å…äº‹ä»¶å¾ªç¯è¿‡è½½
            semaphore = asyncio.Semaphore(min(8, len(media_urls)))  # é™ä½å¹¶å‘æ•°

            async def limited_download(container, key, url):
                async with semaphore:
                    try:
                        # æ£€æŸ¥äº‹ä»¶å¾ªç¯çŠ¶æ€
                        loop = asyncio.get_running_loop()
                        if loop.is_closed():
                            return False
                        return await download_and_update(container, key, url)
                    except Exception as e:
                        # å®‰å…¨çš„é”™è¯¯å¤„ç†
                        error_msg = safe_str(e)
                        if "Event loop is closed" in error_msg:
                            return False
                        return False

            # åˆ›å»ºæ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            download_tasks = [limited_download(container, key, url) for container, key, url in media_urls]

            try:
                # ä½¿ç”¨wait_forè®¾ç½®è¶…æ—¶ï¼Œé¿å…æ— é™ç­‰å¾…
                results = await asyncio.wait_for(
                    asyncio.gather(*download_tasks, return_exceptions=True),
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                success_count = sum(1 for r in results if r is True)
                print(f"      ğŸ“¥ æ‰¹é‡åª’ä½“ä¸‹è½½å®Œæˆ: {success_count}/{len(download_tasks)} æˆåŠŸ")
            except asyncio.TimeoutError:
                print(f"      â° åª’ä½“ä¸‹è½½è¶…æ—¶ï¼Œéƒ¨åˆ†æ–‡ä»¶å¯èƒ½æœªå®Œæˆ")
            except Exception as e:
                # å¤„ç†gatherè¿‡ç¨‹ä¸­çš„å¼‚å¸¸
                error_msg = safe_str(e)
                if "Event loop is closed" in error_msg:
                    print(f"      âš ï¸ äº‹ä»¶å¾ªç¯å·²å…³é—­ï¼Œéƒ¨åˆ†åª’ä½“ä¸‹è½½å¯èƒ½æœªå®Œæˆ")
                else:
                    print(f"      âŒ æ‰¹é‡ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error_msg}")

    def _process_media_urls_sync_fallback_from_urls(self, media_urls, local_dir):
        """ä»åª’ä½“URLåˆ—è¡¨è¿›è¡ŒåŒæ­¥å›é€€å¤„ç†"""
        success_count = 0
        total_count = len(media_urls)

        for container, key, url in media_urls:
            try:
                # ä½¿ç”¨åŒæ­¥æ–¹å¼ä¸‹è½½
                local_path = self._download_media_file_sync(url, local_dir)
                if local_path and local_path != url:
                    container[key] = local_path
                    success_count += 1
            except Exception as e:
                error_msg = safe_str(e)
                safe_url = safe_str(url) if url is not None else "Unknown URL"
                self.logger.error(f"åŒæ­¥åª’ä½“ä¸‹è½½å¤±è´¥ {safe_url}: {error_msg}")

        print(f"      ğŸ“¥ åŒæ­¥åª’ä½“ä¸‹è½½å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")

    def _download_media_file_sync(self, url, local_dir):
        """åŒæ­¥ä¸‹è½½åª’ä½“æ–‡ä»¶çš„å›é€€æ–¹æ³•"""
        try:
            import requests
            from urllib.parse import urlparse
            import os

            # ç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path)
            if not filename or '.' not in filename:
                filename = f"media_{hash(url) % 10000}.jpg"

            # ç¡®ä¿åª’ä½“æ–‡ä»¶å¤¹åœ¨å½“å‰èŠ‚ç‚¹çš„ç›®å½•ä¸‹
            local_path = local_dir / self.media_folder / filename
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¿®å¤URLæ ¼å¼
            if '.thumbnail.medium' in url:
                url = url.replace('.thumbnail.medium', '.medium')

            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://www.ifixit.com/'
            }

            # è·å–ä»£ç†
            proxies = None
            if self.proxy_manager:
                proxy_config = self.proxy_manager.get_proxy()
                if proxy_config:
                    proxies = {
                        'http': proxy_config['http'],
                        'https': proxy_config['https']
                    }

            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, headers=headers, proxies=proxies, timeout=(5, 15), stream=True)
            response.raise_for_status()

            # å†™å…¥æ–‡ä»¶
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            try:
                return str(local_path.relative_to(Path(self.storage_root)))
            except ValueError:
                try:
                    return str(local_path.relative_to(local_dir))
                except ValueError:
                    return str(local_path)

        except Exception as e:
            error_msg = safe_str(e)
            safe_url = safe_str(url) if url is not None else "Unknown URL"
            self.logger.error(f"åŒæ­¥åª’ä½“ä¸‹è½½å¤±è´¥ {safe_url}: {error_msg}")
            return None

    def _download_media_file_sync(self, url, local_dir, filename=None):
        """åŒæ­¥ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆç”¨äºå›é€€æ–¹æ¡ˆï¼‰"""
        if not url or not url.startswith('http'):
            return url

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        # ç¡®ä¿mediaæ–‡ä»¶å¤¹åœ¨å½“å‰èŠ‚ç‚¹çš„ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯åœ¨æ ¹ç›®å½•
        local_path = local_dir / self.media_folder / filename

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›æœ¬åœ°è·¯å¾„
        if local_path.exists():
            return str(local_path.relative_to(local_dir))

        # ä¿®å¤URLæ ¼å¼
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')

        try:
            # åˆ›å»ºç›®å½•
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨requestsåŒæ­¥ä¸‹è½½
            media_headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://www.ifixit.com/",
                "Origin": "https://www.ifixit.com",
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "same-site",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }
            proxies = self._get_next_proxy() if self.use_proxy else None
            response = requests.get(url, headers=media_headers, timeout=5, proxies=proxies, verify=False)
            response.raise_for_status()

            # ä¿å­˜æ–‡ä»¶
            with open(local_path, 'wb') as f:
                f.write(response.content)

            self.stats["media_downloaded"] += 1
            return str(local_path.relative_to(local_dir))

        except Exception as e:
            # å®‰å…¨çš„é”™è¯¯æ¶ˆæ¯å¤„ç†
            error_msg = safe_str(e)
            safe_url = safe_str(url) if url is not None else "Unknown URL"
            self.logger.error(f"åŒæ­¥åª’ä½“ä¸‹è½½å¤±è´¥ {safe_url}: {error_msg}")
            self.stats["media_failed"] += 1
            return url

    def _process_media_urls_sync_fallback(self, data, local_dir):
        """åŒæ­¥å¤„ç†åª’ä½“URLçš„å›é€€æ–¹æ¡ˆ"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                    # ä½¿ç”¨åŒæ­¥ä¸‹è½½ä½œä¸ºå›é€€
                    data[key] = self._download_media_file_sync(value, local_dir)
                elif key == 'images' and isinstance(value, list):
                    for i, img_item in enumerate(value):
                        if isinstance(img_item, str) and img_item.startswith('http'):
                            value[i] = self._download_media_file_sync(img_item, local_dir)
                        elif isinstance(img_item, dict) and 'url' in img_item:
                            img_url = img_item['url']
                            if isinstance(img_url, str) and img_url.startswith('http'):
                                img_item['url'] = self._download_media_file_sync(img_url, local_dir)
                elif key == 'videos' and isinstance(value, list):
                    for i, video_item in enumerate(value):
                        if isinstance(video_item, str) and video_item.startswith('http'):
                            if self.download_videos:
                                value[i] = self._download_media_file_sync(video_item, local_dir)
                        elif isinstance(video_item, dict) and 'url' in video_item:
                            video_url = video_item['url']
                            if isinstance(video_url, str) and video_url.startswith('http'):
                                if self.download_videos:
                                    video_item['url'] = self._download_media_file_sync(video_url, local_dir)
                elif isinstance(value, (dict, list)):
                    self._process_media_urls_sync_fallback(value, local_dir)
        elif isinstance(data, list):
            for item in data:
                self._process_media_urls_sync_fallback(item, local_dir)

    def _tree_crawler_get_soup(self, url):
        """ä¸ºtree_crawleræä¾›çš„get_soupæ–¹æ³•ï¼Œåœ¨éœ€è¦é¢åŒ…å±‘å¯¼èˆªæ—¶ä½¿ç”¨Playwright"""
        # å¯¹äºéœ€è¦æå–é¢åŒ…å±‘å¯¼èˆªçš„é¡µé¢ï¼Œä½¿ç”¨Playwright
        if '/Teardown/' in url or '/Guide/' in url or '/Device/' in url:
            return self.get_soup(url, use_playwright=True)
        else:
            return self.get_soup(url, use_playwright=False)

    def extract_real_name_from_url(self, url):
        """
        ä»URLä¸­æå–çœŸå®çš„nameå­—æ®µï¼ˆURLè·¯å¾„çš„æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰
        """
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_url = url.split('?')[0].split('#')[0]

        # ç§»é™¤æœ«å°¾çš„æ–œæ 
        clean_url = clean_url.rstrip('/')

        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µ
        path_parts = clean_url.split('/')
        if path_parts:
            name = path_parts[-1]
            # URLè§£ç 
            import urllib.parse
            name = urllib.parse.unquote(name)
            # ä¿®å¤å¼•å·ç¼–ç é—®é¢˜ï¼šå°† \" è½¬æ¢ä¸º "
            name = name.replace('\\"', '"')
            return name

        return ""

    def _force_english_content(self, text):
        """
        å¼ºåˆ¶å°†ä¸­æ–‡å†…å®¹è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
        """
        if not text:
            return text

        # ä¸­æ–‡åˆ°è‹±æ–‡çš„æ˜ å°„è¡¨
        chinese_to_english = {
            'è‹±å¯¸': '"',
            'å¯¸': '"',
            'ä¿®å¤': 'Repair',
            'ç»´ä¿®': 'Repair',
            'æŒ‡å—': 'Guide',
            'æ•…éšœæ’é™¤': 'Troubleshooting',
            'é—®é¢˜': 'Issues',
            'è§£å†³æ–¹æ¡ˆ': 'Solutions',
            'æ•™ç¨‹': 'Tutorial',
            'æ‹†è§£': 'Teardown',
            'å®‰è£…': 'Installation',
            'æ›´æ¢': 'Replacement',
            'å‹å·': 'Models',
            'ç³»åˆ—': 'Series'
        }

        # æ›¿æ¢ä¸­æ–‡å†…å®¹
        result = text
        for chinese, english in chinese_to_english.items():
            result = result.replace(chinese, english)

        return result

    def extract_real_title_from_page(self, soup):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µï¼Œå¹¶å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡å†…å®¹
        """
        if not soup:
            return ""

        # ä¼˜å…ˆä»h1æ ‡ç­¾æå–
        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                # å¼ºåˆ¶è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
                title = self._force_english_content(title)
                return title

        # å…¶æ¬¡ä»titleæ ‡ç­¾æå–
        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                # å¼ºåˆ¶è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
                title = self._force_english_content(title)
                return title

        return ""



    # ç¬¬ä¸€ä¸ªextract_real_view_statisticsæ–¹æ³•å·²ç§»é™¤ï¼Œä½¿ç”¨ä¸‹é¢çš„å®ç°

    def _is_specified_target_url(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        """
        if not self.target_url or not url:
            return False

        # æ ‡å‡†åŒ–URLè¿›è¡Œæ¯”è¾ƒ
        def normalize_url(u):
            import urllib.parse
            # URLè§£ç ï¼Œç„¶åæ ‡å‡†åŒ–
            decoded = urllib.parse.unquote(u)
            return decoded.rstrip('/').lower()

        return normalize_url(url) == normalize_url(self.target_url)

    def discover_subcategories_from_page(self, soup, base_url):
        """
        åŠ¨æ€å‘ç°é¡µé¢ä¸­çš„å­ç±»åˆ«é“¾æ¥
        """
        if not soup:
            return []

        subcategories = []

        # æŸ¥æ‰¾è®¾å¤‡ç±»åˆ«é“¾æ¥çš„å¤šç§é€‰æ‹©å™¨
        category_selectors = [
            'a[href*="/Device/"]',  # åŒ…å«/Device/çš„é“¾æ¥
            '.category-link',       # ç±»åˆ«é“¾æ¥ç±»
            '.device-link',         # è®¾å¤‡é“¾æ¥ç±»
            '.subcategory',         # å­ç±»åˆ«ç±»
            'a[href^="/Device/"]'   # ä»¥/Device/å¼€å¤´çš„é“¾æ¥
        ]

        found_links = set()

        for selector in category_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/Device/' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
                    skip_patterns = [
                        '/Guide/', '/Troubleshooting/', '/Edit/', '/History/',
                        '/Answers/', '?revision', 'action=edit', 'action=create'
                    ]

                    if not any(pattern in full_url for pattern in skip_patterns):
                        # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå­ç±»åˆ«ï¼ˆURLæ¯”å½“å‰é¡µé¢æ›´æ·±ä¸€å±‚ï¼‰
                        if full_url != base_url and full_url not in found_links:
                            # æå–é“¾æ¥æ–‡æœ¬ä½œä¸ºåç§°
                            link_text = link.get_text().strip()
                            if link_text:
                                subcategories.append({
                                    'name': self.extract_real_name_from_url(full_url),
                                    'url': full_url,
                                    'title': link_text
                                })
                                found_links.add(full_url)

        return subcategories

    def extract_real_view_statistics(self, soup, url):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®
        """
        if not soup:
            return {}

        stats = {}

        # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®çš„å„ç§å¯èƒ½ä½ç½®
        stat_selectors = [
            '.view-count',
            '.stats-item',
            '.view-stats',
            '[data-stats]',
            '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # è§£æç»Ÿè®¡æ•°æ®æ ¼å¼
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»APIæˆ–å…¶ä»–ä½ç½®è·å–
        if not stats:
            # å°è¯•ä»é¡µé¢çš„JavaScriptæ•°æ®ä¸­æå–
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        # æŸ¥æ‰¾å¯èƒ½çš„ç»Ÿè®¡æ•°æ®
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def build_correct_url_from_path(self, base_url, path_segments):
        """
        æ ¹æ®è·¯å¾„æ®µæ­£ç¡®æ„å»ºURLï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
        """
        if not path_segments:
            return base_url

        # ä»base_urlå¼€å§‹ï¼Œé€æ­¥æ›¿æ¢è·¯å¾„æ®µ
        current_url = base_url

        for segment in path_segments:
            # è·å–å½“å‰URLçš„è·¯å¾„éƒ¨åˆ†
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(current_url)
            path_parts = [p for p in str(parsed.path).split('/') if p]

            # æ·»åŠ æ–°çš„è·¯å¾„æ®µ
            path_parts.append(str(segment))

            # é‡æ–°æ„å»ºURL
            new_path = '/' + '/'.join(path_parts)
            new_parsed = parsed._replace(path=new_path)
            current_url = urlunparse(new_parsed)

        return current_url

    def restructure_target_content(self, node, is_target_page=False):
        """
        é‡æ„ç›®æ ‡æ–‡ä»¶çš„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨guides[]å’Œtroubleshooting[]è€Œéchildren[]
        """
        if not node or not isinstance(node, dict):
            return node

        # å¦‚æœè¿™æ˜¯ç›®æ ‡é¡µé¢ä¸”æœ‰childrenåŒ…å«guideså’Œtroubleshooting
        if is_target_page and 'children' in node and node['children']:
            guides = []
            troubleshooting = []
            real_children = []

            for child in node['children']:
                if 'steps' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæŒ‡å—
                    guide_data = {k: v for k, v in child.items() if k not in ['children']}
                    guides.append(guide_data)
                elif 'causes' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæ•…éšœæ’é™¤
                    ts_data = {k: v for k, v in child.items() if k not in ['children']}
                    troubleshooting.append(ts_data)
                else:
                    # è¿™æ˜¯çœŸæ­£çš„å­ç±»åˆ«
                    real_children.append(child)

            # é‡æ„èŠ‚ç‚¹
            if guides:
                node['guides'] = guides
            if troubleshooting:
                node['troubleshooting'] = troubleshooting

            # åªæœ‰çœŸæ­£çš„å­ç±»åˆ«æ‰ä¿ç•™åœ¨childrenä¸­
            if real_children:
                node['children'] = real_children
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„å­ç±»åˆ«ï¼Œç§»é™¤childrenå­—æ®µ
                if 'children' in node:
                    del node['children']

        return node

    def fix_node_data(self, node, soup=None):
        """ä¿®å¤èŠ‚ç‚¹çš„nameå­—æ®µï¼Œåªåœ¨æœ€ç»ˆäº§å“é¡µé¢æ·»åŠ å…¶ä»–å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_root_device = url == f"{self.base_url}/Device"

        # æ‰€æœ‰èŠ‚ç‚¹éƒ½ä¿®å¤nameå­—æ®µ
        real_name = self.extract_real_name_from_url(url)
        if real_name:
            node['name'] = real_name

        # åªæœ‰åœ¨deep_crawl_product_contentä¸­è¢«è¯†åˆ«ä¸ºç›®æ ‡é¡µé¢æ—¶æ‰æ·»åŠ å…¶ä»–å­—æ®µ
        # è¿™é‡Œä¸æ·»åŠ titleã€instruction_urlã€view_statisticsç­‰å­—æ®µ
        # è¿™äº›å­—æ®µå°†åœ¨deep_crawl_product_contentæ–¹æ³•ä¸­æ·»åŠ 

        return node

    def extract_what_you_need_enhanced(self, guide_url):
        """
        å¢å¼ºç‰ˆ"What You Need"éƒ¨åˆ†æå–æ–¹æ³•ï¼Œç¡®ä¿çœŸå®é¡µé¢è®¿é—®å’Œæ•°æ®å®Œæ•´æ€§
        """
        if not guide_url:
            return {}

        print(f"    å¢å¼ºæå–What You Need: {guide_url}")

        try:
            # ä½¿ç”¨Playwrightè·å–å®Œæ•´æ¸²æŸ“çš„é¡µé¢
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´ç¡®ä¿è‹±æ–‡å†…å®¹ - å¼ºåˆ¶è‹±æ–‡ç‰ˆæœ¬
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=1.0',  # å¼ºåˆ¶è‹±æ–‡ï¼Œæé«˜ä¼˜å…ˆçº§
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Cookie': 'locale=en'  # æ·»åŠ è‹±æ–‡locale cookie
                })

                try:
                    # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…å®Œå…¨åŠ è½½
                    page.goto(guide_url, wait_until='networkidle')

                    # å¼ºåˆ¶è®¾ç½®è‹±æ–‡è¯­è¨€
                    page.evaluate("""
                        // è®¾ç½®localStorageä¸­çš„è¯­è¨€åå¥½
                        localStorage.setItem('locale', 'en');
                        localStorage.setItem('language', 'en');
                        localStorage.setItem('lang', 'en');

                        // è®¾ç½®cookie
                        document.cookie = 'locale=en; path=/; domain=.ifixit.com';
                        document.cookie = 'language=en; path=/; domain=.ifixit.com';
                    """)

                    page.wait_for_timeout(5000)  # å¢åŠ ç­‰å¾…æ—¶é—´ç¡®ä¿å®Œå…¨åŠ è½½

                    # è·å–é¡µé¢HTML
                    html_content = page.content()
                    
                    # è§£æHTML
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html_content, 'html.parser')

                    # å°è¯•å¤šç§æ–¹æ³•æå–"What You Need"æ•°æ®
                    what_you_need = {}

                    # æ–¹æ³•1ï¼šä»Reactç»„ä»¶çš„data-propsä¸­æå–ï¼ˆæœ€å‡†ç¡®ï¼‰
                    what_you_need = self._extract_from_react_props_enhanced(soup)

                    # æ–¹æ³•2ï¼šå¦‚æœReactæ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢çš„What You NeedåŒºåŸŸæå–
                    if not what_you_need:
                        what_you_need = self._extract_from_what_you_need_section(soup)

                    # æ–¹æ³•3ï¼šå¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢çš„äº§å“é“¾æ¥æå–
                    if not what_you_need:
                        what_you_need = self._extract_from_product_links(soup)

                    # æ–¹æ³•4ï¼šæœ€åå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
                    if not what_you_need:
                        what_you_need = self._extract_from_page_text(soup)

                    if what_you_need:
                        print(f"    æˆåŠŸæå–åˆ°: {list(what_you_need.keys())}")
                        # éªŒè¯æ•°æ®å®Œæ•´æ€§
                        total_items = sum(len(items) if isinstance(items, list) else 1
                                        for items in what_you_need.values())
                        print(f"    æ€»è®¡é¡¹ç›®æ•°: {total_items}")
                    else:
                        print(f"    æœªæ‰¾åˆ°What You Needæ•°æ®")

                    return what_you_need
                finally:
                    # ç¡®ä¿æµè§ˆå™¨å…³é—­ï¼Œå³ä½¿å‘ç”Ÿå¼‚å¸¸
                    browser.close()

        except Exception as e:
            print(f"    å¢å¼ºæå–å¤±è´¥: {str(e)}")
            return {}

    def _extract_from_react_props_enhanced(self, soup):
        """ä»Reactç»„ä»¶çš„data-propsä¸­æå–What you needæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾å¤šç§å¯èƒ½çš„Reactç»„ä»¶
            component_selectors = [
                'div[data-name="GuideTopComponent"]',
                'div[data-name="GuideComponent"]',
                'div[data-name="WhatYouNeedComponent"]',
                'div[data-props*="tools"]',
                'div[data-props*="parts"]',
                'div[data-props*="kits"]',
                'div[data-props*="materials"]'
            ]

            for selector in component_selectors:
                component = soup.select_one(selector)
                if component:
                    data_props = component.get('data-props')
                    if data_props:
                        try:
                            import json
                            import html

                            # HTMLè§£ç å¹¶è§£æJSON
                            decoded_props = html.unescape(data_props)
                            props_data = json.loads(decoded_props)

                            # æå–productData
                            product_data = props_data.get('productData', {})

                            # æå–Fix Kits/Parts
                            kits = product_data.get('kits', [])
                            if kits:
                                fix_kits = []
                                parts = []
                                materials = []
                                for kit in kits:
                                    name = kit.get('name', '').strip()
                                    if name:
                                        name = self.clean_product_name(name)
                                        if name:
                                            if 'kit' in name.lower() or 'fix kit' in name.lower():
                                                fix_kits.append(name)
                                            elif 'material' in name.lower():
                                                materials.append(name)
                                            else:
                                                parts.append(name)

                                if fix_kits:
                                    what_you_need['Fix Kits'] = fix_kits
                                if parts:
                                    what_you_need['Parts'] = parts
                                if materials:
                                    what_you_need['Materials'] = materials

                            # æå–Tools
                            tools = product_data.get('tools', [])
                            if tools:
                                tool_names = []
                                for tool in tools:
                                    name = tool.get('name', '').strip()
                                    if name:
                                        name = self.clean_product_name(name)
                                        if name:
                                            tool_names.append(name)

                                if tool_names:
                                    what_you_need['Tools'] = tool_names

                            # æå–å…¶ä»–å¯èƒ½çš„ç±»åˆ«
                            for category_key in ['supplies', 'components', 'accessories']:
                                category_items = product_data.get(category_key, [])
                                if category_items:
                                    category_names = []
                                    for item in category_items:
                                        name = item.get('name', '').strip()
                                        if name:
                                            name = self.clean_product_name(name)
                                            if name:
                                                category_names.append(name)

                                    if category_names:
                                        category_title = category_key.title()
                                        what_you_need[category_title] = category_names

                            if what_you_need:
                                break

                        except Exception as e:
                            continue

            return what_you_need

        except Exception as e:
            return what_you_need

    def _extract_from_what_you_need_section(self, soup):
        """ä»é¡µé¢çš„What You Needä¸“é—¨åŒºåŸŸæå–æ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾What You Needæ ‡é¢˜
            what_you_need_headers = soup.find_all(string=lambda text: text and
                'what you need' in text.lower())

            for header in what_you_need_headers:
                parent = header.parent
                if parent:
                    # æŸ¥æ‰¾çˆ¶çº§å®¹å™¨
                    container = parent.find_parent(['div', 'section'])
                    if container:
                        # æŸ¥æ‰¾å·¥å…·å’Œé›¶ä»¶åˆ—è¡¨
                        lists = container.find_all(['ul', 'ol'])
                        for list_elem in lists:
                            items = list_elem.find_all('li')
                            if items:
                                category_items = []
                                for item in items:
                                    text = item.get_text().strip()
                                    if text and len(text) > 2:
                                        cleaned_text = self.clean_product_name(text)
                                        if cleaned_text:
                                            category_items.append(cleaned_text)

                                if category_items:
                                    # æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ç±»åˆ«
                                    context = container.get_text().lower()
                                    if 'tool' in context:
                                        what_you_need['Tools'] = category_items
                                    elif 'part' in context:
                                        what_you_need['Parts'] = category_items
                                    elif 'kit' in context:
                                        what_you_need['Fix Kits'] = category_items
                                    else:
                                        what_you_need['Items'] = category_items
                                    break

                        if what_you_need:
                            break

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_product_links(self, soup):
        """ä»é¡µé¢çš„äº§å“é“¾æ¥ä¸­æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾äº§å“é“¾æ¥
            product_links = soup.find_all('a', href=lambda x: x and
                any(domain in x for domain in ['ifixit.com/products', 'ifixit.com/store']))

            tools = []
            parts = []
            fix_kits = []

            for link in product_links:
                text = link.get_text().strip()
                if text and len(text) > 2:
                    cleaned_text = self.clean_product_name(text)
                    if cleaned_text:
                        text_lower = cleaned_text.lower()
                        if any(tool_word in text_lower for tool_word in
                               ['screwdriver', 'spudger', 'pick', 'driver', 'tool']):
                            tools.append(cleaned_text)
                        elif any(kit_word in text_lower for kit_word in
                                ['kit', 'set']):
                            fix_kits.append(cleaned_text)
                        else:
                            parts.append(cleaned_text)

            if tools:
                what_you_need['Tools'] = list(dict.fromkeys(tools))  # å»é‡
            if parts:
                what_you_need['Parts'] = list(dict.fromkeys(parts))
            if fix_kits:
                what_you_need['Fix Kits'] = list(dict.fromkeys(fix_kits))

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_html_structure(self, soup):
        """ä»HTMLç»“æ„ä¸­ç›´æ¥æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾"What You Need"æ ‡é¢˜
            what_you_need_headers = soup.find_all(string=lambda text: text and
                any(phrase in text.lower() for phrase in ['what you need', 'tools', 'parts', 'materials']))

            for header in what_you_need_headers:
                parent = header.parent
                if parent:
                    # æŸ¥æ‰¾åç»­çš„åˆ—è¡¨æˆ–å†…å®¹
                    next_elements = parent.find_next_siblings(['ul', 'ol', 'div', 'section'])
                    for elem in next_elements:
                        # æå–åˆ—è¡¨é¡¹
                        items = elem.find_all(['li', 'a'])
                        if items:
                            category_items = []
                            for item in items:
                                text = item.get_text().strip()
                                if text and len(text) > 2:
                                    text = self.clean_product_name(text)
                                    if text:
                                        category_items.append(text)

                            if category_items:
                                # æ ¹æ®å†…å®¹åˆ¤æ–­ç±»åˆ«
                                header_text = header.lower()
                                if 'tool' in header_text:
                                    what_you_need['Tools'] = category_items
                                elif 'part' in header_text:
                                    what_you_need['Parts'] = category_items
                                elif 'kit' in header_text:
                                    what_you_need['Fix Kits'] = category_items
                                else:
                                    what_you_need['Items'] = category_items
                                break

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_page_text(self, soup):
        """ä»é¡µé¢æ–‡æœ¬ä¸­æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾åŒ…å«å·¥å…·å’Œé›¶ä»¶ä¿¡æ¯çš„æ–‡æœ¬
            text_content = soup.get_text()
            lines = text_content.split('\n')

            current_category = None
            items = []

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»åˆ«æ ‡é¢˜
                line_lower = line.lower()
                if any(keyword in line_lower for keyword in ['tools:', 'parts:', 'materials:', 'what you need:']):
                    if current_category and items:
                        what_you_need[current_category] = items

                    if 'tool' in line_lower:
                        current_category = 'Tools'
                    elif 'part' in line_lower:
                        current_category = 'Parts'
                    elif 'material' in line_lower:
                        current_category = 'Materials'
                    else:
                        current_category = 'Items'

                    items = []

                # æ£€æŸ¥æ˜¯å¦æ˜¯é¡¹ç›®
                elif current_category and len(line) > 2 and len(line) < 100:
                    cleaned_item = self.clean_product_name(line)
                    if cleaned_item:
                        items.append(cleaned_item)

            # æ·»åŠ æœ€åä¸€ä¸ªç±»åˆ«
            if current_category and items:
                what_you_need[current_category] = items

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_troubleshooting_images_from_section(self, section):
        """ä»troubleshooting sectionä¸­æå–å›¾ç‰‡ï¼Œä½¿ç”¨enhanced_crawlerçš„è¿‡æ»¤é€»è¾‘å¹¶ä¸‹è½½åˆ°æœ¬åœ°"""
        images = []
        seen_urls = set()

        try:
            if not section:
                if self.verbose:
                    print("âŒ Sectionä¸ºç©º")
                return images

            # æŸ¥æ‰¾è¯¥sectionå†…çš„æ‰€æœ‰å›¾ç‰‡
            img_elements = section.find_all('img')
            if self.verbose:
                print(f"ğŸ“· æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")

            for img in img_elements:
                try:
                    # æ£€æŸ¥æ˜¯å¦åœ¨å•†ä¸šæ¨å¹¿åŒºåŸŸå†…
                    if self._is_element_in_promotional_area(img):
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ¨å¹¿åŒºåŸŸå›¾ç‰‡")
                        continue

                    # é¢å¤–æ£€æŸ¥ï¼šæ£€æŸ¥å›¾ç‰‡çš„altæ–‡æœ¬æ˜¯å¦åŒ…å«æ¨èå†…å®¹ç‰¹å¾
                    alt_text = img.get('alt', '').strip().lower()
                    if self._is_recommendation_image_alt(alt_text):
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ¨èå†…å®¹å›¾ç‰‡: {alt_text}")
                        continue

                    img_src = img.get('src') or img.get('data-src')
                    if img_src and isinstance(img_src, str):
                        # æ„å»ºå®Œæ•´URL
                        if img_src.startswith("//"):
                            img_src = "https:" + img_src
                        elif img_src.startswith("/"):
                            img_src = self.base_url + img_src

                        # ä½¿ç”¨enhanced_crawlerçš„è¿‡æ»¤é€»è¾‘
                        if self._is_valid_troubleshooting_image_enhanced(img_src, img):
                            # ç¡®ä¿ä½¿ç”¨mediumå°ºå¯¸
                            if 'guide-images.cdn.ifixit.com' in img_src and not img_src.endswith('.medium'):
                                # ç§»é™¤ç°æœ‰çš„å°ºå¯¸åç¼€å¹¶æ·»åŠ .medium
                                if '.standard' in img_src or '.large' in img_src or '.small' in img_src:
                                    img_src = img_src.replace('.standard', '.medium').replace('.large', '.medium').replace('.small', '.medium')
                                elif not any(suffix in img_src for suffix in ['.medium', '.jpg', '.png', '.gif']):
                                    img_src += '.medium'

                            # å»é‡å¹¶æ·»åŠ 
                            if img_src not in seen_urls:
                                seen_urls.add(img_src)

                                # è·å–å›¾ç‰‡æè¿°ï¼ˆä½¿ç”¨åŸå§‹altæ–‡æœ¬ï¼Œä¸æ˜¯å°å†™ç‰ˆæœ¬ï¼‰
                                alt_text = img.get('alt', '').strip()
                                title_text = img.get('title', '').strip()
                                description = alt_text or title_text or "Block Image"

                                # æ£€æŸ¥æè¿°æ˜¯å¦æ˜¯å•†ä¸šå†…å®¹
                                if not self._is_commercial_text(description):
                                    # åœ¨troubleshootingæå–é˜¶æ®µï¼Œå…ˆä¿å­˜åŸå§‹URL
                                    # å›¾ç‰‡ä¸‹è½½å°†åœ¨ä¿å­˜é˜¶æ®µè¿›è¡Œï¼Œç¡®ä¿ä¸‹è½½åˆ°æ­£ç¡®çš„ç›®å½•
                                    images.append({
                                        "url": img_src,  # ä¿å­˜åŸå§‹URLï¼Œç¨ååœ¨ä¿å­˜æ—¶ä¸‹è½½
                                        "description": description
                                    })

                                    if self.verbose:
                                        print(f"âœ… æ·»åŠ å›¾ç‰‡: {description}")
                                else:
                                    if self.verbose:
                                        print(f"â­ï¸ è·³è¿‡å•†ä¸šæè¿°: {description}")
                            else:
                                if self.verbose:
                                    print(f"â­ï¸ è·³è¿‡é‡å¤URL: {img_src}")
                        else:
                            if self.verbose:
                                print(f"â­ï¸ è·³è¿‡æ— æ•ˆURL: {img_src}")
                    else:
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ— æ•ˆsrc")
                except Exception as e:
                    if self.verbose:
                        print(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue

            if self.verbose:
                print(f"ğŸ“· æœ€ç»ˆæå–åˆ° {len(images)} ä¸ªå›¾ç‰‡")
            return images

        except Exception as e:
            if self.verbose:
                print(f"âŒ æå–å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return images

    def _remove_promotional_content_from_section(self, section):
        """ç§»é™¤sectionä¸­çš„æ¨å¹¿å†…å®¹"""
        try:
            # ç§»é™¤æ¨å¹¿ç›¸å…³çš„å…ƒç´ 
            promotional_selectors = [
                '.promo', '.promotion', '.ad', '.advertisement',
                '.product-card', '.shop-card', '.buy-now',
                '[class*="promo"]', '[class*="shop"]', '[class*="buy"]'
            ]

            for selector in promotional_selectors:
                elements = section.select(selector)
                for elem in elements:
                    elem.decompose()

        except Exception:
            pass

    def _is_valid_troubleshooting_image_enhanced(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„troubleshootingå›¾ç‰‡ - ä½¿ç”¨enhanced_crawlerçš„é€»è¾‘"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # è¿‡æ»¤æ‰å°å›¾æ ‡ã€logoå’Œå•†ä¸šå›¾ç‰‡
            skip_keywords = [
                'icon', 'logo', 'flag', 'avatar', 'ad', 'banner',
                'promo', 'cart', 'buy', 'purchase', 'shop', 'header',
                'footer', 'nav', 'menu', 'sidebar'
            ]
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            # å¦‚æœæœ‰img_elemï¼Œæ£€æŸ¥altå±æ€§
            if img_elem:
                alt_text = img_elem.get('alt', '').lower()
                title_text = img_elem.get('title', '').lower()

                # æ’é™¤å•†ä¸šæè¿°
                commercial_keywords = [
                    'buy', 'purchase', 'cart', 'shop', 'price', 'review',
                    'advertisement', 'sponsor'
                ]
                if any(keyword in alt_text or keyword in title_text for keyword in commercial_keywords):
                    return False

                # è¿‡æ»¤è£…é¥°æ€§å›¾ç‰‡
                decorative_alt_keywords = [
                    'decoration', 'ornament', 'divider', 'spacer',
                    'bullet', 'arrow', 'pointer', 'separator'
                ]
                if any(keyword in alt_text for keyword in decorative_alt_keywords):
                    return False

            return True

        except Exception:
            return False

    def _is_valid_troubleshooting_image(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„troubleshootingå›¾ç‰‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # åªå…è®¸æ¥è‡ªguide-images.cdn.ifixit.comçš„å›¾ç‰‡
            if 'guide-images.cdn.ifixit.com' not in img_src_lower:
                return False

            # è¿‡æ»¤æ‰æ˜æ˜¾çš„è£…é¥°æ€§å›¾ç‰‡å’Œå›¾æ ‡
            skip_keywords = [
                'icon', 'logo', 'avatar', 'badge', 'star', 'rating',
                'promo', 'ad', 'banner', 'shop', 'buy', 'cart'
            ]
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            # å¦‚æœæœ‰img_elemï¼Œæ£€æŸ¥altå±æ€§
            if img_elem:
                alt_text = img_elem.get('alt', '').lower()
                if alt_text:
                    decorative_alt_keywords = [
                        'decoration', 'ornament', 'divider', 'spacer',
                        'bullet', 'arrow', 'pointer', 'separator'
                    ]
                    if any(keyword in alt_text for keyword in decorative_alt_keywords):
                        return False

            return True

        except Exception:
            return False

    def _is_element_in_promotional_area(self, element):
        """æ£€æŸ¥å…ƒç´ æ˜¯å¦åœ¨æ¨å¹¿åŒºåŸŸå†… - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´ç²¾ç¡®åœ°è¯†åˆ«æ¨å¹¿å®¹å™¨"""
        try:
            # å‘ä¸Šéå†DOMæ ‘æ£€æŸ¥æ˜¯å¦åœ¨æ¨å¹¿å®¹å™¨ä¸­
            current = element
            level = 0
            while current and current.name and level < 5:  # é™åˆ¶éå†å±‚æ•°
                # ä¼˜å…ˆé€šè¿‡classåç§°è¯†åˆ«æ¨å¹¿å®¹å™¨
                classes = current.get('class', [])
                if isinstance(classes, list):
                    class_str = ' '.join(classes).lower()
                else:
                    class_str = str(classes).lower()

                # æ˜ç¡®çš„æ¨å¹¿å®¹å™¨classåç§°
                promo_class_keywords = [
                    'guide-recommendation', 'guide-promo', 'view-guide',
                    'product-box', 'product-purchase', 'buy-box', 'purchase-box',
                    'product-card', 'shop-card', 'buy-now',
                    'parts-finder', 'find-parts'
                ]

                # å¦‚æœæœ‰æ˜ç¡®çš„æ¨å¹¿classï¼Œç›´æ¥è¿”å›True
                if any(keyword in class_str for keyword in promo_class_keywords):
                    return True

                # å¯¹äºæ²¡æœ‰æ˜ç¡®classçš„å…ƒç´ ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ–‡æœ¬æ£€æµ‹
                # åªæœ‰å½“å…ƒç´ ç›¸å¯¹è¾ƒå°ä¸”æ˜ç¡®åŒ…å«æ¨å¹¿å†…å®¹æ—¶æ‰è®¤ä¸ºæ˜¯æ¨å¹¿å®¹å™¨
                if (level <= 2 and  # åªæ£€æŸ¥å‰3å±‚
                    self._is_specific_promotional_container(current)):
                    return True

                current = current.parent
                level += 1
            return False
        except Exception:
            return False

    def _is_specific_promotional_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šçš„æ¨å¹¿å®¹å™¨ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æµ‹ï¼‰"""
        try:
            text = element.get_text().lower()

            # æ–‡æœ¬å¤ªé•¿çš„ä¸å¤ªå¯èƒ½æ˜¯æ¨å¹¿å®¹å™¨
            if len(text) > 300:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"View Guide"æŒ‰é’®
            has_view_guide = 'view guide' in text

            # æ£€æŸ¥æ—¶é—´æ ¼å¼æ¨¡å¼
            import re
            time_pattern = r'\d+\s*(minute|hour)s?(\s*-\s*\d+\s*(minute|hour)s?)?'
            has_time_pattern = bool(re.search(time_pattern, text))

            # æ£€æŸ¥éš¾åº¦ç­‰çº§
            difficulty_levels = ['very easy', 'easy', 'moderate', 'difficult', 'very difficult']
            has_difficulty = any(level in text for level in difficulty_levels)

            # æ£€æŸ¥æŒ‡å—ç›¸å…³ç‰¹å¾
            guide_features = ['how to', 'guide', 'tutorial', 'install', 'replace', 'repair', 'fix']
            has_guide_features = any(feature in text for feature in guide_features)

            # æ£€æŸ¥è´­ä¹°ç›¸å…³ç‰¹å¾
            purchase_features = ['buy', 'purchase', '$', 'â‚¬', 'Â£', 'Â¥', 'add to cart', 'shop']
            has_purchase = any(feature in text for feature in purchase_features)

            # æ¨å¹¿å®¹å™¨çš„åˆ¤æ–­æ¡ä»¶ï¼š
            # 1. åŒ…å«è´­ä¹°ç›¸å…³å†…å®¹ OR
            # 2. åŒ…å«"View Guide"æŒ‰é’® OR
            # 3. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’Œéš¾åº¦ç­‰çº§ OR
            # 4. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’ŒæŒ‡å—ç‰¹å¾
            return (
                has_purchase or
                has_view_guide or
                (has_time_pattern and has_difficulty) or
                (has_time_pattern and has_guide_features)
            )
        except Exception:
            return False

    def _is_recommendation_image_alt(self, alt_text):
        """æ£€æŸ¥å›¾ç‰‡çš„altæ–‡æœ¬æ˜¯å¦åŒ…å«æ¨èå†…å®¹ç‰¹å¾"""
        try:
            if not alt_text:
                return False

            alt_text_lower = alt_text.lower()

            # ç²¾ç¡®çš„æ¨èæŒ‡å—æ ‡é¢˜æ¨¡å¼ - æ›´å…¨é¢çš„æ¨¡å¼åŒ¹é…
            guide_title_patterns = [
                # ç‰¹å®šæ¨¡å¼
                r'how to boot.*into safe mode',
                r'how to use internet recovery',
                r'how to recover data from',
                r'how to install.*to.*ssd',
                r'how to replace.*battery',
                r'how to repair.*screen',
                r'how to fix.*display',
                r'how to troubleshoot',
                r'how to run.*with disk utility',
                r'how to start up.*in.*recovery mode',
                r'how to create a bootable',
                # é€šç”¨æ¨¡å¼
                r'how to .*removal',
                r'how to .*replace',
                r'how to .*install',
                r'how to .*repair',
                r'how to .*upgrade',
                r'how to .*fix',
                r'replacement.*guide',
                r'repair.*guide',
                r'installation.*guide',
                r'removal.*guide',
                # ç‰¹å®šè®¾å¤‡æ¨¡å¼
                r'macbook pro.*unibody.*removal',
                r'macbook pro.*key.*removal',
                r'macbook air.*display',
                r'macbook.*retina.*replacement'
            ]

            # æ£€æŸ¥ç²¾ç¡®çš„æ¨èæŒ‡å—æ ‡é¢˜æ¨¡å¼
            import re
            for pattern in guide_title_patterns:
                if re.search(pattern, alt_text_lower):
                    return True

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å—æ¨èçš„å…³é”®ç‰¹å¾
            recommendation_keywords = [
                'how to use', 'how to install', 'how to replace', 'how to repair',
                'guide', 'tutorial', 'installation', 'replacement', 'repair guide'
            ]

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨èç›¸å…³çš„è¯æ±‡
            has_recommendation_keywords = any(keyword in alt_text_lower for keyword in recommendation_keywords)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è®¾å¤‡/äº§å“åç§°ï¼ˆé€šå¸¸æ¨èå†…å®¹ä¼šåŒ…å«å…·ä½“çš„è®¾å¤‡åï¼‰
            device_keywords = ['macbook', 'iphone', 'ipad', 'ssd', 'battery', 'screen', 'display']
            has_device_keywords = any(keyword in alt_text_lower for keyword in device_keywords)

            # å¦‚æœåŒæ—¶åŒ…å«æ¨èå…³é”®è¯å’Œè®¾å¤‡å…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯æ¨èå†…å®¹
            return has_recommendation_keywords and has_device_keywords

        except Exception:
            return False

    def _is_guide_recommendation_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡å—æ¨èå®¹å™¨ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´ç²¾ç¡®åœ°è¯†åˆ«æ¨èæ¡†"""
        try:
            # åªæ£€æŸ¥å½“å‰å…ƒç´ çš„ç›´æ¥æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…æ‹¬æ·±å±‚å­å…ƒç´ 
            # è¿™æ ·å¯ä»¥é¿å…æ•´ä¸ªæ–‡æ¡£è¢«è¯¯åˆ¤ä¸ºæ¨å¹¿å®¹å™¨
            if element.name in ['html', '[document]', 'body']:
                return False

            # æ£€æŸ¥classå±æ€§ï¼Œä¼˜å…ˆé€šè¿‡classè¯†åˆ«
            classes = element.get('class', [])
            if isinstance(classes, list):
                class_str = ' '.join(classes).lower()
            else:
                class_str = str(classes).lower()

            # é€šè¿‡classåç§°å¿«é€Ÿè¯†åˆ«
            guide_class_keywords = [
                'guide-recommendation', 'guide-promo', 'view-guide',
                'recommendation', 'promo-guide', 'guide-card', 'related-guide'
            ]
            if any(keyword in class_str for keyword in guide_class_keywords):
                return True

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯æ¨èæ¡†ï¼ˆé™¤éæœ‰æ˜ç¡®çš„classæ ‡è¯†ï¼‰
            if len(text) > 500:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"View Guide"æŒ‰é’®æ–‡æœ¬
            has_view_guide = 'view guide' in text

            # æ£€æŸ¥æ—¶é—´æ ¼å¼æ¨¡å¼ (å¦‚ "30 minutes - 1 hour", "5 minutes", "2 hours")
            import re
            time_pattern = r'\d+\s*(minute|hour)s?(\s*-\s*\d+\s*(minute|hour)s?)?'
            has_time_pattern = bool(re.search(time_pattern, text))

            # æ£€æŸ¥éš¾åº¦ç­‰çº§
            difficulty_levels = ['very easy', 'easy', 'moderate', 'difficult', 'very difficult']
            has_difficulty = any(level in text for level in difficulty_levels)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å—ç›¸å…³çš„å…³é”®è¯
            guide_keywords = ['how to', 'guide', 'tutorial', 'install', 'replace', 'repair', 'fix']
            has_guide_keywords = any(keyword in text for keyword in guide_keywords)

            # æ¨èæ¡†çš„ç‰¹å¾ï¼š
            # 1. åŒ…å«"View Guide"æŒ‰é’® OR
            # 2. åŒæ—¶åŒ…å«æ—¶é—´ä¿¡æ¯å’Œéš¾åº¦ç­‰çº§ OR
            # 3. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’ŒæŒ‡å—å…³é”®è¯
            is_recommendation = (
                has_view_guide or
                (has_time_pattern and has_difficulty) or
                (has_time_pattern and has_guide_keywords)
            )

            # æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼šæ¨èæ¡†é€šå¸¸æ¯”è¾ƒç®€æ´
            return is_recommendation and len(text) < 400

        except Exception:
            return False

    def _is_product_purchase_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“è´­ä¹°å®¹å™¨ - ä»enhanced_crawlerç§»æ¤ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·³è¿‡æ–‡æ¡£çº§åˆ«çš„å…ƒç´ 
            if element.name in ['html', '[document]', 'body']:
                return False

            # æ£€æŸ¥classå±æ€§ï¼Œä¼˜å…ˆé€šè¿‡classè¯†åˆ«
            classes = element.get('class', [])
            if isinstance(classes, list):
                class_str = ' '.join(classes).lower()
            else:
                class_str = str(classes).lower()

            # é€šè¿‡classåç§°å¿«é€Ÿè¯†åˆ«
            product_class_keywords = [
                'product-box', 'product-purchase', 'buy-box', 'purchase-box',
                'product-card', 'shop-card', 'buy-now'
            ]
            if any(keyword in class_str for keyword in product_class_keywords):
                return True

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯è´­ä¹°æ¡†
            if len(text) > 400:
                return False

            import re
            has_price = bool(re.search(r'[\$â‚¬Â£Â¥]\s*\d+\.?\d*', text))
            has_buy = 'buy' in text
            has_reviews = bool(re.search(r'\d+\.?\d*\s*(reviews?|stars?)', text))
            # è´­ä¹°æ¡†é€šå¸¸åŒ…å«ä»·æ ¼ã€è´­ä¹°æŒ‰é’®æˆ–è¯„ä»·ä¿¡æ¯
            return (has_price or has_buy or has_reviews) and len(text) < 200
        except Exception:
            return False

    def _is_parts_finder_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯é…ä»¶æŸ¥æ‰¾å®¹å™¨ - ä»enhanced_crawlerç§»æ¤ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·³è¿‡æ–‡æ¡£çº§åˆ«çš„å…ƒç´ 
            if element.name in ['html', '[document]', 'body']:
                return False

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯é…ä»¶æŸ¥æ‰¾æ¡†
            if len(text) > 500:
                return False

            has_parts_text = any(keyword in text for keyword in [
                'find your parts', 'select my model', 'compatible replacement'
            ])
            return has_parts_text and len(text) < 300
        except Exception:
            return False

    def _is_commercial_text(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯å•†ä¸šå†…å®¹ - ä»enhanced_crawlerç§»æ¤"""
        try:
            text_lower = text.lower()
            # å•†ä¸šå†…å®¹å…³é”®è¯
            commercial_keywords = [
                'buy', 'purchase', 'view guide', 'find your parts', 'select my model',
                'add to cart', 'quality guarantee', 'compatible replacement'
            ]

            # æ£€æŸ¥ä»·æ ¼æ¨¡å¼
            import re
            if re.search(r'[\$â‚¬Â£Â¥]\s*\d+\.?\d*', text):
                return True

            # æ£€æŸ¥è¯„åˆ†æ¨¡å¼
            if re.search(r'\d+\.?\d*\s*(reviews?|stars?)', text_lower):
                return True

            # æ£€æŸ¥å•†ä¸šå…³é”®è¯
            keyword_count = sum(1 for keyword in commercial_keywords if keyword in text_lower)
            return keyword_count >= 1

        except Exception:
            return False

    def _is_image_in_commercial_area(self, img_elem):
        """æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åœ¨å•†ä¸šåŒºåŸŸå†…"""
        try:
            # å‘ä¸ŠæŸ¥æ‰¾çˆ¶å…ƒç´ ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å•†ä¸šç›¸å…³çš„å®¹å™¨ä¸­
            current = img_elem
            for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
                if not current or not hasattr(current, 'parent'):
                    break

                current = current.parent
                if not current:
                    break

                # æ£€æŸ¥classå±æ€§
                class_attr = current.get('class', [])
                if isinstance(class_attr, list):
                    class_str = ' '.join(class_attr).lower()
                else:
                    class_str = str(class_attr).lower()

                # å•†ä¸šåŒºåŸŸçš„classå…³é”®è¯
                commercial_keywords = [
                    'shop', 'store', 'buy', 'purchase', 'cart', 'checkout',
                    'product', 'price', 'sale', 'offer', 'deal', 'promo',
                    'advertisement', 'ad-', 'banner', 'sponsor'
                ]

                if any(keyword in class_str for keyword in commercial_keywords):
                    return True

                # æ£€æŸ¥idå±æ€§
                id_attr = current.get('id', '')
                if id_attr and any(keyword in id_attr.lower() for keyword in commercial_keywords):
                    return True

            return False
        except Exception:
            return False

    def _is_valid_guide_image(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„guideå›¾ç‰‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # åªå…è®¸æ¥è‡ªguide-images.cdn.ifixit.comçš„å›¾ç‰‡
            if 'guide-images.cdn.ifixit.com' not in img_src_lower:
                return False

            # è¿‡æ»¤æ‰æ˜æ˜¾çš„è£…é¥°æ€§å›¾ç‰‡
            skip_keywords = ['icon', 'logo', 'avatar', 'badge', 'star', 'rating']
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            return True

        except Exception:
            return False

    def extract_guide_content(self, guide_url):
        """é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨ç®€åŒ–çš„å›¾ç‰‡è¿‡æ»¤é€»è¾‘"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºæœ¬å†…å®¹
        guide_data = super().extract_guide_content(guide_url)

        if not guide_data:
            return None

        # ç®€åŒ–å›¾ç‰‡å¤„ç†ï¼šåªç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„URLæ ¼å¼ï¼Œä¸é‡æ–°æå–
        if 'steps' in guide_data and guide_data['steps']:
            for step in guide_data['steps']:
                if 'images' in step and step['images']:
                    filtered_images = []
                    seen_urls = set()

                    for img_src in step['images']:
                        if isinstance(img_src, str) and img_src:
                            # ç¡®ä¿ä½¿ç”¨å®Œæ•´URL
                            if img_src.startswith("//"):
                                img_src = "https:" + img_src
                            elif img_src.startswith("/"):
                                img_src = self.base_url + img_src

                            # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡
                            if self._is_valid_guide_image(img_src):
                                # ç¡®ä¿ä½¿ç”¨mediumå°ºå¯¸
                                if 'guide-images.cdn.ifixit.com' in img_src and not img_src.endswith('.medium'):
                                    # ç§»é™¤ç°æœ‰çš„å°ºå¯¸åç¼€å¹¶æ·»åŠ .medium
                                    if '.standard' in img_src or '.large' in img_src or '.small' in img_src:
                                        img_src = img_src.replace('.standard', '.medium').replace('.large', '.medium').replace('.small', '.medium')
                                    elif not any(suffix in img_src for suffix in ['.medium', '.jpg', '.png', '.gif']):
                                        img_src += '.medium'

                                # å»é‡
                                if img_src not in seen_urls:
                                    seen_urls.add(img_src)
                                    filtered_images.append(img_src)

                    step['images'] = filtered_images

        return guide_data

    def clean_product_name(self, text):
        """æ¸…ç†äº§å“åç§°ï¼Œç§»é™¤ä»·æ ¼ã€è¯„åˆ†ç­‰æ— å…³ä¿¡æ¯"""
        if not text:
            return ""

        # ç§»é™¤ä»·æ ¼ä¿¡æ¯
        text = re.sub(r'\$[\d.,]+.*$', '', text).strip()
        # ç§»é™¤è¯„åˆ†ä¿¡æ¯
        text = re.sub(r'[\d.]+\s*out of.*$', '', text).strip()
        text = re.sub(r'Sale price.*$', '', text).strip()
        text = re.sub(r'Rated [\d.]+.*$', '', text).strip()
        # ç§»é™¤å¸¸è§çš„æ— å…³è¯æ±‡
        unwanted_words = ['View', 'view', 'æŸ¥çœ‹', 'Add to cart', 'æ·»åŠ åˆ°è´­ç‰©è½¦', 'Buy', 'Sale', 'Available']
        for word in unwanted_words:
            if text.strip() == word:
                return ""

        # ç§»é™¤æè¿°æ€§æ–‡æœ¬
        text = re.sub(r'This kit contains.*$', '', text).strip()
        text = re.sub(r'Available for sale.*$', '', text).strip()

        return text.strip()

    async def crawl_combined_tree_async(self, start_url, category_name=None):
        """
        å¼‚æ­¥æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯
        await self._init_async_http_manager()

        try:
            # è®¾ç½®ç›®æ ‡URL
            self.target_url = start_url

            # æ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥
            if self.cache_manager and self.use_cache and not self.force_refresh:
                print("ğŸ” æ‰§è¡Œæ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥...")
                self._perform_cache_precheck(start_url, category_name)
                self.cache_manager.display_cache_report()

            try:
                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
                print("ğŸ“Š é˜¶æ®µ 1/3: æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
                print("   æ­£åœ¨åˆ†æé¡µé¢ç»“æ„å’Œåˆ†ç±»å±‚æ¬¡...")
                base_tree = await self._crawl_tree_async(start_url, category_name)

                if not base_tree:
                    print("âŒ æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
                    return None

                print(f"âœ… åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆ")
                print("=" * 60)

                # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
                print("ğŸ“ é˜¶æ®µ 2/3: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
                print("   æ­£åœ¨å¤„ç†èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯å’Œå…ƒæ•°æ®...")
                enriched_tree = await self._enrich_tree_with_detailed_content_async(base_tree)

                # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
                print("ğŸ” é˜¶æ®µ 3/3: æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
                print("   æ­£åœ¨æå–æŒ‡å—å’Œæ•…éšœæ’é™¤è¯¦ç»†ä¿¡æ¯...")
                final_tree = await self._deep_crawl_product_content_async(enriched_tree)

                return final_tree
            except Exception as e:
                self.logger.error(f"âœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print(f"âœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                # è¿”å›å·²ç»çˆ¬å–çš„æ•°æ®ï¼Œé¿å…å®Œå…¨å¤±è´¥
                return None

        finally:
            # ç¡®ä¿å…³é—­å¼‚æ­¥HTTPå®¢æˆ·ç«¯
            await self._close_async_http_manager()

    async def _crawl_tree_async(self, start_url, category_name=None):
        """å¼‚æ­¥ç‰ˆæœ¬çš„æ ‘å½¢ç»“æ„æ„å»º"""
        # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„tree_crawlerï¼Œå› ä¸ºå®ƒä¸»è¦æ˜¯æ•°æ®å¤„ç†
        # æˆ–è€…æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå¼‚æ­¥ç‰ˆæœ¬
        return self.tree_crawler.crawl_tree(start_url, category_name)

    async def _enrich_tree_with_detailed_content_async(self, tree):
        """å¼‚æ­¥ç‰ˆæœ¬çš„æ ‘èŠ‚ç‚¹å†…å®¹ä¸°å¯ŒåŒ–"""
        # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•å¤„ç†æ¯ä¸ªèŠ‚ç‚¹
        return await self._enrich_node_async(tree)

    async def _enrich_node_async(self, node):
        """å¼‚æ­¥å¤„ç†å•ä¸ªèŠ‚ç‚¹çš„å†…å®¹ä¸°å¯ŒåŒ–"""
        if isinstance(node, dict):
            # å¦‚æœèŠ‚ç‚¹æœ‰URLï¼Œå¼‚æ­¥è·å–è¯¦ç»†å†…å®¹
            if 'url' in node and node['url']:
                try:
                    soup = await self.get_soup_async(node['url'])
                    if soup:
                        # æå–èŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
                        node = await self._extract_node_details_async(node, soup)
                except httpx.HTTPStatusError as e:
                    if e.response and e.response.status_code == 404:
                        self.logger.warning(f"é¡µé¢ä¸å­˜åœ¨ {node.get('url', '')}: 404 Not Found")
                        print(f"âš ï¸ é¡µé¢ä¸å­˜åœ¨: {node.get('url', '')}")
                        # æ ‡è®°èŠ‚ç‚¹ä¸ºæ— æ•ˆä½†ç»§ç»­å¤„ç†
                        node['valid'] = False
                        node['error'] = "404 Not Found"
                    else:
                        self.logger.error(f"å¼‚æ­¥å¤„ç†èŠ‚ç‚¹å¤±è´¥ {node.get('url', '')}: {e}")
                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥å¤„ç†èŠ‚ç‚¹å¤±è´¥ {node.get('url', '')}: {e}")
                    # æ ‡è®°èŠ‚ç‚¹ä½†ä¸ä¸­æ–­å¤„ç†
                    node['error'] = str(e)

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in node and node['children']:
                # å¹¶å‘å¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
                child_tasks = [self._enrich_node_async(child) for child in node['children']]
                try:
                    node['children'] = await asyncio.gather(*child_tasks, return_exceptions=True)

                    # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœï¼Œä½†è®°å½•é”™è¯¯
                    filtered_children = []
                    for child in node['children']:
                        if isinstance(child, Exception):
                            self.logger.error(f"å­èŠ‚ç‚¹å¤„ç†å¤±è´¥: {child}")
                        else:
                            filtered_children.append(child)
                    node['children'] = filtered_children
                except Exception as e:
                    self.logger.error(f"å¤„ç†å­èŠ‚ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # ä¿ç•™ç°æœ‰å­èŠ‚ç‚¹ï¼Œä¸è®©æ•´ä¸ªå¤„ç†å¤±è´¥

        return node

    async def _extract_node_details_async(self, node, soup):
        """å¼‚æ­¥æå–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„èŠ‚ç‚¹ä¿¡æ¯æå–é€»è¾‘
        # ç›®å‰ä¿æŒåŸæœ‰é€»è¾‘
        return node

    async def _deep_crawl_product_content_async(self, tree):
        """å¼‚æ­¥æ·±å…¥çˆ¬å–äº§å“é¡µé¢å†…å®¹"""
        return await self._deep_crawl_node_async(tree)

    async def _deep_crawl_node_async(self, node):
        """å¼‚æ­¥æ·±å…¥çˆ¬å–å•ä¸ªèŠ‚ç‚¹çš„äº§å“å†…å®¹"""
        if isinstance(node, dict):
            # å¦‚æœæ˜¯äº§å“é¡µé¢ï¼Œæå–æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
            if node.get('type') == 'product' and node.get('url'):
                try:
                    # ä½¿ç”¨ç°æœ‰çš„å¼‚æ­¥å¹¶å‘å¤„ç†æ–¹æ³•
                    guides_data, troubleshooting_data = await self._extract_product_content_async(node['url'])

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥æå–äº§å“å†…å®¹å¤±è´¥ {node['url']}: {e}")

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in node and node['children']:
                child_tasks = [self._deep_crawl_node_async(child) for child in node['children']]
                node['children'] = await asyncio.gather(*child_tasks, return_exceptions=True)

                # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
                node['children'] = [child for child in node['children']
                                  if not isinstance(child, Exception)]

        return node

    async def _extract_product_content_async(self, product_url):
        """å¼‚æ­¥æå–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹"""
        try:
            soup = await self.get_soup_async(product_url)
            if not soup:
                return [], []

            # ä½¿ç”¨ç°æœ‰çš„å†…å®¹æå–é€»è¾‘
            guides_data = []
            troubleshooting_data = []

            # æŸ¥æ‰¾æŒ‡å—å’Œæ•…éšœæ’é™¤é“¾æ¥
            guide_links = []
            troubleshooting_links = []

            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„é“¾æ¥æå–é€»è¾‘
            # ç›®å‰ä½¿ç”¨ç°æœ‰çš„æ–¹æ³•ä½œä¸ºå‚è€ƒ

            # å¦‚æœæœ‰é“¾æ¥ï¼Œä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†
            if guide_links or troubleshooting_links:
                tasks = []
                for url in guide_links:
                    tasks.append(('guide', url))
                for url in troubleshooting_links:
                    tasks.append(('troubleshooting', url))

                if tasks:
                    guides_data, troubleshooting_data = await self._process_tasks_async(
                        tasks, guides_data, troubleshooting_data)

            return guides_data, troubleshooting_data

        except Exception as e:
            self.logger.error(f"å¼‚æ­¥æå–äº§å“å†…å®¹å¤±è´¥ {product_url}: {e}")
            return [], []

    def crawl_combined_tree(self, start_url, category_name=None):
        """
        æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {start_url}")

        # è®¾ç½®ç›®æ ‡URL
        self.target_url = start_url

        # ç¬¬ä¸€æ­¥ï¼šæ„å»ºåŸºç¡€æ ‘ç»“æ„
        print("ğŸ“Š é˜¶æ®µ 1/2: æ„å»ºæ ‘å½¢ç»“æ„...")
        base_tree = self.tree_crawler.crawl_tree(start_url, category_name)

        if not base_tree:
            print("âŒ æ— æ³•æ„å»ºæ ‘ç»“æ„")
            return None

        print(f"âœ… æ ‘ç»“æ„æ„å»ºå®Œæˆ")

        # ç¬¬äºŒæ­¥ï¼šé€æ­¥æå–å†…å®¹å¹¶ä¿å­˜åˆ°æ­£ç¡®çš„ç›®å½•ç»“æ„
        print("ğŸ“ é˜¶æ®µ 2/2: æå–å†…å®¹å¹¶ä¿å­˜...")
        final_tree = self._process_tree_and_save_incrementally(base_tree)

        return final_tree

    def _process_tree_and_save_incrementally(self, tree_data):
        """é€æ­¥å¤„ç†æ ‘ç»“æ„å¹¶ä¿å­˜åˆ°æ­£ç¡®çš„ç›®å½•ç»“æ„"""
        if not tree_data:
            return None

        # æ„å»ºåŸºç¡€ç›®å½•è·¯å¾„
        base_path = self._build_base_path_from_url(self.target_url)

        # é€’å½’å¤„ç†æ ‘ç»“æ„ï¼Œé€æ­¥ä¿å­˜æ¯ä¸ªèŠ‚ç‚¹
        processed_tree = self._process_node_incrementally(tree_data, base_path, [])

        return processed_tree

    def _build_base_path_from_url(self, url):
        """ä»URLæ„å»ºåŸºç¡€è·¯å¾„ - ä¿®å¤ç‰ˆæœ¬"""
        # å§‹ç»ˆè¿”å›Deviceæ ¹ç›®å½•ï¼Œè®©æ ‘ç»“æ„å†³å®šå®Œæ•´è·¯å¾„
        return Path(self.storage_root) / "Device"

    def _process_node_incrementally(self, node, base_path, path_segments):
        """é€’å½’å¤„ç†èŠ‚ç‚¹ï¼Œé€æ­¥ä¿å­˜å†…å®¹"""
        if not node or not isinstance(node, dict):
            return node

        node_name = node.get('name', 'Unknown')
        node_url = node.get('url', '')

        # æ„å»ºå½“å‰èŠ‚ç‚¹çš„è·¯å¾„æ®µ
        current_segments = path_segments.copy()
        if node_name and node_name.lower() != 'device':
            current_segments.append(node_name)

        # æ„å»ºå½“å‰èŠ‚ç‚¹çš„å®Œæ•´è·¯å¾„
        node_path = base_path
        for segment in current_segments:
            safe_segment = self._clean_directory_name(segment)
            node_path = node_path / safe_segment

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†å½“å‰èŠ‚ç‚¹
        if node_url and not node_url in self.processed_nodes:
            if self.verbose:
                print(f"ğŸ“¦ å¤„ç†: {' > '.join(current_segments)}")

            # æå–èŠ‚ç‚¹å†…å®¹
            enriched_node = self._extract_node_content(node)

            # å¦‚æœæœ‰å†…å®¹ï¼Œç«‹å³ä¿å­˜
            if self._has_content(enriched_node):
                self._save_node_immediately(enriched_node, node_path)

            self.processed_nodes.add(node_url)
        else:
            enriched_node = node

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in enriched_node and enriched_node['children']:
            for i, child in enumerate(enriched_node['children']):
                enriched_node['children'][i] = self._process_node_incrementally(
                    child, base_path, current_segments
                )

        return enriched_node

    def _extract_node_content(self, node):
        """æå–èŠ‚ç‚¹çš„è¯¦ç»†å†…å®¹ï¼ŒåŒ…æ‹¬guideå’Œtroubleshootingçš„å®Œæ•´å†…å®¹"""
        url = node.get('url', '')
        if not url:
            return node

        try:
            # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥æå–å†…å®¹ï¼Œä¸ä½¿ç”¨å¤æ‚ç¼“å­˜
            soup = self.get_soup(url)
            if not soup:
                return node

            # æå–guideså’Œtroubleshootingçš„åŸºæœ¬ä¿¡æ¯
            guides_basic = self.extract_guides_from_device_page(soup, url)
            troubleshooting_basic = self.extract_troubleshooting_from_device_page(soup, url)

            # æ›´æ–°èŠ‚ç‚¹æ•°æ®
            enriched_node = node.copy()

            # ä¸ºæ¯ä¸ªguideæå–è¯¦ç»†å†…å®¹
            if guides_basic:
                detailed_guides = []
                for guide_info in guides_basic:
                    guide_url = guide_info.get('url', '')
                    if guide_url:
                        if self.verbose:
                            print(f"   ğŸ“– æå–guideè¯¦ç»†å†…å®¹: {guide_info.get('title', 'Unknown')}")

                        # æå–è¯¦ç»†çš„guideå†…å®¹
                        detailed_guide = self.extract_guide_content(guide_url)
                        if detailed_guide:
                            detailed_guides.append(detailed_guide)
                        else:
                            # å¦‚æœè¯¦ç»†æå–å¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸºæœ¬ä¿¡æ¯
                            detailed_guides.append(guide_info)
                    else:
                        detailed_guides.append(guide_info)

                enriched_node['guides'] = detailed_guides

            # ä¸ºæ¯ä¸ªtroubleshootingæå–è¯¦ç»†å†…å®¹
            if troubleshooting_basic:
                detailed_troubleshooting = []
                for ts_info in troubleshooting_basic:
                    ts_url = ts_info.get('url', '')
                    if ts_url:
                        if self.verbose:
                            print(f"   ğŸ”§ æå–troubleshootingè¯¦ç»†å†…å®¹: {ts_info.get('title', 'Unknown')}")

                        # æå–è¯¦ç»†çš„troubleshootingå†…å®¹
                        detailed_ts = self.extract_troubleshooting_content(ts_url)
                        if detailed_ts:
                            detailed_troubleshooting.append(detailed_ts)
                        else:
                            # å¦‚æœè¯¦ç»†æå–å¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸºæœ¬ä¿¡æ¯
                            detailed_troubleshooting.append(ts_info)
                    else:
                        detailed_troubleshooting.append(ts_info)

                enriched_node['troubleshooting'] = detailed_troubleshooting

            return enriched_node

        except Exception as e:
            # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸è¾“å‡ºæ—¥å¿—
            return node

    def _has_content(self, node):
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰å®é™…å†…å®¹"""
        return (node.get('guides') and len(node['guides']) > 0) or \
               (node.get('troubleshooting') and len(node['troubleshooting']) > 0)

    def _save_node_immediately(self, node, node_path):
        """ç«‹å³ä¿å­˜èŠ‚ç‚¹å†…å®¹åˆ°æŒ‡å®šè·¯å¾„"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            node_path.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜åŸºæœ¬ä¿¡æ¯
            info_data = {k: v for k, v in node.items()
                        if k not in ['children', 'guides', 'troubleshooting']}

            info_file = node_path / "info.json"
            with open(info_file, 'w', encoding='utf-8') as f:
                safe_json_dump(info_data, f, ensure_ascii=False, indent=2)

            # ä¿å­˜guides
            if node.get('guides'):
                self._save_guides_to_directory(node['guides'], node_path)

            # ä¿å­˜troubleshooting
            if node.get('troubleshooting'):
                self._save_troubleshooting_to_directory(node['troubleshooting'], node_path)

            if self.verbose:
                print(f"   âœ… å·²ä¿å­˜åˆ°: {node_path}")

        except Exception as e:
            if self.verbose:
                print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")

    def _save_guides_to_directory(self, guides, base_path):
        """ä¿å­˜guidesåˆ°ç›®å½•"""
        guides_dir = base_path / "guides"
        guides_dir.mkdir(exist_ok=True)

        for i, guide in enumerate(guides):
            guide_dir = guides_dir / f"guide_{i+1}"
            guide_dir.mkdir(exist_ok=True)

            # å¤„ç†åª’ä½“æ–‡ä»¶
            self._process_media_urls(guide, guide_dir)

            # ä¿å­˜guideæ–‡ä»¶
            guide_file = guide_dir / "guide.json"
            with open(guide_file, 'w', encoding='utf-8') as f:
                safe_json_dump(guide, f, ensure_ascii=False, indent=2)

    def _save_troubleshooting_to_directory(self, troubleshooting, base_path):
        """ä¿å­˜troubleshootingåˆ°ç›®å½•"""
        ts_dir = base_path / "troubleshooting"
        ts_dir.mkdir(exist_ok=True)

        for i, ts in enumerate(troubleshooting):
            ts_item_dir = ts_dir / f"troubleshooting_{i+1}"
            ts_item_dir.mkdir(exist_ok=True)

            # å¤„ç†åª’ä½“æ–‡ä»¶
            self._process_media_urls(ts, ts_item_dir)

            # ä¿å­˜troubleshootingæ–‡ä»¶
            ts_file = ts_item_dir / "troubleshooting.json"
            with open(ts_file, 'w', encoding='utf-8') as f:
                safe_json_dump(ts, f, ensure_ascii=False, indent=2)

    def _perform_cache_precheck(self, start_url, category_name=None):
        """æ‰§è¡Œç¼“å­˜é¢„æ£€æŸ¥ï¼Œåˆ†æå“ªäº›å†…å®¹éœ€è¦é‡æ–°å¤„ç†"""
        try:
            # ç¡®ä¿cache_managerå­˜åœ¨
            if not self.cache_manager:
                print("   âš ï¸ ç¼“å­˜ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é¢„æ£€æŸ¥")
                return
                
            # æ£€æŸ¥clean_invalid_cacheæ–¹æ³•æ˜¯å¦å­˜åœ¨
            if hasattr(self.cache_manager, 'clean_invalid_cache'):
                # æ¸…ç†æ— æ•ˆç¼“å­˜
                cleaned_count = self.cache_manager.clean_invalid_cache()
                if cleaned_count > 0:
                    print(f"   å·²æ¸…ç† {cleaned_count} ä¸ªæ— æ•ˆç¼“å­˜æ¡ç›®")
            else:
                print("   âš ï¸ ç¼“å­˜ç®¡ç†å™¨æ²¡æœ‰clean_invalid_cacheæ–¹æ³•ï¼Œè·³è¿‡æ¸…ç†")

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„é¢„æ£€æŸ¥é€»è¾‘
            # æ¯”å¦‚æ£€æŸ¥ç›®æ ‡URLçš„æ•´ä½“ç¼“å­˜çŠ¶æ€
            print("   ç¼“å­˜é¢„æ£€æŸ¥å®Œæˆ")
                
        except Exception as e:
            self.logger.error(f"ç¼“å­˜é¢„æ£€æŸ¥å¤±è´¥: {e}")

    def _get_node_cache_path(self, url, node_name):
        """è·å–èŠ‚ç‚¹çš„ç¼“å­˜è·¯å¾„ï¼Œç¡®ä¿ä¸ä¿å­˜æ—¶çš„è·¯å¾„ä¸€è‡´"""
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜ç´¢å¼•ä¸­æ˜¯å¦æœ‰è¿™ä¸ªURLçš„è®°å½•
        if self.cache_manager:
            url_hash = self.cache_manager.get_url_hash(url)
            if url_hash in self.cache_manager.cache_index:
                cache_entry = self.cache_manager.cache_index[url_hash]
                cached_path = cache_entry.get('local_path', '')
                if cached_path:
                    return Path(self.storage_root) / cached_path

        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰è®°å½•ï¼Œå°è¯•æŸ¥æ‰¾å®é™…å­˜åœ¨çš„è·¯å¾„
        if '/Device/' in url:
            # é¦–å…ˆå°è¯•æŸ¥æ‰¾å®é™…å­˜åœ¨çš„è®¾å¤‡ç›®å½•
            actual_path = self._find_actual_device_path_for_cache(url)
            if actual_path:
                return actual_path

            # å¦‚æœæ‰¾ä¸åˆ°å®é™…è·¯å¾„ï¼Œä½¿ç”¨æ ‡å‡†è·¯å¾„æ„å»ºé€»è¾‘
            device_path = url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            if device_path:
                import urllib.parse
                device_path = urllib.parse.unquote(device_path)

                # è§£æè·¯å¾„å±‚çº§ï¼Œç›´æ¥ä»ä¸»è¦è®¾å¤‡ç±»å‹å¼€å§‹
                path_parts = device_path.split('/')
                if len(path_parts) > 0:
                    # æ‰¾åˆ°ä¸»è¦è®¾å¤‡ç±»å‹ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼‰
                    main_category = path_parts[0]

                    # å¦‚æœæœ‰å­è·¯å¾„ï¼Œä¿ç•™å®Œæ•´çš„å±‚çº§ç»“æ„ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
                    if len(path_parts) > 1:
                        sub_path = '/'.join(path_parts[1:])
                        return Path(self.storage_root) / "Device" / main_category / sub_path
                    else:
                        return Path(self.storage_root) / "Device" / main_category
                else:
                    return Path(self.storage_root) / "Device" / device_path
            else:
                return Path(self.storage_root) / "Device"
        else:
            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨èŠ‚ç‚¹åç§°ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
            safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")
            return Path(self.storage_root) / "Device" / safe_name

    def _find_actual_device_path_for_cache(self, url):
        """ä¸ºç¼“å­˜æŸ¥æ‰¾å®é™…å­˜åœ¨çš„è®¾å¤‡è·¯å¾„"""
        try:
            device_path = url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            from urllib.parse import unquote
            device_path = unquote(device_path)

            device_name = device_path.split('/')[-1]
            device_root = Path(self.storage_root) / "Device"

            if not device_root.exists():
                return None

            # é€’å½’æœç´¢åŒ¹é…çš„è®¾å¤‡ç›®å½•
            for path in device_root.rglob("*"):
                if path.is_dir() and path.name == device_name:
                    # æ£€æŸ¥è¿™ä¸ªç›®å½•æ˜¯å¦åŒ…å«info.jsonæ–‡ä»¶ï¼ˆç¡®è®¤æ˜¯è®¾å¤‡ç›®å½•ï¼‰
                    info_file = path / "info.json"
                    if info_file.exists():
                        return path

            return None

        except Exception as e:
            if self.verbose:
                self.logger.warning(f"æŸ¥æ‰¾å®é™…è®¾å¤‡è·¯å¾„å¤±è´¥: {e}")
            return None

    def _load_cached_node_data(self, local_path):
        """ä»ç¼“å­˜åŠ è½½èŠ‚ç‚¹æ•°æ®"""
        try:
            # åŠ è½½åŸºæœ¬ä¿¡æ¯
            info_file = local_path / "info.json"
            if not info_file.exists():
                return None

            with open(info_file, 'r', encoding='utf-8') as f:
                node_data = json.load(f)

            # åŠ è½½guidesæ•°æ®
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                guides = []
                for guide_file in sorted(guides_dir.glob("guide_*.json")):
                    with open(guide_file, 'r', encoding='utf-8') as f:
                        guides.append(json.load(f))
                if guides:
                    node_data['guides'] = guides

            # åŠ è½½troubleshootingæ•°æ®
            ts_dir = local_path / "troubleshooting"
            if ts_dir.exists():
                troubleshooting = []
                for ts_file in sorted(ts_dir.glob("troubleshooting_*.json")):
                    with open(ts_file, 'r', encoding='utf-8') as f:
                        troubleshooting.append(json.load(f))
                if troubleshooting:
                    node_data['troubleshooting'] = troubleshooting

            return node_data

        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None

    def _enrich_tree_with_detailed_content_and_save(self, node, parent_path=None):
        """ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼Œå¹¶åœ¨å¤„ç†æ¯ä¸ªèŠ‚ç‚¹åç«‹å³ä¿å­˜æ•°æ®"""
        if not node or not isinstance(node, dict):
            return node
            
        if parent_path is None:
            parent_path = []
            
        # å¤åˆ¶å½“å‰è·¯å¾„ä¿¡æ¯
        current_path = parent_path.copy()
        node_name = node.get('name', 'Unknown')
        current_path.append(node_name)
        path_str = " > ".join(current_path)
            
        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in node and node['children']:
                for i, child in enumerate(node['children']):
                    node['children'][i] = self._enrich_tree_with_detailed_content_and_save(child, current_path)
            return node

        # æ£€æŸ¥ç¼“å­˜ - ä½¿ç”¨ä¸ä¿å­˜æ—¶ä¸€è‡´çš„è·¯å¾„æ„å»ºé€»è¾‘
        local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

        if self.verbose:
            print(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
            print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

        if self._check_cache_validity(url, local_path):
            self.stats["cache_hits"] += 1
            if self.verbose:
                print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡å¤„ç†: {node.get('name', '')}")
            else:
                print(f"   âœ… è·³è¿‡å·²ç¼“å­˜: {node.get('name', '')}")
                
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if 'children' in node and node['children']:
                for i, child in enumerate(node['children']):
                    node['children'][i] = self._enrich_tree_with_detailed_content_and_save(child, current_path)
                    
            return node

        self.processed_nodes.add(url)
        self.stats["cache_misses"] += 1
        
        try:
            print(f"ğŸ” å¤„ç†èŠ‚ç‚¹: {path_str}")
            print(f"   ğŸ“ URL: {url}")

            # è·å–é¡µé¢å†…å®¹
            soup = self.get_soup(url)
            if soup:
                # ä¿®å¤èŠ‚ç‚¹æ•°æ®
                node = self.fix_node_data(node, soup)
                
                # å¤„ç†å®Œæˆåç«‹å³ä¿å­˜å½“å‰èŠ‚ç‚¹æ•°æ®
                if node.get('guides') or node.get('troubleshooting'):
                    print(f"   ğŸ’¾ ç«‹å³ä¿å­˜èŠ‚ç‚¹æ•°æ®: {path_str}")
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    local_path.mkdir(parents=True, exist_ok=True)
                    self._save_node_content(node, local_path)
                    print(f"   âœ… èŠ‚ç‚¹æ•°æ®å·²ä¿å­˜åˆ°: {local_path}")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†èŠ‚ç‚¹å¤±è´¥: {str(e)}")
            self.stats["errors"] += 1
            self.logger.error(f"å¤„ç†èŠ‚ç‚¹å¤±è´¥: {url} - {str(e)}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for i, child in enumerate(node['children']):
                node['children'][i] = self._enrich_tree_with_detailed_content_and_save(child, current_path)

        return node
        
    def enrich_tree_with_detailed_content(self, node):
        """ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ­£ç¡®çš„å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            return node

        # æ£€æŸ¥ç¼“å­˜ - ä½¿ç”¨ä¸ä¿å­˜æ—¶ä¸€è‡´çš„è·¯å¾„æ„å»ºé€»è¾‘
        local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

        if self.verbose:
            print(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
            print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

        if self._check_cache_validity(url, local_path):
            self.stats["cache_hits"] += 1
            if self.verbose:
                print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡å¤„ç†: {node.get('name', '')}")
            else:
                print(f"   âœ… è·³è¿‡å·²ç¼“å­˜: {node.get('name', '')}")
            return node

        self.processed_nodes.add(url)
        self.stats["cache_misses"] += 1

        time.sleep(random.uniform(0.5, 1.0))
        soup = self.get_soup(url)

        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½ç»è¿‡fix_node_dataå¤„ç†
        node = self.fix_node_data(node, soup)

        if self.verbose:
            self.logger.info(f"å¤„ç†èŠ‚ç‚¹: {node.get('name', '')} - {url}")
        else:
            print(f"   ğŸ“„ å¤„ç†èŠ‚ç‚¹: {node.get('name', '')}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            children_count = len(node['children'])
            if children_count > 0:
                print(f"   ğŸŒ³ å¤„ç† {children_count} ä¸ªå­èŠ‚ç‚¹...")
            enriched_children = []
            for i, child in enumerate(node['children'], 1):
                if children_count > 1:
                    print(f"      â””â”€ [{i}/{children_count}] {child.get('name', 'Unknown')}")
                enriched_child = self.enrich_tree_with_detailed_content(child)
                if enriched_child:
                    enriched_children.append(enriched_child)
            node['children'] = enriched_children

        return node

    def _process_content_concurrently(self, guide_links, device_url, skip_troubleshooting, soup):
        """å¹¶å‘å¤„ç†guideså’Œtroubleshootingå†…å®¹"""
        guides_data = []
        troubleshooting_data = []

        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []

        # æ·»åŠ guideä»»åŠ¡
        for guide_link in guide_links:
            tasks.append(('guide', guide_link))

        # æ·»åŠ troubleshootingä»»åŠ¡
        if not skip_troubleshooting:
            print(f"    ğŸ”§ æ­£åœ¨æœç´¢æ•…éšœæ’é™¤é¡µé¢...")

            # æ£€æŸ¥troubleshootingéƒ¨åˆ†çš„ç¼“å­˜
            troubleshooting_cache_key = f"{device_url}#troubleshooting"
            cached_troubleshooting = self._check_troubleshooting_cache(troubleshooting_cache_key, device_url)

            if cached_troubleshooting is not None:
                print(f"    âœ… ä½¿ç”¨ç¼“å­˜çš„æ•…éšœæ’é™¤æ•°æ® ({len(cached_troubleshooting)} ä¸ª)")
                troubleshooting_data = cached_troubleshooting
                # æ ‡è®°ä¸ºå·²ä»ç¼“å­˜åŠ è½½ï¼Œé¿å…é‡å¤ä¿å­˜
                self._troubleshooting_from_cache = True
            else:
                print(f"    ğŸ” æœªæ‰¾åˆ°æ•…éšœæ’é™¤ç¼“å­˜ï¼Œå¼€å§‹æå–...")
                troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, device_url)
                print(f"    ğŸ”§ æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢")
                self._troubleshooting_from_cache = False

                for ts_link in troubleshooting_links:
                    if isinstance(ts_link, dict):
                        ts_url = ts_link.get('url', '')
                    else:
                        ts_url = ts_link
                    if ts_url:
                        tasks.append(('troubleshooting', ts_url))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        if self.max_workers > 1 and len(tasks) > 1:
            print(f"    ğŸ”„ å¯åŠ¨å¹¶å‘å¤„ç† ({self.max_workers} çº¿ç¨‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡)...")

        if len(tasks) > 0:
            # ä½¿ç”¨ä¼ ç»Ÿçº¿ç¨‹æ± ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            print(f"    ğŸ”„ å¯åŠ¨çº¿ç¨‹æ± å¹¶å‘å¤„ç† ({self.max_workers} çº¿ç¨‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡)...")
            with ThreadPoolExecutor(max_workers=min(self.max_workers, len(tasks))) as executor:
                future_to_task = {}
                completed_count = 0
                total_tasks = len(tasks)

                for i, (task_type, url) in enumerate(tasks):
                    if task_type == 'guide':
                        future = executor.submit(self._process_guide_task_with_proxy, url, i)
                    else:  # troubleshooting
                        future = executor.submit(self._process_troubleshooting_task_with_proxy, url, i)
                    future_to_task[future] = (task_type, url)
                    # æ˜¾ç¤ºä»»åŠ¡å¼€å§‹æ—¶é—´
                    current_time = time.strftime("%H:%M:%S", time.localtime())
                    proxy_id = i % (self.proxy_manager.pool_size if self.proxy_manager else 1)
                    print(f"    ğŸš€ [{current_time}] å¯åŠ¨ {task_type} ä»»åŠ¡ (ä»£ç†#{proxy_id}): {url.split('/')[-1]}")

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task_type, url = future_to_task[future]
                    completed_count += 1
                    current_time = time.strftime("%H:%M:%S", time.localtime())
                    try:
                        result = future.result()
                        if result:
                            if task_type == 'guide':
                                guides_data.append(result)
                                print(f"    âœ… [{current_time}] [{completed_count}/{total_tasks}] æŒ‡å—: {result.get('title', '')}")
                            else:
                                troubleshooting_data.append(result)
                                print(f"    âœ… [{current_time}] [{completed_count}/{total_tasks}] æ•…éšœæ’é™¤: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{current_time}] [{completed_count}/{total_tasks}] {task_type} å¤„ç†å¤±è´¥")
                    except Exception as e:
                        print(f"    âŒ [{current_time}] [{completed_count}/{total_tasks}] {task_type} ä»»åŠ¡å¤±è´¥: {str(e)[:50]}...")
                        self.logger.error(f"å¹¶å‘ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")
        else:
            # å•çº¿ç¨‹å¤„ç†
            print(f"    ğŸ”„ é¡ºåºå¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
            for i, (task_type, url) in enumerate(tasks, 1):
                try:
                    print(f"    â³ [{i}/{len(tasks)}] æ­£åœ¨å¤„ç† {task_type}...")
                    if task_type == 'guide':
                        result = self._process_guide_task(url)
                        if result:
                            guides_data.append(result)
                            print(f"    âœ… [{i}/{len(tasks)}] æŒ‡å—: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{i}/{len(tasks)}] æŒ‡å—å¤„ç†å¤±è´¥")
                    else:
                        result = self._process_troubleshooting_task(url)
                        if result:
                            troubleshooting_data.append(result)
                            print(f"    âœ… [{i}/{len(tasks)}] æ•…éšœæ’é™¤: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{i}/{len(tasks)}] æ•…éšœæ’é™¤å¤„ç†å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")

        # ä¸åœ¨è¿™é‡Œä¿å­˜troubleshootingç¼“å­˜ï¼Œç­‰å¾…æ–‡ä»¶ç³»ç»Ÿä¿å­˜é˜¶æ®µ
        # if troubleshooting_data and not skip_troubleshooting and not getattr(self, '_troubleshooting_from_cache', False):
        #     troubleshooting_cache_key = f"{device_url}#troubleshooting"
        #     print(f"    ğŸ’¾ ä¿å­˜æ•…éšœæ’é™¤æ•°æ®åˆ°ç¼“å­˜ ({len(troubleshooting_data)} ä¸ª)")
        #     self._save_troubleshooting_cache(troubleshooting_cache_key, device_url, troubleshooting_data)

        return guides_data, troubleshooting_data

    def _process_guide_task(self, guide_url):
        """å¤„ç†å•ä¸ªguideä»»åŠ¡"""
        try:
            guide_content = self.extract_guide_content(guide_url)
            if guide_content:
                guide_content['url'] = guide_url

                # ç¡®ä¿"What You Need"éƒ¨åˆ†è¢«æ­£ç¡®æå–
                if 'what_you_need' not in guide_content or not guide_content['what_you_need']:
                    print(f"  é‡æ–°å°è¯•æå–What You Needéƒ¨åˆ†: {guide_url}")
                    what_you_need = self.extract_what_you_need_enhanced(guide_url)
                    if what_you_need:
                        guide_content['what_you_need'] = what_you_need
                        print(f"  æˆåŠŸæå–What You Need: {list(what_you_need.keys())}")

                return guide_content
        except Exception as e:
            self.logger.error(f"å¤„ç†guideå¤±è´¥ {guide_url}: {e}")
            self._log_failed_url(guide_url, f"Guideå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_troubleshooting_task(self, ts_url):
        """å¤„ç†å•ä¸ªtroubleshootingä»»åŠ¡"""
        try:
            ts_content = self.extract_troubleshooting_content(ts_url)
            if ts_content:
                ts_content['url'] = ts_url
                return ts_content
        except Exception as e:
            self.logger.error(f"å¤„ç†troubleshootingå¤±è´¥ {ts_url}: {e}")
            self._log_failed_url(ts_url, f"Troubleshootingå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_guide_task_with_proxy(self, guide_url, thread_id):
        """å¤„ç†å•ä¸ªguideä»»åŠ¡ï¼ˆå¸¦ç‹¬ç«‹ä»£ç†ï¼‰"""
        try:
            # ä¸ºå½“å‰çº¿ç¨‹è®¾ç½®ä»£ç†ID
            current_thread = threading.current_thread()
            # ä½¿ç”¨çº¿ç¨‹æœ¬åœ°å­˜å‚¨æ¥å­˜å‚¨ä»£ç†ID
            thread_local = threading.local()
            if not hasattr(thread_local, 'proxy_id'):
                thread_local.proxy_id = thread_id

            guide_content = self.extract_guide_content(guide_url)
            if guide_content:
                guide_content['url'] = guide_url

                # ç¡®ä¿"What You Need"éƒ¨åˆ†è¢«æ­£ç¡®æå–
                if 'what_you_need' not in guide_content or not guide_content['what_you_need']:
                    print(f"  é‡æ–°å°è¯•æå–What You Needéƒ¨åˆ†: {guide_url}")
                    what_you_need = self.extract_what_you_need_enhanced(guide_url)
                    if what_you_need:
                        guide_content['what_you_need'] = what_you_need
                        print(f"  æˆåŠŸæå–What You Need: {list(what_you_need.keys())}")

                return guide_content
        except Exception as e:
            self.logger.error(f"å¤„ç†guideå¤±è´¥ {guide_url}: {e}")
            self._log_failed_url(guide_url, f"Guideå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_troubleshooting_task_with_proxy(self, ts_url, thread_id):
        """å¤„ç†å•ä¸ªtroubleshootingä»»åŠ¡ï¼ˆå¸¦ç‹¬ç«‹ä»£ç†ï¼‰"""
        try:
            # ä¸ºå½“å‰çº¿ç¨‹è®¾ç½®ä»£ç†ID
            current_thread = threading.current_thread()
            # ä½¿ç”¨çº¿ç¨‹æœ¬åœ°å˜é‡å­˜å‚¨ä»£ç†ID
            thread_local = threading.local()
            thread_local.proxy_id = thread_id

            ts_content = self.extract_troubleshooting_content(ts_url)
            if ts_content:
                ts_content['url'] = ts_url
                return ts_content
        except Exception as e:
            self.logger.error(f"å¤„ç†troubleshootingå¤±è´¥ {ts_url}: {e}")
            self._log_failed_url(ts_url, f"Troubleshootingå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _check_troubleshooting_cache(self, cache_key, device_url):
        """æ£€æŸ¥troubleshootingéƒ¨åˆ†çš„ç¼“å­˜"""
        if not self.cache_manager:
            return None

        if self.cache_manager.is_troubleshooting_section_cached(cache_key, device_url):
            return self.cache_manager.load_troubleshooting_cache(cache_key, device_url)
        return None

    def _should_have_troubleshooting_content(self, url):
        """æ£€æŸ¥é¡µé¢æ˜¯å¦åº”è¯¥æœ‰troubleshootingå†…å®¹"""
        try:
            # å¯¹äºäº§å“é¡µé¢ï¼Œé€šå¸¸éƒ½åº”è¯¥æœ‰troubleshootingå†…å®¹
            # è¿™é‡Œå¯ä»¥æ ¹æ®URLæ¨¡å¼æˆ–é¡µé¢ç±»å‹åˆ¤æ–­
            if '/Device/' in url and not url.endswith('/Device'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å…·ä½“çš„äº§å“é¡µé¢è€Œä¸æ˜¯åˆ†ç±»é¡µé¢
                # å¯ä»¥é€šè¿‡è®¿é—®é¡µé¢å¿«é€Ÿæ£€æŸ¥æ˜¯å¦æœ‰troubleshootingé“¾æ¥
                return True
            return False
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰troubleshootingå†…å®¹å¤±è´¥: {e}")
            return False

    def _mark_cache_invalid(self, url, local_path):
        """æ ‡è®°ç¼“å­˜ä¸ºæ— æ•ˆå¹¶ä»ç´¢å¼•ä¸­ç§»é™¤"""
        try:
            if self.cache_manager:
                url_hash = self.cache_manager.get_url_hash(url)
                if url_hash in self.cache_manager.cache_index:
                    del self.cache_manager.cache_index[url_hash]
                    self.cache_manager.save_cache_index()
                    print(f"    ğŸ—‘ï¸ å·²ä»ç¼“å­˜ç´¢å¼•ä¸­ç§»é™¤æ— æ•ˆæ¡ç›®: {url}")
        except Exception as e:
            self.logger.error(f"æ ‡è®°ç¼“å­˜æ— æ•ˆå¤±è´¥: {e}")

    def _is_troubleshooting_cached(self, cache_key):
        """æ£€æŸ¥troubleshootingæ˜¯å¦å·²ç¼“å­˜"""
        if not self.cache_manager:
            return False
        return self.cache_manager.is_troubleshooting_section_cached(cache_key, "")

    def _save_troubleshooting_cache(self, cache_key, device_url, troubleshooting_data):
        """ä¿å­˜troubleshootingåˆ°ç¼“å­˜"""
        if self.cache_manager:
            self.cache_manager.save_troubleshooting_cache(cache_key, device_url, troubleshooting_data)

    def _print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)

        total_requests = self.stats["total_requests"]
        cache_hits = self.stats["cache_hits"]
        cache_misses = self.stats["cache_misses"]

        if total_requests > 0:
            cache_hit_rate = (cache_hits / (cache_hits + cache_misses)) * 100 if (cache_hits + cache_misses) > 0 else 0
            print(f"ğŸŒ æ€»è¯·æ±‚æ•°: {total_requests}")
            print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_hits} ({cache_hit_rate:.1f}%)")
            print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­: {cache_misses}")

        retry_success = self.stats["retry_success"]
        retry_failed = self.stats["retry_failed"]
        total_retries = retry_success + retry_failed

        if total_retries > 0:
            retry_success_rate = (retry_success / total_retries) * 100
            print(f"ğŸ” é‡è¯•æˆåŠŸ: {retry_success}/{total_retries} ({retry_success_rate:.1f}%)")
            print(f"âŒ é‡è¯•å¤±è´¥: {retry_failed}")

        media_downloaded = self.stats["media_downloaded"]
        media_failed = self.stats["media_failed"]
        total_media = media_downloaded + media_failed

        if total_media > 0:
            media_success_rate = (media_downloaded / total_media) * 100
            print(f"ğŸ“ åª’ä½“ä¸‹è½½æˆåŠŸ: {media_downloaded}/{total_media} ({media_success_rate:.1f}%)")
            print(f"ğŸ“ åª’ä½“ä¸‹è½½å¤±è´¥: {media_failed}")

        # è§†é¢‘å¤„ç†ç»Ÿè®¡
        videos_downloaded = self.stats.get("videos_downloaded", 0)
        videos_skipped = self.stats.get("videos_skipped", 0)
        total_videos = videos_downloaded + videos_skipped

        if total_videos > 0:
            print(f"ğŸ¥ è§†é¢‘æ–‡ä»¶å¤„ç†:")
            print(f"   âœ… å·²ä¸‹è½½: {videos_downloaded}")
            print(f"   â­ï¸ å·²è·³è¿‡: {videos_skipped}")
            if not self.download_videos:
                print(f"   ğŸ’¡ æç¤º: è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå¦‚éœ€ä¸‹è½½è¯·è®¾ç½® download_videos=True")
            else:
                print(f"   ğŸ“ å¤§å°é™åˆ¶: {self.max_video_size_mb}MB")

        # æ€§èƒ½ä¼˜åŒ–ç»Ÿè®¡
        cache_size = len(getattr(self, '_file_exists_cache', {}))
        if cache_size > 0:
            print(f"âš¡ æ€§èƒ½ä¼˜åŒ–ç»Ÿè®¡:")
            print(f"   ğŸ“‹ æ–‡ä»¶ç¼“å­˜æ¡ç›®: {cache_size}")
            print(f"   ğŸš€ å¼‚æ­¥ä¸‹è½½ä¼˜åŒ–: å·²å¯ç”¨")
            print(f"   ğŸ”„ é«˜å¹¶å‘ä¸‹è½½: å·²å¯ç”¨")

        # æ£€æŸ¥ä»£ç†åˆ‡æ¢æ¬¡æ•°
        if self.use_proxy and hasattr(self, 'proxy_manager') and self.proxy_manager:
            proxy_stats = self.proxy_manager.get_stats()
            if proxy_stats and 'switches' in proxy_stats:
                print(f"ğŸ”„ ä»£ç†åˆ‡æ¢æ¬¡æ•°: {proxy_stats['switches']}")

        print("=" * 60)
        


    def count_detailed_nodes(self, node):
        """
        ç»Ÿè®¡æ ‘ä¸­åŒ…å«è¯¦ç»†å†…å®¹çš„èŠ‚ç‚¹æ•°é‡
        """
        count = {'guides': 0, 'troubleshooting': 0, 'products': 0, 'categories': 0}

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„guideså’Œtroubleshootingæ•°ç»„
        if 'guides' in node and isinstance(node['guides'], list):
            count['guides'] += len(node['guides'])

        if 'troubleshooting' in node and isinstance(node['troubleshooting'], list):
            count['troubleshooting'] += len(node['troubleshooting'])

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹
        if 'title' in node and 'steps' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹ï¼ˆä¸åœ¨guidesæ•°ç»„ä¸­ï¼‰
            count['guides'] += 1
        elif 'causes' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹ï¼ˆä¸åœ¨troubleshootingæ•°ç»„ä¸­ï¼‰
            count['troubleshooting'] += 1
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            count['products'] += 1
        else:
            # åˆ†ç±»èŠ‚ç‚¹
            count['categories'] += 1

        # é€’å½’ç»Ÿè®¡å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for child in node['children']:
                child_count = self.count_detailed_nodes(child)
                for key in count:
                    count[key] += child_count[key]

        return count

    def extract_guides_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æŒ‡å—é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§é¡µé¢å¸ƒå±€
        """
        guides = []
        seen_guide_urls = set()

        if not soup:
            return guides

        # æŸ¥æ‰¾æ‰€æœ‰æŒ‡å—é“¾æ¥ï¼ˆåŒ…æ‹¬Guideå’ŒTeardownï¼‰
        guide_links = soup.select("a[href*='/Guide/'], a[href*='/Teardown/']")

        for link in guide_links:
            href = link.get("href")
            text = link.get_text().strip()

            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
            skip_patterns = [
                "Create Guide", "åˆ›å»ºæŒ‡å—", "Guide/new", "Guide/edit", "/edit/",
                "Guide/history", "/history/", "Edit Guide", "ç¼–è¾‘æŒ‡å—",
                "Version History", "ç‰ˆæœ¬å†å²", "Teardown/edit", "#comment", "permalink=comment"
            ]

            if href and text and not any(pattern in href or pattern in text for pattern in skip_patterns):
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_guide_urls:
                    seen_guide_urls.add(href)
                    guides.append({
                        "title": text,
                        "url": href
                    })

        return guides

    def extract_troubleshooting_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æ•…éšœæ’é™¤é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬
        """
        troubleshooting_links = []
        seen_ts_urls = set()

        if not soup:
            return troubleshooting_links

        # æŸ¥æ‰¾æ‰€æœ‰æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
        # 1. ç›´æ¥çš„Troubleshootingé“¾æ¥
        ts_links = soup.select("a[href*='/Troubleshooting/']")

        # 2. æŸ¥æ‰¾åŒ…å«æ•…éšœæ’é™¤æ–‡æœ¬çš„é“¾æ¥
        all_links = soup.select("a[href]")
        for link in all_links:
            text = link.get_text().strip().lower()
            href = link.get("href", "")

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
            troubleshooting_keywords = [
                'troubleshooting', 'æ•…éšœæ’é™¤', 'æ•…éšœ', 'problems', 'issues',
                'won\'t turn on', 'not working', 'broken'
            ]

            if any(keyword in text for keyword in troubleshooting_keywords) or '/Troubleshooting/' in href:
                ts_links.append(link)

        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        for link in ts_links:
            href = link.get("href")
            text = link.get_text().strip()

            if href and text:
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_ts_urls and '/Troubleshooting/' in href:
                    seen_ts_urls.add(href)
                    troubleshooting_links.append({
                        "title": text,
                        "url": href
                    })

        return troubleshooting_links

    def extract_troubleshooting_content(self, troubleshooting_url):
        """
        æå–æ•…éšœæ’é™¤é¡µé¢çš„è¯¦ç»†å†…å®¹ - å®Œå…¨æŒ‰ç…§combined_crawler.pyçš„é€»è¾‘
        """
        if not troubleshooting_url:
            return None

        # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in troubleshooting_url:
            troubleshooting_url = troubleshooting_url.replace('zh.ifixit.com', 'www.ifixit.com')

        # æ·»åŠ lang=enå‚æ•°
        if '?' in troubleshooting_url:
            if 'lang=' not in troubleshooting_url:
                troubleshooting_url += '&lang=en'
        else:
            troubleshooting_url += '?lang=en'

        print(f"Crawling troubleshooting content: {troubleshooting_url}")

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(random.uniform(1, 2))

        soup = self.get_soup(troubleshooting_url)
        if not soup:
            return None

        troubleshooting_data = {
            "url": troubleshooting_url,
            "title": "",
            "causes": []
        }

        try:
            # æå–æ ‡é¢˜
            title_elem = soup.select_one("h1")
            if title_elem:
                title_text = title_elem.get_text().strip()
                title_text = re.sub(r'\s+', ' ', title_text)
                troubleshooting_data["title"] = title_text
                print(f"æå–æ ‡é¢˜: {title_text}")

            # åŠ¨æ€æå–é¡µé¢ä¸Šçš„çœŸå®å­—æ®µå†…å®¹ï¼ˆå¦‚first_stepsç­‰ï¼‰
            dynamic_sections = self.extract_dynamic_sections(soup)
            troubleshooting_data.update(dynamic_sections)

            # æå–causeså¹¶ä¸ºæ¯ä¸ªcauseæå–å¯¹åº”çš„å›¾ç‰‡å’Œè§†é¢‘
            troubleshooting_data["causes"] = self.extract_causes_sections_with_media(soup)

            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•…éšœæ’é™¤é¡µé¢
            if not self._is_valid_troubleshooting_page(troubleshooting_data):
                print(f"è·³è¿‡é€šç”¨æˆ–æ— æ•ˆçš„æ•…éšœæ’é™¤é¡µé¢: {troubleshooting_data.get('title', '')}")
                return None

            # æå–ç»Ÿè®¡æ•°æ®
            statistics = self.extract_page_statistics(soup)

            # é‡æ„ç»Ÿè®¡æ•°æ®ç»“æ„ - å°†view_statisticsç§»åŠ¨åˆ°titleä¸‹é¢
            # é‡æ–°æ„å»ºtroubleshooting_dataï¼Œç¡®ä¿å­—æ®µé¡ºåºæ­£ç¡®
            ordered_ts_data = {}
            ordered_ts_data["url"] = troubleshooting_data["url"]
            ordered_ts_data["title"] = troubleshooting_data["title"]

            # å°†ç»Ÿè®¡æ•°æ®ç´§è·Ÿåœ¨titleåé¢ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if statistics:
                # åˆ›å»ºview_statisticså¯¹è±¡ï¼ŒåªåŒ…å«æ—¶é—´ç›¸å…³çš„ç»Ÿè®¡
                view_stats = {}
                if 'past_24_hours' in statistics:
                    view_stats['past_24_hours'] = statistics['past_24_hours']
                if 'past_7_days' in statistics:
                    view_stats['past_7_days'] = statistics['past_7_days']
                if 'past_30_days' in statistics:
                    view_stats['past_30_days'] = statistics['past_30_days']
                if 'all_time' in statistics:
                    view_stats['all_time'] = statistics['all_time']

                if view_stats:
                    ordered_ts_data["view_statistics"] = view_stats
                if 'completed' in statistics:
                    ordered_ts_data["completed"] = statistics['completed']
                if 'favorites' in statistics:
                    ordered_ts_data["favorites"] = statistics['favorites']

            # æ·»åŠ åŠ¨æ€æå–çš„å­—æ®µï¼ˆæŒ‰é¡µé¢å®é™…å­—æ®µåç§°ï¼‰
            for key, value in troubleshooting_data.items():
                if key not in ["url", "title", "causes", "images", "videos"] and value:
                    ordered_ts_data[key] = value

            # æ·»åŠ å…¶ä»–å­—æ®µ
            if "causes" in troubleshooting_data and troubleshooting_data["causes"]:
                ordered_ts_data["causes"] = troubleshooting_data["causes"]

            troubleshooting_data = ordered_ts_data

            # æ‰“å°æå–ç»“æœç»Ÿè®¡
            dynamic_fields = [k for k in troubleshooting_data.keys()
                            if k not in ['url', 'title', 'causes', 'view_statistics', 'completed', 'favorites']]

            print(f"æå–å®Œæˆ:")
            for field in dynamic_fields:
                content = troubleshooting_data.get(field, '')
                if content:
                    print(f"  {field}: {len(content)} å­—ç¬¦")

            causes = troubleshooting_data.get('causes', [])
            print(f"  Causes: {len(causes)} ä¸ª")

            # ç»Ÿè®¡æ¯ä¸ªcauseä¸­çš„å›¾ç‰‡å’Œè§†é¢‘æ•°é‡
            total_images = sum(len(cause.get('images', [])) for cause in causes)
            total_videos = sum(len(cause.get('videos', [])) for cause in causes)
            print(f"  Images (in causes): {total_images} ä¸ª")
            print(f"  Videos (in causes): {total_videos} ä¸ª")

            if statistics:
                print(f"  Statistics: {len(statistics)} é¡¹")

            return troubleshooting_data

        except Exception as e:
            print(f"æå–æ•…éšœæ’é™¤å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _is_valid_troubleshooting_page(self, troubleshooting_data):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å…·ä½“æ•…éšœæ’é™¤é¡µé¢ï¼Œè€Œä¸æ˜¯é€šç”¨é¡µé¢"""
        if not troubleshooting_data:
            return False

        title = troubleshooting_data.get('title', '').lower()
        causes = troubleshooting_data.get('causes', [])

        # æ£€æŸ¥æ˜¯å¦ä¸ºé€šç”¨é¡µé¢çš„æ ‡é¢˜
        generic_keywords = [
            'common problems', 'fix common', 'general troubleshooting',
            'troubleshooting guide', 'basic troubleshooting', 'general issues',
            'common issues', 'general problems'
        ]

        for keyword in generic_keywords:
            if keyword in title:
                print(f"æ£€æµ‹åˆ°é€šç”¨é¡µé¢æ ‡é¢˜å…³é”®è¯: {keyword}")
                return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ•…éšœåŸå› 
        if not causes or len(causes) == 0:
            print("æ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„æ•…éšœåŸå› ")
            return False

        # æ£€æŸ¥causeså†…å®¹æ˜¯å¦è¶³å¤Ÿå…·ä½“
        if len(causes) < 2:  # è‡³å°‘è¦æœ‰2ä¸ªå…·ä½“çš„æ•…éšœåŸå› 
            print(f"æ•…éšœåŸå› å¤ªå°‘: {len(causes)} ä¸ª")
            return False

        # æ£€æŸ¥causesæ˜¯å¦æœ‰å®é™…å†…å®¹
        valid_causes = 0
        for cause in causes:
            content = cause.get('content', '')
            if content and len(content.strip()) > 50:  # å†…å®¹è¦è¶³å¤Ÿè¯¦ç»†
                valid_causes += 1

        if valid_causes < 2:
            print(f"æœ‰æ•ˆçš„æ•…éšœåŸå› å¤ªå°‘: {valid_causes} ä¸ª")
            return False

        print(f"éªŒè¯é€šè¿‡: {len(causes)} ä¸ªæ•…éšœåŸå› , {valid_causes} ä¸ªæœ‰æ•ˆ")
        return True

    def _deep_crawl_product_content_and_save(self, node, parent_path=None, skip_troubleshooting=False):
        """æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶ç«‹å³ä¿å­˜æ•°æ®"""
        if not node or not isinstance(node, dict):
            return node
            
        if parent_path is None:
            parent_path = []
            
        # å¤åˆ¶å½“å‰è·¯å¾„ä¿¡æ¯
        current_path = parent_path.copy()
        node_name = node.get('name', 'Unknown')
        current_path.append(node_name)
        path_str = " > ".join(current_path)

        url = node.get('url', '')
        is_specified_target = self._is_specified_target_url(url)

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        children_count = len(node.get('children', []))
        print(f"ğŸ” æ·±åº¦çˆ¬å–èŠ‚ç‚¹: {node_name} (å­èŠ‚ç‚¹: {children_count})")

        if self.verbose:
            print(f"   URL: {url}")
            print(f"   æ˜¯å¦æŒ‡å®šç›®æ ‡: {is_specified_target}")
            print(f"   æ˜¯å¦è·³è¿‡troubleshooting: {skip_troubleshooting}")
            if node.get('children'):
                child_names = [child.get('name', 'Unknown') for child in node['children'][:3]]
                if len(node['children']) > 3:
                    child_names.append(f"...ç­‰{len(node['children'])}ä¸ª")
                print(f"   å­èŠ‚ç‚¹: {', '.join(child_names)}")
                
        # åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡é¡µé¢
        has_real_children = node.get('children') and len(node.get('children', [])) > 0
        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (is_specified_target or not has_real_children)
        )

        if is_target_page:
            # æ£€æŸ¥æ·±åº¦å†…å®¹çš„ç¼“å­˜
            local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

            if self.verbose:
                print(f"ğŸ” æ£€æŸ¥æ·±åº¦å†…å®¹ç¼“å­˜: {url}")
                print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

            if self._check_cache_validity(url, local_path):
                if self.verbose:
                    print(f"âœ… æ·±åº¦å†…å®¹ç¼“å­˜å‘½ä¸­: {node.get('name', '')}")
                else:
                    print(f"   âœ… ä¸»ç¼“å­˜å‘½ä¸­: {node.get('name', '')}")
                    
                # ä»ç¼“å­˜åŠ è½½æ•°æ®
                try:
                    cached_node = self._load_cached_node_data(local_path)
                    if cached_node:
                        # æ›´æ–°èŠ‚ç‚¹æ•°æ®
                        for key, value in cached_node.items():
                            if key not in ['children']:  # ä¿ç•™åŸå§‹children
                                node[key] = value
                except Exception as e:
                    if self.verbose:
                        print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            else:
                # ç¼“å­˜æœªå‘½ä¸­ï¼Œå¤„ç†å¹¶ä¿å­˜æ•°æ®
                try:
                    print(f"ğŸ¯ æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")
                    print(f"   ğŸ“ URL: {url}")
                    
                    # è·å–é¡µé¢å†…å®¹å¹¶å¤„ç†
                    soup = self.get_soup(url)
                    if soup:
                        self._process_and_save_product_page(node, soup, local_path)
                except Exception as e:
                    print(f"   âŒ å¤„ç†äº§å“é¡µé¢å¤±è´¥: {str(e)}")
                    self.stats["errors"] += 1
                    self.logger.error(f"å¤„ç†äº§å“é¡µé¢å¤±è´¥: {url} - {str(e)}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for i, child in enumerate(node['children']):
                node['children'][i] = self._deep_crawl_product_content_and_save(child, current_path, skip_troubleshooting)

        return node
        
    def _process_and_save_product_page(self, node, soup, local_path):
        """å¤„ç†äº§å“é¡µé¢å†…å®¹å¹¶ç«‹å³ä¿å­˜æ•°æ®"""
        url = node.get('url', '')
        
        # åŸºæœ¬æ•°æ®ä¿®å¤
        node = self.fix_node_data(node, soup)
        
        # åªåœ¨ç›®æ ‡äº§å“é¡µé¢æ·»åŠ è¿™äº›å­—æ®µ
        is_root_device = url == f"{self.base_url}/Device"
        if not is_root_device:
            # æ·»åŠ titleå­—æ®µ
            real_title = self.extract_real_title_from_page(soup)
            if real_title:
                node['title'] = real_title

            # æ·»åŠ instruction_urlå­—æ®µ
            node['instruction_url'] = ""

            # æ·»åŠ view_statisticså­—æ®µ
            real_stats = self.extract_real_view_statistics(soup, url)
            if real_stats:
                node['view_statistics'] = real_stats

        print(f"   ğŸ” æ­£åœ¨æœç´¢æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹...")
        guides = self.extract_guides_from_device_page(soup, url)
        guide_links = [guide["url"] for guide in guides]
        print(f"   ğŸ“– æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†troubleshooting
        should_process_troubleshooting = (
            not self._is_troubleshooting_processed_in_parent_path(url)
        )

        # ä½¿ç”¨å¹¶å‘å¤„ç†guideså’Œtroubleshooting
        print(f"   âš™ï¸  å¼€å§‹å¤„ç†è¯¦ç»†å†…å®¹...")
        
        # æ­£å¸¸å¤„ç†guideså’Œtroubleshooting
        guides_data, troubleshooting_data = self._process_content_concurrently(
            guide_links, url, not should_process_troubleshooting, soup
        )

        if guides_data:
            node['guides'] = guides_data
        if troubleshooting_data:
            node['troubleshooting'] = troubleshooting_data
            
        # å¤„ç†å®Œæˆåç«‹å³ä¿å­˜æ•°æ®
        print(f"   ğŸ’¾ ç«‹å³ä¿å­˜äº§å“é¡µé¢æ•°æ®...")
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        local_path.mkdir(parents=True, exist_ok=True)
        self._save_node_content(node, local_path)
        print(f"   âœ… äº§å“é¡µé¢æ•°æ®å·²ä¿å­˜åˆ°: {local_path}")
        
        return node
        
    def deep_crawl_product_content(self, node, skip_troubleshooting=False):
        """æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶æ­£ç¡®æ„å»ºæ•°æ®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        node_name = node.get('name', 'Unknown')
        is_specified_target = self._is_specified_target_url(url)

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        children_count = len(node.get('children', []))
        print(f"ğŸ” æ·±åº¦çˆ¬å–èŠ‚ç‚¹: {node_name} (å­èŠ‚ç‚¹: {children_count})")

        if self.verbose:
            print(f"   URL: {url}")
            print(f"   æ˜¯å¦æŒ‡å®šç›®æ ‡: {is_specified_target}")
            print(f"   æ˜¯å¦è·³è¿‡troubleshooting: {skip_troubleshooting}")
            if node.get('children'):
                child_names = [child.get('name', 'Unknown') for child in node['children'][:3]]
                if len(node['children']) > 3:
                    child_names.append(f"...ç­‰{len(node['children'])}ä¸ª")
                print(f"   å­èŠ‚ç‚¹: {', '.join(child_names)}")

        # åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡é¡µé¢ï¼Œéœ€è¦å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
        # 1. æŒ‡å®šçš„ç›®æ ‡URL - å¿…é¡»å¤„ç†
        # 2. æ²¡æœ‰å­ç±»åˆ«çš„é¡µé¢ - å¿…é¡»å¤„ç†
        # 3. æœ‰å­ç±»åˆ«ä½†å­ç±»åˆ«ä¸ºç©ºæ•°ç»„çš„é¡µé¢ - å¿…é¡»å¤„ç†
        # 4. æœ‰å­ç±»åˆ«ä¸”å­ç±»åˆ«éç©ºçš„é¡µé¢ - ä¸åº”è¯¥è¢«å½“ä½œç›®æ ‡é¡µé¢ï¼Œåº”è¯¥é€’å½’å¤„ç†å­èŠ‚ç‚¹
        has_real_children = node.get('children') and len(node.get('children', [])) > 0

        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (is_specified_target or not has_real_children)
        )

        if is_target_page:
            # æ£€æŸ¥æ·±åº¦å†…å®¹çš„ç¼“å­˜
            local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

            if self.verbose:
                print(f"ğŸ” æ£€æŸ¥æ·±åº¦å†…å®¹ç¼“å­˜: {url}")
                print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

            if self._check_cache_validity(url, local_path):
                if self.verbose:
                    print(f"âœ… æ·±åº¦å†…å®¹ç¼“å­˜å‘½ä¸­ï¼Œæ£€æŸ¥troubleshootingç¼“å­˜: {node.get('name', '')}")
                else:
                    print(f"   âœ… ä¸»ç¼“å­˜å‘½ä¸­ï¼Œæ£€æŸ¥troubleshootingç¼“å­˜: {node.get('name', '')}")

                # ä»ç¼“å­˜åŠ è½½æ•°æ®
                try:
                    cached_node = self._load_cached_node_data(local_path)
                    if cached_node:
                        # å³ä½¿ä¸»ç¼“å­˜å‘½ä¸­ï¼Œä¹Ÿè¦æ£€æŸ¥troubleshootingç¼“å­˜
                        troubleshooting_cache_key = f"{url}#troubleshooting"
                        cached_troubleshooting = self._check_troubleshooting_cache(troubleshooting_cache_key, url)

                        if cached_troubleshooting is not None:
                            print(f"    âœ… ä½¿ç”¨ç¼“å­˜çš„æ•…éšœæ’é™¤æ•°æ® ({len(cached_troubleshooting)} ä¸ª)")
                            cached_node['troubleshooting'] = cached_troubleshooting
                            return cached_node
                        else:
                            print(f"    ğŸ” æœªæ‰¾åˆ°æ•…éšœæ’é™¤ç¼“å­˜ï¼Œæ£€æŸ¥ä¸»ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰æ•°æ®")
                            # æ£€æŸ¥ä¸»ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰troubleshootingæ•°æ®
                            if 'troubleshooting' in cached_node and cached_node['troubleshooting']:
                                print(f"    âœ… ä¸»ç¼“å­˜ä¸­å·²æœ‰æ•…éšœæ’é™¤æ•°æ® ({len(cached_node['troubleshooting'])} ä¸ª)")
                                return cached_node

                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰troubleshootingå†…å®¹ä½†ç¼“å­˜ç¼ºå¤±
                            should_have_troubleshooting = self._should_have_troubleshooting_content(url)
                            if should_have_troubleshooting:
                                print(f"    âš ï¸ é¡µé¢åº”è¯¥æœ‰æ•…éšœæ’é™¤å†…å®¹ä½†ç¼“å­˜ç¼ºå¤±ï¼Œä»…é‡æ–°çˆ¬å–æ•…éšœæ’é™¤éƒ¨åˆ†")
                                # åªé‡æ–°çˆ¬å–troubleshootingéƒ¨åˆ†ï¼Œä¸é‡æ–°å¤„ç†æ•´ä¸ªé¡µé¢
                                try:
                                    soup = self.get_soup(url)
                                    if soup:
                                        troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, url)
                                        if troubleshooting_links:
                                            print(f"    ğŸ”§ æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢ï¼Œå¼€å§‹å¤„ç†...")
                                            troubleshooting_data = []
                                            for ts_link in troubleshooting_links:
                                                if isinstance(ts_link, dict):
                                                    ts_url = ts_link.get('url', '')
                                                else:
                                                    ts_url = ts_link
                                                if ts_url:
                                                    ts_content = self.extract_troubleshooting_content(ts_url)
                                                    if ts_content:
                                                        ts_content['url'] = ts_url
                                                        troubleshooting_data.append(ts_content)

                                            if troubleshooting_data:
                                                cached_node['troubleshooting'] = troubleshooting_data
                                                # ä¸åœ¨è¿™é‡Œä¿å­˜troubleshootingç¼“å­˜ï¼Œç­‰å¾…æ–‡ä»¶ç³»ç»Ÿä¿å­˜é˜¶æ®µ
                                                # troubleshooting_cache_key = f"{url}#troubleshooting"
                                                # self._save_troubleshooting_cache(troubleshooting_cache_key, url, troubleshooting_data)
                                                print(f"    âœ… æˆåŠŸè¡¥å……æ•…éšœæ’é™¤æ•°æ® ({len(troubleshooting_data)} ä¸ª)")
                                    return cached_node
                                except Exception as e:
                                    print(f"    âŒ é‡æ–°çˆ¬å–æ•…éšœæ’é™¤å¤±è´¥: {e}")
                                    return cached_node
                            else:
                                print(f"    âœ… é¡µé¢ä¸éœ€è¦æ•…éšœæ’é™¤å†…å®¹ï¼Œä½¿ç”¨ä¸»ç¼“å­˜")
                                return cached_node
                except Exception as e:
                    if self.verbose:
                        print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡æ–°å¤„ç†: {e}")

            if self.verbose:
                print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹æ·±åº¦å¤„ç†: {node.get('name', '')}")
            print(f"ğŸ¯ æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")
            print(f"   ğŸ“ URL: {url}")

            try:
                print(f"   ğŸŒ æ­£åœ¨è·å–é¡µé¢å†…å®¹...")
                soup = self.get_soup(url)
                if soup:
                    # åŸºæœ¬æ•°æ®ä¿®å¤
                    node = self.fix_node_data(node, soup)

                    # åªåœ¨ç›®æ ‡äº§å“é¡µé¢æ·»åŠ è¿™äº›å­—æ®µ
                    is_root_device = url == f"{self.base_url}/Device"
                    if not is_root_device:
                        # æ·»åŠ titleå­—æ®µ
                        real_title = self.extract_real_title_from_page(soup)
                        if real_title:
                            node['title'] = real_title

                        # æ·»åŠ instruction_urlå­—æ®µ
                        node['instruction_url'] = ""

                        # æ·»åŠ view_statisticså­—æ®µ
                        real_stats = self.extract_real_view_statistics(soup, url)
                        if real_stats:
                            node['view_statistics'] = real_stats

                    print(f"   ğŸ” æ­£åœ¨æœç´¢æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹...")
                    guides = self.extract_guides_from_device_page(soup, url)
                    guide_links = [guide["url"] for guide in guides]
                    print(f"   ğŸ“– æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†troubleshootingï¼ˆåŸºäºæ–°çš„é€»è¾‘ï¼‰
                    should_process_troubleshooting = (
                        not skip_troubleshooting and
                        not self._is_troubleshooting_processed_in_parent_path(url)
                    )

                    # åˆå§‹åŒ–å˜é‡
                    current_path = url.split('/Device/')[-1].split('?')[0].rstrip('/') if '/Device/' in url else ''
                    cached_troubleshooting = None

                    # å¦‚æœåº”è¯¥å¤„ç†troubleshootingï¼Œå…ˆæ£€æŸ¥ç¼“å­˜
                    if should_process_troubleshooting and current_path:
                        # é¦–å…ˆæ£€æŸ¥troubleshootingç¼“å­˜
                        troubleshooting_cache_key = f"{url}#troubleshooting"
                        cached_troubleshooting = self._check_troubleshooting_cache(troubleshooting_cache_key, url)

                        if cached_troubleshooting is not None:
                            # å¦‚æœæœ‰ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œä¸éœ€è¦é‡æ–°å¤„ç†
                            print(f"   âœ… å‘ç°troubleshootingç¼“å­˜ ({len(cached_troubleshooting)} ä¸ª)ï¼Œè·³è¿‡é‡æ–°å¤„ç†")
                            should_process_troubleshooting = False
                            # æ ‡è®°å½“å‰è·¯å¾„å·²å¤„ç†troubleshooting
                            self.processed_troubleshooting_paths.add(current_path)
                        else:
                            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œæ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦æœ‰troubleshootingå†…å®¹
                            troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, url)
                            if troubleshooting_links:
                                # æ ‡è®°å½“å‰è·¯å¾„å·²å¤„ç†troubleshooting
                                self.processed_troubleshooting_paths.add(current_path)
                                print(f"   âœ… åœ¨è·¯å¾„ '{current_path}' é¦–æ¬¡å‘ç°troubleshootingï¼Œå¼€å§‹å¤„ç†...")
                                print(f"   ğŸ”§ åç»­å­è·¯å¾„å°†è·³è¿‡troubleshootingå¤„ç†")
                            else:
                                should_process_troubleshooting = False

                    # ä½¿ç”¨å¹¶å‘å¤„ç†guideså’Œtroubleshooting
                    print(f"   âš™ï¸  å¼€å§‹å¤„ç†è¯¦ç»†å†…å®¹...")

                    # å¦‚æœæœ‰ç¼“å­˜çš„troubleshootingæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                    if cached_troubleshooting is not None:
                        print(f"   ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„troubleshootingæ•°æ® ({len(cached_troubleshooting)} ä¸ª)")
                        # åªå¤„ç†guidesï¼Œtroubleshootingä½¿ç”¨ç¼“å­˜
                        guides_data, _ = self._process_content_concurrently(
                            guide_links, url, True, soup  # skip_troubleshooting=True
                        )
                        troubleshooting_data = cached_troubleshooting
                    else:
                        # æ­£å¸¸å¤„ç†guideså’Œtroubleshooting
                        guides_data, troubleshooting_data = self._process_content_concurrently(
                            guide_links, url, not should_process_troubleshooting, soup
                        )

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                    # å¤„ç†å­ç±»åˆ« - æ— è®ºæ˜¯å¦ä¸ºæŒ‡å®šç›®æ ‡éƒ½è¦ä¿æŒå±‚çº§ç»“æ„
                    if is_specified_target:
                        # å¦‚æœæ˜¯æŒ‡å®šç›®æ ‡ï¼Œå°è¯•å‘ç°æ–°çš„å­ç±»åˆ«
                        subcategories = self.discover_subcategories_from_page(soup, url)
                        if subcategories:
                            print(f"  å‘ç° {len(subcategories)} ä¸ªå­ç±»åˆ«ï¼Œå¼€å§‹å¤„ç†...")

                            subcategory_children = []
                            for subcat in subcategories:
                                print(f"    å¤„ç†å­ç±»åˆ«: {subcat['name']}")

                                subcat_node = {
                                    'name': subcat['name'],
                                    'url': subcat['url'],
                                    'title': subcat['title']
                                }

                                # æ£€æŸ¥å­ç±»åˆ«æ˜¯å¦åº”è¯¥è·³è¿‡troubleshootingï¼ˆåŸºäºçˆ¶è·¯å¾„æ˜¯å¦å·²å¤„ç†ï¼‰
                                parent_processed_ts = current_path in self.processed_troubleshooting_paths if current_path else False
                                should_skip_ts = self._should_skip_troubleshooting_for_subcategory(subcat_node, parent_processed_ts)
                                processed_subcat = self.deep_crawl_product_content(subcat_node, skip_troubleshooting=should_skip_ts)
                                if processed_subcat:
                                    subcategory_children.append(processed_subcat)

                            if subcategory_children:
                                node['children'] = subcategory_children
                                print(f"  æˆåŠŸå¤„ç† {len(subcategory_children)} ä¸ªå­ç±»åˆ«")

                    # ä¿æŒç°æœ‰çš„childrenç»“æ„ï¼Œä¸è¦åˆ é™¤å±‚çº§å…³ç³»

                    print(f"  æ€»è®¡: {len(guides_data)} ä¸ªæŒ‡å—, {len(troubleshooting_data)} ä¸ªæ•…éšœæ’é™¤")

            except Exception as e:
                print(f"  âœ— æ·±å…¥çˆ¬å–æ—¶å‡ºé”™: {str(e)}")

            # å³ä½¿æ˜¯ç›®æ ‡é¡µé¢ï¼Œä¹Ÿè¦å¤„ç†å…¶å­èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
            # è¿™ç¡®ä¿äº†å³ä½¿æ˜¯æœ‰å†…å®¹çš„é¡µé¢ï¼Œå…¶å­ç±»åˆ«ä¹Ÿä¼šè¢«é€’å½’å¤„ç†
            if 'children' in node and node['children'] and len(node['children']) > 0:
                children_count = len(node['children'])
                print(f"ğŸ”„ é€’å½’å¤„ç†ç›®æ ‡é¡µé¢çš„ {children_count} ä¸ªå­èŠ‚ç‚¹...")

                # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å·²å¤„ç†troubleshooting
                current_url = node.get('url', '')
                current_path = current_url.split('/Device/')[-1].split('?')[0].rstrip('/') if '/Device/' in current_url else ''
                child_should_skip_troubleshooting = (
                    skip_troubleshooting or
                    (current_path and current_path in self.processed_troubleshooting_paths)
                )

                for i, child in enumerate(node['children']):
                    if children_count > 1:
                        print(f"   â””â”€ [{i+1}/{children_count}] å¤„ç†ç›®æ ‡é¡µé¢çš„å­èŠ‚ç‚¹: {child.get('name', 'Unknown')}")
                    # ç¡®ä¿ä¼ é€’å¸ƒå°”å€¼å‚æ•°
                    skip_ts = True if child_should_skip_troubleshooting else False
                    node['children'][i] = self.deep_crawl_product_content(child, skip_ts)

        elif 'children' in node and node['children']:
            children_count = len(node['children'])
            if children_count > 0:
                print(f"ğŸ”„ é€’å½’å¤„ç† {children_count} ä¸ªå­èŠ‚ç‚¹...")

            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦å·²å¤„ç†troubleshooting
            current_url = node.get('url', '')
            current_path = current_url.split('/Device/')[-1].split('?')[0].rstrip('/') if '/Device/' in current_url else ''
            child_should_skip_troubleshooting = (
                skip_troubleshooting or
                (current_path and current_path in self.processed_troubleshooting_paths)
            )

            for i, child in enumerate(node['children']):
                if children_count > 1:
                    print(f"   â””â”€ [{i+1}/{children_count}] å¤„ç†å­èŠ‚ç‚¹: {child.get('name', 'Unknown')}")
                # ç¡®ä¿ä¼ é€’å¸ƒå°”å€¼å‚æ•°
                skip_ts = True if child_should_skip_troubleshooting else False
                node['children'][i] = self.deep_crawl_product_content(child, skip_ts)

        return node

    def _check_target_completeness(self, target_name):
        """æ£€æŸ¥ç›®æ ‡äº§å“ç›®å½•æ˜¯å¦å·²å­˜åœ¨å®Œæ•´çš„æ–‡ä»¶ç»“æ„"""
        if not target_name:
            return False, None

        safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
        target_dir = Path(self.storage_root) / "Device" / safe_name

        if not target_dir.exists():
            return False, target_dir

        # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶ç»“æ„
        info_file = target_dir / "info.json"
        if not info_file.exists():
            return False, target_dir

        # æ£€æŸ¥æ˜¯å¦æœ‰guidesæˆ–troubleshootingç›®å½•
        guides_dir = target_dir / "guides"
        troubleshooting_dir = target_dir / "troubleshooting"

        has_content = False
        if guides_dir.exists() and list(guides_dir.glob("*.json")):
            has_content = True
        if troubleshooting_dir.exists() and list(troubleshooting_dir.glob("*.json")):
            has_content = True

        return has_content, target_dir

    def _get_target_root_dir(self, target_url):
        """ä»ç›®æ ‡URLè·å–çœŸå®çš„è®¾å¤‡è·¯å¾„ï¼Œç›´æ¥ä»ä¸»è¦è®¾å¤‡ç±»å‹å¼€å§‹"""
        if not target_url:
            return None

        # ä»URLä¸­æå–Deviceåçš„è·¯å¾„
        if '/Device/' in target_url:
            # æå–Deviceåé¢çš„è·¯å¾„éƒ¨åˆ†
            device_path = target_url.split('/Device/')[-1]
            # ç§»é™¤æŸ¥è¯¢å‚æ•°
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            # ç§»é™¤æœ«å°¾çš„æ–œæ 
            device_path = device_path.rstrip('/')

            if device_path:
                # URLè§£ç 
                import urllib.parse
                device_path = urllib.parse.unquote(device_path)

                # è§£æè·¯å¾„å±‚çº§ï¼Œç›´æ¥ä»ä¸»è¦è®¾å¤‡ç±»å‹å¼€å§‹
                path_parts = device_path.split('/')
                if len(path_parts) > 0:
                    # æ‰¾åˆ°ä¸»è¦è®¾å¤‡ç±»å‹ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼‰
                    main_category = path_parts[0]

                    # å¦‚æœæœ‰å­è·¯å¾„ï¼Œä¿ç•™å®Œæ•´çš„å±‚çº§ç»“æ„ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
                    if len(path_parts) > 1:
                        sub_path = '/'.join(path_parts[1:])
                        return Path(self.storage_root) / "Device" / main_category / sub_path
                    else:
                        return Path(self.storage_root) / "Device" / main_category
                else:
                    return Path(self.storage_root) / "Device" / device_path
            else:
                # å¦‚æœæ˜¯æ ¹Deviceé¡µé¢ï¼Œä½¿ç”¨Deviceä½œä¸ºæ ¹ç›®å½•
                return Path(self.storage_root) / "Device"

        # å¦‚æœä¸æ˜¯Device URLï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ä½œä¸ºåå¤‡ï¼Œç¡®ä¿åœ¨Deviceç›®å½•ä¸‹
        safe_name = target_url.replace("/", "_").replace("\\", "_").replace(":", "_")
        return Path(self.storage_root) / "Device" / safe_name

    def _validate_path_structure(self, url):
        """éªŒè¯å¹¶æå–çœŸå®çš„é¡µé¢è·¯å¾„ç»“æ„ - é€šç”¨åŠ¨æ€æ–¹æ³•"""
        print(f"   ğŸ” éªŒè¯è·¯å¾„ç»“æ„: {url}")

        # è·å–é¡µé¢å†…å®¹ä»¥æå–é¢åŒ…å±‘å¯¼èˆª
        soup = self.get_soup(url)
        if not soup:
            return None

        # é¦–å…ˆå°è¯•æå–é¢åŒ…å±‘å¯¼èˆª
        breadcrumbs = self._extract_real_breadcrumbs(soup)
        if breadcrumbs:
            print(f"   ğŸ“ æ‰¾åˆ°é¢åŒ…å±‘å¯¼èˆª: {' > '.join([b['name'] for b in breadcrumbs])}")
            return breadcrumbs

        # å¦‚æœæ²¡æœ‰é¢åŒ…å±‘ï¼Œé€šè¿‡åŠ¨æ€æ–¹å¼æ„å»ºè·¯å¾„ç»“æ„
        return self._build_dynamic_breadcrumbs(url)

    def _build_dynamic_breadcrumbs(self, url):
        """åŠ¨æ€æ„å»ºé¢åŒ…å±‘å¯¼èˆª - ä¸ä¾èµ–ç¡¬ç¼–ç è·¯å¾„"""
        if '/Device/' not in url:
            return None

        device_path = url.split('/Device/')[-1]
        if '?' in device_path:
            device_path = device_path.split('?')[0]
        device_path = device_path.rstrip('/')

        if not device_path:
            return [{"name": "Device", "url": self.base_url + "/Device"}]

        import urllib.parse
        device_path = urllib.parse.unquote(device_path)

        # åˆ†è§£è·¯å¾„ä¸ºå„ä¸ªéƒ¨åˆ†
        path_parts = device_path.split('/')
        breadcrumbs = [{"name": "Device", "url": self.base_url + "/Device"}]

        # åŠ¨æ€æ„å»ºæ¯ä¸€çº§çš„é¢åŒ…å±‘
        current_path = "/Device"
        for i, part in enumerate(path_parts):
            if not part:
                continue

            current_path += "/" + urllib.parse.quote(part, safe='')
            current_url = self.base_url + current_path

            # å°è¯•ä»è¯¥çº§åˆ«é¡µé¢æå–çœŸå®åç§°
            real_name = self._get_real_category_name(current_url, part)
            if real_name:
                breadcrumbs.append({
                    "name": real_name,
                    "url": current_url
                })
            else:
                # å¦‚æœæ— æ³•è·å–çœŸå®åç§°ï¼Œä½¿ç”¨å¤„ç†åçš„è·¯å¾„åç§°
                display_name = part.replace('_', ' ').replace('%22', '"')
                breadcrumbs.append({
                    "name": display_name,
                    "url": current_url
                })

        print(f"   ğŸ“ åŠ¨æ€æ„å»ºé¢åŒ…å±‘: {' > '.join([b['name'] for b in breadcrumbs])}")
        return breadcrumbs

    def _get_real_category_name(self, url, fallback_name):
        """ä»é¡µé¢è·å–çœŸå®çš„ç±»åˆ«åç§°"""
        try:
            soup = self.get_soup(url)
            if not soup:
                return None

            # å°è¯•ä»é¡µé¢æ ‡é¢˜è·å–åç§°
            title_elem = soup.find('h1')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text and title_text != "Device":
                    return title_text

            # å°è¯•ä»é¢åŒ…å±‘è·å–åç§°
            breadcrumb_elems = soup.select('[data-testid="breadcrumb"] a, .breadcrumb a')
            for elem in breadcrumb_elems:
                if elem.get('href', '').endswith(url.split(self.base_url)[-1]):
                    return elem.get_text(strip=True)

        except Exception as e:
            print(f"   âš ï¸  è·å–ç±»åˆ«åç§°å¤±è´¥: {e}")

        return None


    def _extract_real_breadcrumbs(self, soup):
        """ä»é¡µé¢æå–çœŸå®çš„é¢åŒ…å±‘å¯¼èˆª"""
        breadcrumbs = []

        # æ–¹æ³•1: æŸ¥æ‰¾æ–°ç‰ˆChakra UIé¢åŒ…å±‘å¯¼èˆª
        chakra_breadcrumb = soup.select_one("nav[aria-label='breadcrumb'].chakra-breadcrumb")
        if chakra_breadcrumb:
            breadcrumb_items = chakra_breadcrumb.select("li[itemtype='http://schema.org/ListItem']")
            if breadcrumb_items:
                print(f"   ğŸ“ æ‰¾åˆ°Chakra UIé¢åŒ…å±‘å¯¼èˆªï¼Œå…±{len(breadcrumb_items)}é¡¹")
                for item in breadcrumb_items:  # ä¸åå‘ï¼Œä¿æŒåŸå§‹é¡ºåº
                    # ä»data-nameå±æ€§è·å–åç§°
                    name = item.get("data-name")
                    if name:
                        # å°è¯•è·å–é“¾æ¥
                        link = item.select_one("a")
                        if link and link.get('href'):
                            href = link.get('href')
                            if '%22' in href:
                                href = href.replace('%22', '"')
                            full_url = self.base_url + href if href.startswith('/') else href
                            breadcrumbs.append({"name": name, "url": full_url})
                        else:
                            # å½“å‰é¡µé¢ï¼ˆæ²¡æœ‰é“¾æ¥ï¼‰
                            breadcrumbs.append({"name": name, "url": ""})
                    else:
                        # å¤‡é€‰ï¼šä»é“¾æ¥æ–‡æœ¬è·å–
                        link = item.select_one("a")
                        if link:
                            text = link.get_text(strip=True)
                            href = link.get('href')
                            if text and href:
                                if '%22' in href:
                                    href = href.replace('%22', '"')
                                full_url = self.base_url + href if href.startswith('/') else href
                                breadcrumbs.append({"name": text, "url": full_url})

                if breadcrumbs:
                    print(f"   âœ… Chakra UIé¢åŒ…å±‘æå–æˆåŠŸ: {' > '.join([b['name'] for b in breadcrumbs])}")
                    return breadcrumbs

        # æ–¹æ³•2: æŸ¥æ‰¾ä¼ ç»Ÿé¢åŒ…å±‘å¯¼èˆªå…ƒç´ 
        breadcrumb_selectors = [
            'nav[aria-label="breadcrumb"]',
            '.breadcrumb',
            '.breadcrumbs',
            '[data-testid="breadcrumb"]'
        ]

        for selector in breadcrumb_selectors:
            breadcrumb_nav = soup.select_one(selector)
            if breadcrumb_nav:
                # è·å–é¢åŒ…å±‘åˆ—è¡¨é¡¹
                breadcrumb_items = breadcrumb_nav.find_all('li')
                if breadcrumb_items:
                    print(f"   ğŸ“ æ‰¾åˆ°ä¼ ç»Ÿé¢åŒ…å±‘å¯¼èˆªï¼Œå…±{len(breadcrumb_items)}é¡¹")
                    # æå–æ‰€æœ‰é¢åŒ…å±‘é¡¹ç›®
                    temp_breadcrumbs = []
                    for item in breadcrumb_items:
                        link = item.find('a', href=True)
                        if link:
                            href = link.get('href')
                            text = link.get_text(strip=True)
                            if href and text:
                                # å¤„ç†ç‰¹æ®Šçš„URLç¼–ç 
                                if '%22' in href:
                                    href = href.replace('%22', '"')
                                full_url = self.base_url + href if href.startswith('/') else href
                                temp_breadcrumbs.append({"name": text, "url": full_url})
                        else:
                            # å½“å‰é¡µé¢ï¼ˆæ²¡æœ‰é“¾æ¥ï¼‰
                            text = item.get_text(strip=True)
                            if text and text not in [b['name'] for b in temp_breadcrumbs]:
                                temp_breadcrumbs.append({"name": text, "url": ""})

                    # iFixitçš„é¢åŒ…å±‘å¯¼èˆªæ˜¯å€’åºçš„ï¼Œéœ€è¦åå‘å¤„ç†
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åå‘ï¼šå¦‚æœæœ€åä¸€ä¸ªé¡¹ç›®åŒ…å«"è®¾å¤‡"æˆ–"Device"ï¼Œåˆ™éœ€è¦åå‘
                    if temp_breadcrumbs:
                        last_item = temp_breadcrumbs[-1]['name'].lower()
                        first_item = temp_breadcrumbs[0]['name'].lower()

                        # å¦‚æœæœ€åä¸€ä¸ªæ˜¯"è®¾å¤‡"æˆ–"device"ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªä¸æ˜¯"è®¾å¤‡"ï¼Œåˆ™åå‘
                        if ('è®¾å¤‡' in last_item or 'device' in last_item or
                            ('è®¾å¤‡' not in first_item and 'device' not in first_item)):
                            temp_breadcrumbs = list(reversed(temp_breadcrumbs))
                            print(f"   ğŸ”„ é¢åŒ…å±‘å¯¼èˆªå·²åå‘å¤„ç†")

                    breadcrumbs = temp_breadcrumbs
                    if breadcrumbs:
                        print(f"   âœ… ä¼ ç»Ÿé¢åŒ…å±‘æå–æˆåŠŸ: {' > '.join([b['name'] for b in breadcrumbs])}")
                        return breadcrumbs
                else:
                    # åå¤‡æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    links = breadcrumb_nav.find_all('a', href=True)
                    temp_breadcrumbs = []
                    for link in links:
                        href = link.get('href')
                        text = link.get_text(strip=True)
                        if href and text:
                            if '%22' in href:
                                href = href.replace('%22', '"')
                            full_url = self.base_url + href if href.startswith('/') else href
                            temp_breadcrumbs.append({"name": text, "url": full_url})

                    # æ£€æŸ¥é¡ºåº
                    if temp_breadcrumbs and temp_breadcrumbs[0]['name'].lower() != 'device':
                        temp_breadcrumbs = list(reversed(temp_breadcrumbs))

                    breadcrumbs = temp_breadcrumbs
                    if breadcrumbs:
                        print(f"   âœ… é“¾æ¥é¢åŒ…å±‘æå–æˆåŠŸ: {' > '.join([b['name'] for b in breadcrumbs])}")
                        return breadcrumbs

        return None

    # ç¬¬6087è¡Œæœ‰é‡å¤å£°æ˜çš„_clean_directory_nameæ–¹æ³•ï¼Œè¿™é‡Œåˆ é™¤é‡å¤å£°æ˜

    def _find_existing_path(self, breadcrumbs):
        """åœ¨å·²æœ‰ç›®å½•ç»“æ„ä¸­æŸ¥æ‰¾åŒ¹é…çš„è·¯å¾„"""
        if not breadcrumbs:
            return Path(self.storage_root) / "Device"

        # ä»Deviceå¼€å§‹æ„å»ºè·¯å¾„
        current_path = Path(self.storage_root) / "Device"

        # è·³è¿‡"Device"é¢åŒ…å±‘ï¼Œä»å®é™…åˆ†ç±»å¼€å§‹
        device_index = -1
        for i, crumb in enumerate(breadcrumbs):
            if crumb['name'].lower() in ['device', 'è®¾å¤‡']:
                device_index = i
                break

        if device_index >= 0:
            relevant_crumbs = breadcrumbs[device_index + 1:]
        else:
            relevant_crumbs = breadcrumbs

        # é€çº§æ£€æŸ¥å’Œåˆ›å»ºè·¯å¾„ï¼Œä½¿ç”¨çœŸå®çš„URLè·¯å¾„æ˜ å°„
        for crumb in relevant_crumbs:
            crumb_name = crumb['name']
            crumb_url = crumb.get('url', '')

            # æ ¹æ®URLç¡®å®šæ­£ç¡®çš„ç›®å½•åç§°
            if crumb_url:
                # ä»URLæå–è·¯å¾„æ®µ
                if '/Device/' in crumb_url:
                    url_segment = crumb_url.split('/Device/')[-1]
                    if url_segment:
                        # åªä½¿ç”¨URLè·¯å¾„çš„æœ€åä¸€ä¸ªæ®µä½œä¸ºç›®å½•åï¼Œé¿å…åˆ›å»ºåµŒå¥—è·¯å¾„
                        import urllib.parse
                        url_parts = url_segment.split('/')
                        last_part = url_parts[-1] if url_parts else url_segment
                        safe_name = urllib.parse.unquote(last_part)
                        safe_name = safe_name.replace("%22", '"')
                        # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
                        safe_name = self._clean_directory_name(safe_name)
                    else:
                        # åå¤‡æ–¹æ¡ˆï¼šæ¸…ç†æ˜¾ç¤ºåç§°
                        safe_name = self._clean_directory_name(crumb_name)
                else:
                    # åå¤‡æ–¹æ¡ˆï¼šæ¸…ç†æ˜¾ç¤ºåç§°
                    safe_name = self._clean_directory_name(crumb_name)
            else:
                # æ²¡æœ‰URLï¼Œæ¸…ç†æ˜¾ç¤ºåç§°
                safe_name = self._clean_directory_name(crumb_name)

            potential_path = current_path / safe_name

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒ¹é…çš„ç›®å½•
            existing_match = self._find_matching_directory(current_path, safe_name)
            if existing_match:
                current_path = existing_match
                print(f"   âœ… æ‰¾åˆ°å·²æœ‰ç›®å½•: {existing_match}")
            else:
                current_path = potential_path
                print(f"   ğŸ“ å°†åˆ›å»ºæ–°ç›®å½•: {potential_path}")

        return current_path

    def _build_full_save_path(self, target_url, base_dir):
        """æ ¹æ®ç›®æ ‡URLæ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„"""
        if not target_url or '/Device/' not in target_url:
            return base_dir

        # æå–è®¾å¤‡è·¯å¾„
        device_path = target_url.split('/Device/')[-1]
        if '?' in device_path:
            device_path = device_path.split('?')[0]
        device_path = device_path.rstrip('/')

        if not device_path:
            return base_dir

        import urllib.parse
        device_path = urllib.parse.unquote(device_path)

        # æ„å»ºå®Œæ•´è·¯å¾„
        path_parts = device_path.split('/')
        full_path = Path(self.storage_root) / "Device"

        for part in path_parts:
            if part:
                # ä½¿ç”¨ç»Ÿä¸€çš„ç›®å½•åæ¸…ç†æ–¹æ³•
                safe_part = self._clean_directory_name(part)
                full_path = full_path / safe_part

        return full_path



    def _troubleshooting_belongs_to_current_page(self, troubleshooting_list, current_url):
        """æ£€æŸ¥troubleshootingæ˜¯å¦çœŸçš„å±äºå½“å‰é¡µé¢"""
        if not troubleshooting_list or not current_url:
            return False

        # æå–å½“å‰é¡µé¢çš„è®¾å¤‡è·¯å¾„
        if '/Device/' not in current_url:
            return False

        current_device_path = current_url.split('/Device/')[-1]
        if '?' in current_device_path:
            current_device_path = current_device_path.split('?')[0]
        current_device_path = current_device_path.rstrip('/')

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å…·ä½“çš„è®¾å¤‡é¡µé¢ï¼ˆåŒ…å«å…·ä½“å‹å·ä¿¡æ¯ï¼‰
        if not self._is_specific_device_page(current_device_path):
            return False

        # å¯¹äºå…·ä½“çš„è®¾å¤‡é¡µé¢ï¼Œå¦‚æœæœ‰ troubleshooting æ•°æ®ï¼Œå°±è®¤ä¸ºå±äºå½“å‰é¡µé¢
        # è¿™æ˜¯å› ä¸ºè¿™äº›æ•°æ®æ˜¯ä»å½“å‰é¡µé¢æå–çš„ï¼Œå³ä½¿æ˜¯é€šç”¨çš„ troubleshooting
        if troubleshooting_list:
            return True

        # ä»¥ä¸‹æ˜¯åŸæ¥çš„ä¸¥æ ¼æ£€æŸ¥é€»è¾‘ï¼Œä½œä¸ºå¤‡ç”¨
        # æ£€æŸ¥troubleshootingçš„URLæ˜¯å¦ä¸å½“å‰é¡µé¢ç›¸å…³
        for ts in troubleshooting_list:
            ts_url = ts.get('url', '')
            ts_title = ts.get('title', '')

            if not ts_url and not ts_title:
                continue

            # æ–¹æ³•1: æ£€æŸ¥troubleshooting URLæ˜¯å¦åŒ…å«å½“å‰è®¾å¤‡è·¯å¾„
            if ts_url and current_device_path in ts_url:
                print(f"   âœ… Troubleshooting URLåŒ¹é…å½“å‰è®¾å¤‡è·¯å¾„")
                return True

            # æ–¹æ³•2: æ£€æŸ¥troubleshootingæ ‡é¢˜æ˜¯å¦åŒ…å«å½“å‰è®¾å¤‡ä¿¡æ¯
            if ts_title:
                # ä»å½“å‰è®¾å¤‡è·¯å¾„æå–å…³é”®ä¿¡æ¯
                device_keywords = self._extract_device_keywords(current_device_path)
                title_lower = ts_title.lower()

                # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨æ ‡é¢˜ä¸­
                matches = 0
                for keyword in device_keywords:
                    if keyword.lower() in title_lower:
                        matches += 1

                # å¦‚æœå¤§éƒ¨åˆ†å…³é”®è¯éƒ½åŒ¹é…ï¼Œè®¤ä¸ºå±äºå½“å‰é¡µé¢
                if matches >= len(device_keywords) * 0.6:  # 60%çš„å…³é”®è¯åŒ¹é…
                    print(f"   âœ… Troubleshootingæ ‡é¢˜åŒ¹é…å½“å‰è®¾å¤‡: {matches}/{len(device_keywords)} å…³é”®è¯åŒ¹é…")
                    return True

        print(f"   âŒ Troubleshootingä¸å±äºå½“å‰é¡µé¢")
        return False

    def _is_specific_device_page(self, device_path):
        """æ£€æŸ¥æ˜¯å¦æ˜¯å…·ä½“çš„è®¾å¤‡é¡µé¢ï¼ˆåŒ…å«å…·ä½“å‹å·ä¿¡æ¯ï¼‰"""
        if not device_path:
            return False

        # é€šç”¨ç±»åˆ«é¡µé¢ï¼ˆä¸æ˜¯å…·ä½“è®¾å¤‡ï¼‰
        generic_categories = [
            'Mac', 'Mac_Laptop', 'Mac_Desktop', 'Mac_Hardware',
            'iPhone', 'iPad', 'iPod', 'Apple_Watch',
            'MacBook', 'MacBook_Pro', 'MacBook_Air',
            'iMac', 'Mac_Pro', 'Mac_mini'
        ]

        # å¦‚æœè·¯å¾„å°±æ˜¯è¿™äº›é€šç”¨ç±»åˆ«ä¹‹ä¸€ï¼Œä¸æ˜¯å…·ä½“è®¾å¤‡
        if device_path in generic_categories:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„å‹å·ä¿¡æ¯
        specific_indicators = [
            # å‹å·ç¼–å·
            'A1', 'A2', 'A3',  # Appleå‹å·ç¼–å·
            'Models_', 'Model_',
            # å…·ä½“è§„æ ¼
            '13"', '15"', '17"', '21.5"', '27"',
            'Unibody', 'Retina', 'Touch_Bar',
            # å¹´ä»½
            '2020', '2021', '2022', '2023', '2024',
            # ä»£æ•°
            '1st_Gen', '2nd_Gen', '3rd_Gen', '4th_Gen', '5th_Gen',
            'Generation'
        ]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æŒ‡æ ‡
        for indicator in specific_indicators:
            if indicator in device_path:
                return True

        # æ£€æŸ¥è·¯å¾„æ·±åº¦ï¼ˆå…·ä½“è®¾å¤‡é€šå¸¸è·¯å¾„æ›´æ·±ï¼‰
        path_depth = len(device_path.split('/'))
        if path_depth >= 2:  # è‡³å°‘ä¸¤çº§æ·±åº¦ï¼Œå¦‚ MacBook_Pro/MacBook_Pro_17"
            return True

        return False

    def _extract_device_keywords(self, device_path):
        """ä»è®¾å¤‡è·¯å¾„æå–å…³é”®è¯"""
        if not device_path:
            return []

        keywords = []

        # åˆ†å‰²è·¯å¾„
        parts = device_path.replace('_', ' ').split('/')
        for part in parts:
            # è¿›ä¸€æ­¥åˆ†å‰²æ¯ä¸ªéƒ¨åˆ†
            sub_parts = part.split()
            keywords.extend(sub_parts)

        # æ¸…ç†å’Œè¿‡æ»¤å…³é”®è¯
        filtered_keywords = []
        for keyword in keywords:
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            clean_keyword = keyword.replace('"', '').replace("'", '').strip()
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯å’Œå¸¸è§è¯
            if len(clean_keyword) >= 2 and clean_keyword.lower() not in ['and', 'or', 'the', 'of', 'in']:
                filtered_keywords.append(clean_keyword)

        return filtered_keywords

    def _is_troubleshooting_processed_in_parent_path(self, current_url):
        """æ£€æŸ¥å½“å‰URLçš„çˆ¶è·¯å¾„æ˜¯å¦å·²ç»å¤„ç†è¿‡troubleshooting"""
        if not current_url or '/Device/' not in current_url:
            return False

        current_path = current_url.split('/Device/')[-1].split('?')[0].rstrip('/')

        # æ£€æŸ¥æ‰€æœ‰å·²å¤„ç†çš„è·¯å¾„ï¼Œçœ‹æ˜¯å¦æœ‰çˆ¶è·¯å¾„
        for processed_path in self.processed_troubleshooting_paths:
            if current_path.startswith(processed_path + '/') or current_path == processed_path:
                return True
        return False

    def _should_skip_troubleshooting_for_subcategory(self, subcat_node, parent_processed_troubleshooting=False):
        """åˆ¤æ–­å­ç±»åˆ«æ˜¯å¦åº”è¯¥è·³è¿‡troubleshootingæå–"""
        if not subcat_node:
            return True

        # å¦‚æœçˆ¶çº§å·²ç»å¤„ç†è¿‡troubleshootingï¼Œåˆ™è·³è¿‡
        if parent_processed_troubleshooting:
            return True

        url = subcat_node.get('url', '')
        if not url or '/Device/' not in url:
            return True

        # æ£€æŸ¥æ˜¯å¦åœ¨çˆ¶è·¯å¾„ä¸­å·²ç»å¤„ç†è¿‡
        if self._is_troubleshooting_processed_in_parent_path(url):
            device_path = url.split('/Device/')[-1].split('?')[0].rstrip('/')
            print(f"      â­ï¸  è·³è¿‡troubleshootingï¼ˆå·²åœ¨çˆ¶è·¯å¾„å¤„ç†ï¼‰: {device_path}")
            return True

        return False

    def _find_matching_directory(self, parent_path, target_name):
        """åœ¨çˆ¶ç›®å½•ä¸­æŸ¥æ‰¾åŒ¹é…çš„å­ç›®å½•"""
        if not parent_path.exists():
            return None

        target_name_lower = target_name.lower()

        for item in parent_path.iterdir():
            if item.is_dir():
                item_name_lower = item.name.lower()
                # æ£€æŸ¥å®Œå…¨åŒ¹é…æˆ–ç›¸ä¼¼åŒ¹é…
                if (item_name_lower == target_name_lower or
                    target_name_lower in item_name_lower or
                    item_name_lower in target_name_lower):
                    return item

        return None

    def _build_path_from_tree_structure(self, tree_data):
        """ä»æ ‘ç»“æ„ä¸­æ„å»ºæ­£ç¡®çš„æ–‡ä»¶å¤¹è·¯å¾„"""
        if not tree_data:
            return None

        # æ‰¾åˆ°æ‰€æœ‰åŒ…å«å†…å®¹çš„èŠ‚ç‚¹
        content_nodes = self._find_all_content_nodes_in_tree(tree_data)
        if not content_nodes:
            return None

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå†…å®¹èŠ‚ç‚¹æ¥æ„å»ºè·¯å¾„
        target_node = content_nodes[0]

        # æ„å»ºä»æ ¹åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è·¯å¾„
        path_segments = self._build_path_segments_to_node(tree_data, target_node)
        if not path_segments:
            return None

        # æ„å»ºæ–‡ä»¶ç³»ç»Ÿè·¯å¾„ï¼Œç¡®ä¿ä»Deviceå¼€å§‹
        root_dir = Path(self.storage_root) / "Device"

        # è·³è¿‡è·¯å¾„æ®µä¸­çš„"Device"ï¼Œé¿å…é‡å¤
        filtered_segments = []
        for segment in path_segments:
            if segment.lower() != "device":
                filtered_segments.append(segment)

        # æ·»åŠ è¿‡æ»¤åçš„è·¯å¾„æ®µ
        for segment in filtered_segments:
            safe_name = self._clean_directory_name(segment)
            root_dir = root_dir / safe_name

        print(f"   ğŸ—‚ï¸  ä»æ ‘ç»“æ„æ„å»ºè·¯å¾„: Device > {' > '.join(filtered_segments)}")
        print(f"   ğŸ“ æ–‡ä»¶ç³»ç»Ÿè·¯å¾„: {root_dir}")

        return root_dir

    def _build_hierarchical_path_from_tree(self, tree_data):
        """ä»æ ‘ç»“æ„ä¸­æ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„ - ä¿®å¤ç‰ˆæœ¬"""
        if not tree_data:
            return None

        # å°è¯•ä»æ ‘ç»“æ„ä¸­æå–å®Œæ•´çš„è·¯å¾„ä¿¡æ¯
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰breadcrumbæˆ–pathä¿¡æ¯
        breadcrumbs = tree_data.get('breadcrumbs', [])
        if breadcrumbs:
            # ä½¿ç”¨breadcrumbsæ„å»ºè·¯å¾„
            return self._build_path_from_breadcrumbs(breadcrumbs)

        # å¦‚æœæ²¡æœ‰breadcrumbsï¼Œå°è¯•ä»URLæ„å»º
        tree_url = tree_data.get('url', '')
        if tree_url and '/Device/' in tree_url:
            return self._build_hierarchical_path_from_url(tree_url)

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»èŠ‚ç‚¹åç§°å’Œå±‚çº§ç»“æ„æ¨æ–­
        return self._infer_hierarchical_path_from_tree(tree_data)

    def _build_hierarchical_path_from_url(self, url):
        """ä»URLæ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„"""
        if not url or '/Device/' not in url:
            return None

        try:
            # æå–è®¾å¤‡è·¯å¾„éƒ¨åˆ†
            device_path = url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            if not device_path:
                return Path(self.storage_root) / "Device"

            # URLè§£ç 
            import urllib.parse
            device_path = urllib.parse.unquote(device_path)

            # æ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„
            path_parts = device_path.split('/')
            root_dir = Path(self.storage_root) / "Device"

            for part in path_parts:
                if part:  # è·³è¿‡ç©ºéƒ¨åˆ†
                    safe_part = self._clean_directory_name(part)
                    root_dir = root_dir / safe_part

            print(f"   ğŸ—‚ï¸  ä»URLæ„å»ºå±‚çº§è·¯å¾„: {url}")
            print(f"   ğŸ“ æ„å»ºçš„è·¯å¾„: {root_dir}")
            return root_dir

        except Exception as e:
            self.logger.error(f"ä»URLæ„å»ºå±‚çº§è·¯å¾„å¤±è´¥: {e}")
            return None

    def _build_path_from_breadcrumbs(self, breadcrumbs):
        """ä»breadcrumbsæ„å»ºè·¯å¾„"""
        if not breadcrumbs:
            return None

        try:
            root_dir = Path(self.storage_root) / "Device"

            for crumb in breadcrumbs:
                if isinstance(crumb, dict):
                    name = crumb.get('name', '')
                elif isinstance(crumb, str):
                    name = crumb
                else:
                    continue

                if name and name.lower() != 'device':
                    safe_name = self._clean_directory_name(name)
                    root_dir = root_dir / safe_name

            return root_dir

        except Exception as e:
            self.logger.error(f"ä»breadcrumbsæ„å»ºè·¯å¾„å¤±è´¥: {e}")
            return None

    def _infer_hierarchical_path_from_tree(self, tree_data):
        """ä»æ ‘ç»“æ„æ¨æ–­å±‚çº§è·¯å¾„"""
        if not tree_data:
            return None

        # å°è¯•æ‰¾åˆ°åŒ…å«å†…å®¹çš„èŠ‚ç‚¹å¹¶æ„å»ºå…¶å®Œæ•´è·¯å¾„
        content_nodes = self._find_all_content_nodes_in_tree(tree_data)
        if content_nodes:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå†…å®¹èŠ‚ç‚¹
            target_node = content_nodes[0]
            path_segments = self._build_complete_path_to_node(tree_data, target_node)
            if path_segments:
                return self._build_path_from_segments(path_segments)

        # å¦‚æœæ²¡æœ‰å†…å®¹èŠ‚ç‚¹ï¼Œä½¿ç”¨æ ¹èŠ‚ç‚¹ä¿¡æ¯
        root_name = tree_data.get('name', 'Unknown')
        if root_name and root_name != 'Unknown':
            safe_name = self._clean_directory_name(root_name)
            return Path(self.storage_root) / "Device" / safe_name

        return None

    def _build_path_from_tree_data(self, tree_data):
        """ä»æ ‘æ•°æ®æ„å»ºè·¯å¾„ - åå¤‡æ–¹æ¡ˆ"""
        if not tree_data:
            return Path(self.storage_root) / "Device"

        root_name = tree_data.get("name", "Unknown")
        if " > " in root_name:
            # å¦‚æœåç§°åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼Œè§£æå®Œæ•´è·¯å¾„
            path_parts = root_name.split(" > ")
            root_dir = Path(self.storage_root) / "Device"
            for part in path_parts:
                if part and part.lower() != 'device':
                    safe_name = self._clean_directory_name(part)
                    root_dir = root_dir / safe_name
            return root_dir
        else:
            safe_name = self._clean_directory_name(root_name)
            return Path(self.storage_root) / "Device" / safe_name

    def _build_complete_path_to_node(self, tree_root, target_node, current_path=None):
        """æ„å»ºä»æ ¹èŠ‚ç‚¹åˆ°ç›®æ ‡èŠ‚ç‚¹çš„å®Œæ•´è·¯å¾„æ®µ - æ”¹è¿›ç‰ˆæœ¬"""
        if current_path is None:
            current_path = []

        if not tree_root or not isinstance(tree_root, dict):
            return None

        # æ·»åŠ å½“å‰èŠ‚ç‚¹åç§°åˆ°è·¯å¾„
        node_name = tree_root.get('name', '')
        if node_name:
            current_path.append(node_name)

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹
        if self._is_same_node(tree_root, target_node):
            return current_path.copy()

        # é€’å½’æœç´¢å­èŠ‚ç‚¹
        children = tree_root.get('children', [])
        for child in children:
            result = self._build_complete_path_to_node(child, target_node, current_path.copy())
            if result:
                return result

        return None

    def _build_path_from_segments(self, path_segments):
        """ä»è·¯å¾„æ®µæ„å»ºæ–‡ä»¶ç³»ç»Ÿè·¯å¾„"""
        if not path_segments:
            return None

        root_dir = Path(self.storage_root) / "Device"

        # è·³è¿‡è·¯å¾„æ®µä¸­çš„"Device"ï¼Œé¿å…é‡å¤
        filtered_segments = []
        for segment in path_segments:
            if segment and segment.lower() != "device":
                filtered_segments.append(segment)

        # æ·»åŠ è¿‡æ»¤åçš„è·¯å¾„æ®µ
        for segment in filtered_segments:
            safe_name = self._clean_directory_name(segment)
            root_dir = root_dir / safe_name

        return root_dir

    def _is_same_node(self, node1, node2):
        """åˆ¤æ–­ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦ç›¸åŒ"""
        if not node1 or not node2:
            return False

        # ä¼˜å…ˆä½¿ç”¨URLæ¯”è¾ƒ
        url1 = node1.get('url', '').strip()
        url2 = node2.get('url', '').strip()
        if url1 and url2:
            return url1 == url2

        # å¦‚æœæ²¡æœ‰URLï¼Œä½¿ç”¨åç§°æ¯”è¾ƒ
        name1 = node1.get('name', '').strip()
        name2 = node2.get('name', '').strip()
        if name1 and name2:
            return name1 == name2

        return False

    def _save_tree_with_hierarchical_structure(self, tree_data, base_dir, current_path=""):
        """ä¿å­˜æ ‘ç»“æ„ï¼Œç»´æŠ¤å®Œæ•´çš„å±‚çº§ç»“æ„"""
        if not tree_data or not isinstance(tree_data, dict):
            return

        node_name = tree_data.get('name', 'Unknown')
        node_url = tree_data.get('url', '')

        # æ„å»ºå½“å‰èŠ‚ç‚¹çš„è·¯å¾„
        if current_path:
            full_path = f"{current_path} > {node_name}"
        else:
            full_path = node_name

        print(f"   ğŸ” å¤„ç†èŠ‚ç‚¹: {full_path}")

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹éœ€è¦ä¿å­˜
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0
        has_content = has_guides or has_troubleshooting

        if has_content:
            # æ„å»ºå½“å‰èŠ‚ç‚¹çš„ç›®å½•è·¯å¾„
            if current_path:
                # å¦‚æœæœ‰çˆ¶è·¯å¾„ï¼Œæ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„
                path_parts = current_path.split(' > ')
                path_parts.append(node_name)
            else:
                # å¦‚æœæ˜¯æ ¹èŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨èŠ‚ç‚¹åç§°
                path_parts = [node_name]

            # è¿‡æ»¤æ‰"Device"é¿å…é‡å¤
            filtered_parts = []
            for part in path_parts:
                if part and part.lower() != "device":
                    filtered_parts.append(part)

            # æ„å»ºæ–‡ä»¶ç³»ç»Ÿè·¯å¾„
            node_dir = base_dir
            for part in filtered_parts:
                safe_name = self._clean_directory_name(part)
                node_dir = node_dir / safe_name

            print(f"       ğŸ“ ä¿å­˜å†…å®¹åˆ°: {node_dir}")
            print(f"       ğŸ“– æŒ‡å—: {len(tree_data.get('guides', []))} ä¸ª")
            print(f"       ğŸ”§ æ•…éšœæ’é™¤: {len(tree_data.get('troubleshooting', []))} ä¸ª")

            # ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶ä¿å­˜å†…å®¹
            node_dir.mkdir(parents=True, exist_ok=True)
            self._save_node_content(tree_data, node_dir)

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹ï¼Œä¿æŒå±‚çº§ç»“æ„
        children = tree_data.get('children', [])
        if children:
            print(f"       ğŸŒ³ å¤„ç† {len(children)} ä¸ªå­èŠ‚ç‚¹...")
            for child in children:
                # ä¸ºå­èŠ‚ç‚¹æ„å»ºæ–°çš„è·¯å¾„ä¸Šä¸‹æ–‡
                if current_path:
                    child_path = f"{current_path} > {node_name}"
                else:
                    child_path = node_name

                self._save_tree_with_hierarchical_structure(child, base_dir, child_path)

    def _find_target_node_in_tree(self, node):
        """åœ¨æ ‘ä¸­æŸ¥æ‰¾åŒ…å«æŒ‡å—å’Œæ•…éšœæ’é™¤çš„ç›®æ ‡èŠ‚ç‚¹"""
        if not node or not isinstance(node, dict):
            return None

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦åŒ…å«æŒ‡å—æˆ–æ•…éšœæ’é™¤
        if (node.get('guides') and len(node.get('guides', [])) > 0) or \
           (node.get('troubleshooting') and len(node.get('troubleshooting', [])) > 0):
            return node

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        children = node.get('children', [])
        for child in children:
            result = self._find_target_node_in_tree(child)
            if result:
                return result

        return None

    def _build_path_segments_to_node(self, tree_root, target_node, current_path=None):
        """æ„å»ºä»æ ¹èŠ‚ç‚¹åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è·¯å¾„æ®µ"""
        if current_path is None:
            current_path = []

        if not tree_root or not isinstance(tree_root, dict):
            return None

        # æ·»åŠ å½“å‰èŠ‚ç‚¹åç§°åˆ°è·¯å¾„
        node_name = tree_root.get('name', '')
        if node_name:
            current_path.append(node_name)

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹ - ä½¿ç”¨å¤šç§åŒ¹é…æ–¹å¼
        target_url = target_node.get('url', '').strip()
        current_url = tree_root.get('url', '').strip()

        # æ–¹å¼1: ç²¾ç¡®URLåŒ¹é…
        if target_url and current_url and target_url == current_url:
            return current_path.copy()

        # æ–¹å¼2: URLè·¯å¾„åŒ¹é…ï¼ˆå¿½ç•¥åè®®å’ŒåŸŸåï¼‰
        if target_url and current_url:
            target_path = target_url.split('/')[-1] if '/' in target_url else target_url
            current_path_part = current_url.split('/')[-1] if '/' in current_url else current_url
            if target_path and current_path_part and target_path == current_path_part:
                return current_path.copy()

        # æ–¹å¼3: åç§°åŒ¹é…ï¼ˆä½œä¸ºåå¤‡ï¼‰
        target_name = target_node.get('name', '').strip()
        if target_name and node_name and target_name == node_name:
            return current_path.copy()

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        children = tree_root.get('children', [])
        for child in children:
            result = self._build_path_segments_to_node(child, target_node, current_path.copy())
            if result:
                return result

        return None

    def _build_fallback_path_from_url(self, node_url, node_name):
        """ä»èŠ‚ç‚¹URLæ„å»ºåå¤‡è·¯å¾„ï¼Œå°è¯•ä¿æŒå±‚çº§ç»“æ„"""
        if not node_url:
            return None

        try:
            # ä»URLä¸­æå–è·¯å¾„ä¿¡æ¯
            if '/Device/' in node_url:
                # æå–Deviceåé¢çš„è·¯å¾„éƒ¨åˆ†
                device_path = node_url.split('/Device/')[-1]
                if '?' in device_path:
                    device_path = device_path.split('?')[0]
                device_path = device_path.rstrip('/')

                if device_path:
                    import urllib.parse
                    device_path = urllib.parse.unquote(device_path)

                    # åŠ¨æ€æ„å»ºè·¯å¾„ï¼Œä¸ä½¿ç”¨ç¡¬ç¼–ç 
                    path_parts = device_path.split('/')
                    base_path = Path(self.storage_root) / "Device"

                    # é€çº§æ„å»ºè·¯å¾„ï¼Œä¿æŒåŸæœ‰çš„å±‚çº§ç»“æ„
                    for part in path_parts:
                        if part:  # è·³è¿‡ç©ºçš„è·¯å¾„æ®µ
                            safe_part = self._clean_directory_name(part)
                            base_path = base_path / safe_part

                    # å¦‚æœnode_nameä¸æœ€åä¸€ä¸ªè·¯å¾„æ®µä¸åŒï¼Œæ·»åŠ node_name
                    if node_name and node_name != path_parts[-1]:
                        safe_name = self._clean_directory_name(node_name)
                        return base_path / safe_name
                    else:
                        return base_path

            return None
        except Exception as e:
            self.logger.warning(f"æ„å»ºåå¤‡è·¯å¾„å¤±è´¥: {e}")
            return None

    def save_combined_result(self, tree_data, target_name=None):
        """ä¿å­˜æ•´åˆç»“æœåˆ°çœŸå®çš„è®¾å¤‡è·¯å¾„ç»“æ„ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            if self.verbose:
                print("   ğŸ“ åˆ›å»ºç›®å½•ç»“æ„...")

            # æ–°çš„è·¯å¾„æ„å»ºç­–ç•¥ï¼šç¡®ä¿ç»´æŠ¤å®Œæ•´çš„å±‚çº§ç»“æ„
            root_dir = self._build_hierarchical_path_from_tree(tree_data)

            # å¦‚æœæ²¡æœ‰ä»æ ‘ç»“æ„æ„å»ºè·¯å¾„ï¼Œä½¿ç”¨æ”¹è¿›çš„åå¤‡æ–¹æ¡ˆ
            if not root_dir:
                target_url = getattr(self, 'target_url', None)
                if target_url and '/Device/' in target_url:
                    # ä»URLæ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„
                    root_dir = self._build_hierarchical_path_from_url(target_url)
                elif target_name:
                    if target_name.startswith('http') and '/Device/' in target_name:
                        root_dir = self._build_hierarchical_path_from_url(target_name)
                    else:
                        # å‡è®¾æ˜¯è®¾å¤‡åç§°ï¼Œä½†ä»ç„¶å°è¯•æ„å»ºå±‚çº§ç»“æ„
                        safe_name = self._clean_directory_name(target_name)
                        root_dir = Path(self.storage_root) / "Device" / safe_name
                else:
                    # ä»æ ‘æ•°æ®ä¸­æå–è·¯å¾„ä¿¡æ¯ï¼Œä¿æŒå±‚çº§ç»“æ„
                    root_dir = self._build_path_from_tree_data(tree_data)

            # ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶ä¿å­˜å†…å®¹
            if root_dir:
                root_dir.mkdir(parents=True, exist_ok=True)
                if self.verbose:
                    print(f"   ğŸ“‚ ç›®æ ‡è·¯å¾„: {root_dir}")
                    print("   ğŸ’¾ ä¿å­˜å†…å®¹å’Œä¸‹è½½åª’ä½“æ–‡ä»¶...")
                    print(f"   ğŸ” å¼€å§‹ä¿å­˜åˆ°: {root_dir}")
                    print(f"   ğŸ¯ ç›®æ ‡URL: {getattr(self, 'target_url', 'None')}")
                    print("   âœ… æ•°æ®å·²åœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¿å­˜")
            else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹èŠ‚ç‚¹ï¼Œå°è¯•æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹å¹¶åªä¿å­˜å…¶å­ç»“æ„
                    target_node = self._find_target_node_by_url(tree_data, getattr(self, 'target_url', ''))
                    if target_node:
                        print(f"   ğŸ¯ æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹: {target_node.get('name', 'Unknown')}")
                        # æ„å»ºç›®æ ‡èŠ‚ç‚¹çš„å®Œæ•´è·¯å¾„
                        target_path_segments = self._build_path_segments_to_node(tree_data, target_node)
                        if target_path_segments:
                            # æ„å»ºå®Œæ•´çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ï¼Œç¡®ä¿ä»Deviceå¼€å§‹
                            target_dir = Path(self.storage_root) / "Device"

                            # è·³è¿‡è·¯å¾„æ®µä¸­çš„"Device"ï¼Œé¿å…é‡å¤
                            filtered_segments = []
                            for segment in target_path_segments:
                                if segment.lower() != "device":
                                    filtered_segments.append(segment)

                            # æ·»åŠ è¿‡æ»¤åçš„è·¯å¾„æ®µ
                            for segment in filtered_segments:
                                safe_name = self._clean_directory_name(segment)
                                target_dir = target_dir / safe_name

                            print(f"   ğŸ“ ç›®æ ‡èŠ‚ç‚¹å®Œæ•´è·¯å¾„: Device > {' > '.join(filtered_segments)}")
                            # åªä¿å­˜ç›®æ ‡èŠ‚ç‚¹çš„å­ç»“æ„ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ ‘
                            self._save_target_node_children(target_node, target_dir)
                        else:
                            # å¦‚æœæ— æ³•æ„å»ºè·¯å¾„ï¼Œä½¿ç”¨åŸæ¥çš„root_dir
                            self._save_target_node_children(target_node, root_dir)
                    else:
                        # æœ€ç»ˆåå¤‡æ–¹æ¡ˆï¼šä¿å­˜æ•´ä¸ªæ ‘ç»“æ„
                        print("   âš ï¸  æœªæ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹ï¼Œä½¿ç”¨æ•´ä¸ªæ ‘ç»“æ„")
                        self._save_tree_structure(tree_data, root_dir)

            # æ•°æ®å®Œæ•´æ€§éªŒè¯å·²ç¦ç”¨ï¼ˆéªŒè¯é€»è¾‘æœ‰bugï¼Œä½†æ•°æ®æœ¬èº«æ˜¯æ­£ç¡®çš„ï¼‰
            if self.verbose:
                print(f"\nâœ… æ•°æ®ä¿å­˜å®Œæˆï¼Œè·³è¿‡éªŒè¯æ­¥éª¤")

            print(f"\nğŸ“ æ•´åˆç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {root_dir}")
            return str(root_dir)
        except Exception as e:
            print(f"\nâŒ ä¿å­˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("âš ï¸ å°è¯•ä½¿ç”¨å¤‡ç”¨ä¿å­˜æ–¹æ¡ˆ...")
            try:
                # å¤‡ç”¨ä¿å­˜æ–¹æ¡ˆï¼šä¿å­˜åˆ°ç®€å•çš„ç›®å½•ç»“æ„
                backup_dir = Path(self.storage_root) / "backup_data" / datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_dir.mkdir(parents=True, exist_ok=True)

                # ä¿å­˜åŸºæœ¬ä¿¡æ¯
                info_file = backup_dir / "info.json"
                with open(info_file, 'w', encoding='utf-8') as f:
                    safe_json_dump(tree_data, f, ensure_ascii=False, indent=2)

                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°å¤‡ç”¨ä½ç½®: {backup_dir}")
                return str(backup_dir)
            except Exception as backup_error:
                print(f"âŒ å¤‡ç”¨ä¿å­˜æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(backup_error)}")
                return None

    def _save_tree_structure(self, tree_data, base_dir, current_path_parts=None):
        """ä¿å­˜æ•´ä¸ªæ ‘ç»“æ„åˆ°æŒ‡å®šç›®å½•ï¼Œä¿®å¤è·¯å¾„é‡å¤é—®é¢˜"""
        if not tree_data or not isinstance(tree_data, dict):
            print(f"   âš ï¸  æ— æ•ˆçš„æ ‘æ•°æ®: {type(tree_data)}")
            return

        # åˆå§‹åŒ–è·¯å¾„éƒ¨åˆ†åˆ—è¡¨
        if current_path_parts is None:
            current_path_parts = []

        node_name = tree_data.get('name', 'Unknown')
        current_url = tree_data.get('url', '')

        print(f"   ğŸ” å¤„ç†èŠ‚ç‚¹: {node_name}")
        print(f"      èŠ‚ç‚¹URL: {current_url}")
        node_type = 'äº§å“' if tree_data.get('guides') or tree_data.get('troubleshooting') else 'åˆ†ç±»'
        print(f"      æŒ‡å—æ•°é‡: {len(tree_data.get('guides', []))}")
        print(f"      æ•…éšœæ’é™¤æ•°é‡: {len(tree_data.get('troubleshooting', []))}")

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å®é™…å†…å®¹éœ€è¦ä¿å­˜
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0
        has_content = has_guides or has_troubleshooting

        if has_content:
            print(f"   âœ… å‘ç°å†…å®¹èŠ‚ç‚¹: {node_name}")
            print(f"      æŒ‡å—æ•°é‡: {len(tree_data.get('guides', []))}")
            print(f"      æ•…éšœæ’é™¤æ•°é‡: {len(tree_data.get('troubleshooting', []))}")
            print(f"      ä¿å­˜è·¯å¾„: {base_dir}")

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            base_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜èŠ‚ç‚¹å†…å®¹åˆ°å½“å‰ç›®å½•ï¼Œä¸å†åˆ›å»ºé‡å¤çš„å­ç›®å½•
            print(f"   ğŸ’¾ å¼€å§‹ä¿å­˜èŠ‚ç‚¹å†…å®¹...")
            self._save_node_content(tree_data, base_dir)
            print(f"   âœ… èŠ‚ç‚¹å†…å®¹ä¿å­˜å®Œæˆ")

        # å¤„ç†å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            print(f"   ğŸŒ³ å¤„ç† {len(tree_data['children'])} ä¸ªå­èŠ‚ç‚¹...")
            for child in tree_data['children']:
                child_name = child.get('name', 'Unknown')

                # æ¸…ç†å­èŠ‚ç‚¹åç§°ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦
                safe_name = self._clean_directory_name(child_name)
                child_dir = base_dir / safe_name

                # æ£€æŸ¥å­èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
                child_has_content = (child.get('guides') and len(child.get('guides', [])) > 0) or \
                                  (child.get('troubleshooting') and len(child.get('troubleshooting', [])) > 0)

                # æ£€æŸ¥å­èŠ‚ç‚¹æ˜¯å¦æœ‰çœŸæ­£çš„å­èŠ‚ç‚¹ï¼ˆéç©ºçš„childrenæ•°ç»„ï¼‰
                child_has_children = child.get('children') and len(child.get('children', [])) > 0

                # ä¸ºæ‰€æœ‰å­èŠ‚ç‚¹åˆ›å»ºç›®å½•ï¼ŒåŒ…æ‹¬å¶å­èŠ‚ç‚¹ï¼ˆå³ä½¿æ²¡æœ‰å†…å®¹ï¼‰
                # è¿™ç¡®ä¿äº†æ ‘ç»“æ„çš„å®Œæ•´æ€§ï¼Œå³ä½¿æŸäº›å¶å­èŠ‚ç‚¹æš‚æ—¶æ²¡æœ‰guidesæˆ–troubleshooting
                print(f"      å­èŠ‚ç‚¹ '{child_name}': å†…å®¹={child_has_content}, å­èŠ‚ç‚¹={child_has_children}, URL={bool(child.get('url'))}")

                if child_has_content or child_has_children or child.get('url'):
                    try:
                        if not child_dir.exists():
                            child_dir.mkdir(parents=True, exist_ok=True)
                            print(f"   ğŸ“ åˆ›å»ºå­èŠ‚ç‚¹ç›®å½•: {child_dir}")
                            # è®°å½•ç›®å½•åç§°å¤„ç†è¿‡ç¨‹ï¼Œå¸®åŠ©è°ƒè¯•
                            print(f"      åŸå§‹åç§°: '{child_name}'")
                            print(f"      æ¸…ç†ååç§°: '{safe_name}'")
                        else:
                            print(f"   ğŸ“ ä½¿ç”¨å·²å­˜åœ¨çš„å­èŠ‚ç‚¹ç›®å½•: {child_dir}")

                        # é€’å½’å¤„ç†å­èŠ‚ç‚¹ï¼Œä½¿ç”¨å­èŠ‚ç‚¹çš„ç›®å½•
                        self._save_tree_structure(child, child_dir, current_path_parts + [node_name])
                    except Exception as e:
                        # å¢å¼ºé”™è¯¯å¤„ç†ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯ä½†ä¸ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
                        print(f"   âŒ åˆ›å»ºæˆ–å¤„ç†ç›®å½•å¤±è´¥: {child_dir}")
                        print(f"      é”™è¯¯ä¿¡æ¯: {str(e)}")
                        print(f"      å­èŠ‚ç‚¹åç§°: '{child_name}'")
                        print(f"      æ¸…ç†ååç§°: '{safe_name}'")
                        # å°è¯•ä½¿ç”¨å¤‡ç”¨åç§°
                        try:
                            backup_name = f"Category_{hash(child_name) % 10000:04d}"
                            backup_dir = base_dir / backup_name
                            print(f"      å°è¯•ä½¿ç”¨å¤‡ç”¨åç§°: '{backup_name}'")
                            backup_dir.mkdir(parents=True, exist_ok=True)
                            self._save_tree_structure(child, backup_dir, current_path_parts + [node_name])
                        except Exception as backup_error:
                            print(f"      å¤‡ç”¨åç§°ä¹Ÿå¤±è´¥: {str(backup_error)}")
                else:
                    # åªæœ‰å½“å­èŠ‚ç‚¹å®Œå…¨æ²¡æœ‰ä»»ä½•æœ‰ç”¨ä¿¡æ¯æ—¶æ‰è·³è¿‡
                    print(f"   â­ï¸  è·³è¿‡æ— æ•ˆå­èŠ‚ç‚¹: {child_name} (æ— å†…å®¹ã€æ— å­èŠ‚ç‚¹ã€æ— URL)")

    def _clean_directory_name(self, name):
        """æ¸…ç†ç›®å½•åç§°ï¼Œç§»é™¤æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦"""
        if not name:
            return "Unknown"

        # æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
        safe_name = name.replace('/', '_').replace('\\', '_')
        safe_name = safe_name.replace(':', '_').replace('*', '_')
        safe_name = safe_name.replace('?', '_').replace('<', '_')
        safe_name = safe_name.replace('>', '_').replace('|', '_')
        
        # ä¸æ›¿æ¢å¼•å·ï¼Œä¿æŒåŸæ ·ï¼ˆmacOSå’ŒLinuxæ”¯æŒå¼•å·åœ¨æ–‡ä»¶åä¸­ï¼‰
        # safe_name = safe_name.replace('"', '_').replace("'", "_")
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼ï¼Œä½†ä¿æŒå…¶ä»–å­—ç¬¦
        safe_name = safe_name.strip()
        
        # å¤„ç†ç©ºåç§°
        if not safe_name:
            return "Unknown"
            
        # é™åˆ¶é•¿åº¦ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿé—®é¢˜
        if len(safe_name) > 100:
            safe_name = safe_name[:97] + "..."
            
        return safe_name

    def _validate_data_completeness(self, tree_data, root_dir):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œæ£€æŸ¥æ‰€æœ‰ç±»åˆ«æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„ç›®å½•"""
        missing_categories = []
        total_categories = 0
        existing_directories = 0

        def check_node(node, current_path=""):
            nonlocal missing_categories, total_categories, existing_directories

            if not node or not isinstance(node, dict):
                return

            node_name = node.get('name', 'Unknown')
            node_url = node.get('url', '')

            # æ£€æŸ¥æ˜¯å¦æœ‰å­èŠ‚ç‚¹
            children = node.get('children', [])
            if children:
                total_categories += len(children)

                for child in children:
                    child_name = child.get('name', 'Unknown')
                    safe_name = self._clean_directory_name(child_name)

                    # æ„å»ºé¢„æœŸçš„ç›®å½•è·¯å¾„
                    if current_path:
                        expected_dir = root_dir / current_path / safe_name
                    else:
                        expected_dir = root_dir / safe_name

                    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                    if expected_dir.exists():
                        existing_directories += 1
                        print(f"   âœ… ç›®å½•å­˜åœ¨: {safe_name}")
                    else:
                        missing_categories.append({
                            'name': child_name,
                            'safe_name': safe_name,
                            'url': child.get('url', ''),
                            'expected_path': str(expected_dir),
                            'has_children': bool(child.get('children')),
                            'has_content': bool(child.get('guides') or child.get('troubleshooting'))
                        })
                        print(f"   âŒ ç›®å½•ç¼ºå¤±: {safe_name} (åŸå: {child_name})")

                    # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
                    if current_path:
                        check_node(child, f"{current_path}/{safe_name}")
                    else:
                        check_node(child, safe_name)

        print(f"   æ­£åœ¨æ£€æŸ¥æ ‘ç»“æ„å®Œæ•´æ€§...")
        check_node(tree_data)

        # è¾“å‡ºéªŒè¯ç»“æœ
        print(f"\nğŸ“Š æ•°æ®å®Œæ•´æ€§éªŒè¯ç»“æœ:")
        print(f"   æ€»ç±»åˆ«æ•°: {total_categories}")
        print(f"   å·²åˆ›å»ºç›®å½•: {existing_directories}")
        print(f"   ç¼ºå¤±ç›®å½•: {len(missing_categories)}")

        if missing_categories:
            print(f"\nâŒ å‘ç° {len(missing_categories)} ä¸ªç¼ºå¤±çš„ç±»åˆ«ç›®å½•:")
            for missing in missing_categories:
                print(f"   - ç±»åˆ«: '{missing['name']}'")
                print(f"     æ¸…ç†ååç§°: '{missing['safe_name']}'")
                print(f"     é¢„æœŸè·¯å¾„: {missing['expected_path']}")
                print(f"     æœ‰å­èŠ‚ç‚¹: {missing['has_children']}")
                print(f"     æœ‰å†…å®¹: {missing['has_content']}")
                if missing['url']:
                    print(f"     URL: {missing['url']}")
                print()
        else:
            print(f"   âœ… æ‰€æœ‰ç±»åˆ«ç›®å½•éƒ½å·²æ­£ç¡®åˆ›å»º")

        return len(missing_categories) == 0

    def _save_node_content(self, node_data, node_dir):
        """ä¿å­˜å•ä¸ªèŠ‚ç‚¹çš„å†…å®¹ï¼ˆguideså’Œtroubleshootingï¼‰"""
        if not node_data or not isinstance(node_data, dict):
            print(f"   âš ï¸  èŠ‚ç‚¹æ•°æ®æ— æ•ˆ: {type(node_data)}")
            return

        # åœ¨ä¿å­˜å‰éªŒè¯æ•°æ®å®Œæ•´æ€§
        print(f"   ğŸ” éªŒè¯èŠ‚ç‚¹æ•°æ®å®Œæ•´æ€§...")
        self._validate_node_data_before_save(node_data)

        # åˆ›å»ºèŠ‚ç‚¹ç›®å½•
        node_dir.mkdir(parents=True, exist_ok=True)
        print(f"   ğŸ“ åˆ›å»ºç›®å½•: {node_dir}")

        # ä¿å­˜èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
        info_data = {k: v for k, v in node_data.items()
                    if k not in ['children', 'guides', 'troubleshooting']}

        # ç¡®ä¿info_dataä¸ä¸ºç©ºï¼Œè‡³å°‘åŒ…å«åŸºæœ¬ä¿¡æ¯
        if not info_data:
            info_data = {
                'name': node_data.get('name', 'Unknown'),
                'url': node_data.get('url', ''),
                'saved_at': datetime.now().isoformat()
            }

        try:
            info_file = node_dir / "info.json"
            print(f"   ğŸ’¾ ä¿å­˜åŸºæœ¬ä¿¡æ¯åˆ°: {info_file}")
            print(f"   ğŸ“Š åŸºæœ¬ä¿¡æ¯å­—æ®µæ•°: {len(info_data)}")
            with open(info_file, 'w', encoding='utf-8') as f:
                safe_json_dump(info_data, f, ensure_ascii=False, indent=2)
            print(f"   âœ… åŸºæœ¬ä¿¡æ¯ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ ä¿å­˜åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")

        # å¤„ç†guides
        guides_count = 0
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            print(f"   ğŸ“– å¤„ç† {len(node_data['guides'])} ä¸ªæŒ‡å—")

            for i, guide in enumerate(node_data['guides']):
                try:
                    if not guide or not isinstance(guide, dict):
                        print(f"   âš ï¸  è·³è¿‡æ— æ•ˆæŒ‡å— #{i+1}: {type(guide)}")
                        continue
                        
                    guide_data = guide.copy()
                    print(f"   ğŸ“– [{i+1}/{len(node_data['guides'])}] ä¿å­˜æŒ‡å—: {guide_data.get('title', 'Unknown')[:50]}")
                    
                    # ä¸ºæ¯ä¸ªguideåˆ›å»ºå•ç‹¬çš„ç›®å½•ï¼Œåª’ä½“æ–‡ä»¶å’Œguideæ–‡ä»¶åœ¨åŒä¸€å±‚çº§
                    guide_dir = guides_dir / f"guide_{i+1}"
                    guide_dir.mkdir(exist_ok=True)

                    # å¤„ç†åª’ä½“æ–‡ä»¶ï¼Œå­˜å‚¨åœ¨guideç›®å½•ä¸‹
                    try:
                        self._process_media_urls(guide_data, guide_dir)
                    except Exception as media_error:
                        print(f"   âš ï¸  åª’ä½“æ–‡ä»¶å¤„ç†å¤±è´¥: {media_error}")

                    # ä¿å­˜guideæ–‡ä»¶åˆ°guideç›®å½•
                    guide_file = guide_dir / "guide.json"
                    with open(guide_file, 'w', encoding='utf-8') as f:
                        safe_json_dump(guide_data, f, ensure_ascii=False, indent=2)
                    
                    guides_count += 1
                    print(f"   âœ… æŒ‡å—ä¿å­˜æˆåŠŸ: {guide_file}")
                    
                except Exception as e:
                    print(f"   âŒ ä¿å­˜æŒ‡å— #{i+1} å¤±è´¥: {e}")
            
            print(f"   ğŸ“– æŒ‡å—å¤„ç†å®Œæˆ: {guides_count}/{len(node_data['guides'])} æˆåŠŸ")

        # å¤„ç†troubleshooting
        troubleshooting_count = 0
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = node_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)
            print(f"   ğŸ”§ å¤„ç† {len(node_data['troubleshooting'])} ä¸ªæ•…éšœæ’é™¤")

            for i, ts in enumerate(node_data['troubleshooting']):
                try:
                    if not ts or not isinstance(ts, dict):
                        print(f"   âš ï¸  è·³è¿‡æ— æ•ˆæ•…éšœæ’é™¤ #{i+1}: {type(ts)}")
                        continue
                        
                    ts_data = ts.copy()
                    print(f"   ğŸ”§ [{i+1}/{len(node_data['troubleshooting'])}] ä¿å­˜æ•…éšœæ’é™¤: {ts_data.get('title', 'Unknown')[:50]}")
                    
                    # ä¸ºæ¯ä¸ªtroubleshootingåˆ›å»ºå•ç‹¬çš„ç›®å½•ï¼Œåª’ä½“æ–‡ä»¶å’Œtsæ–‡ä»¶åœ¨åŒä¸€å±‚çº§
                    ts_item_dir = ts_dir / f"troubleshooting_{i+1}"
                    ts_item_dir.mkdir(exist_ok=True)

                    # å¤„ç†åª’ä½“æ–‡ä»¶ï¼Œå­˜å‚¨åœ¨troubleshootingç›®å½•ä¸‹
                    try:
                        self._process_media_urls(ts_data, ts_item_dir)
                    except Exception as media_error:
                        print(f"   âš ï¸  åª’ä½“æ–‡ä»¶å¤„ç†å¤±è´¥: {media_error}")

                    # ä¿å­˜troubleshootingæ–‡ä»¶åˆ°troubleshootingç›®å½•
                    ts_file = ts_item_dir / "troubleshooting.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        safe_json_dump(ts_data, f, ensure_ascii=False, indent=2)
                    
                    troubleshooting_count += 1
                    print(f"   âœ… æ•…éšœæ’é™¤ä¿å­˜æˆåŠŸ: {ts_file}")
                    
                except Exception as e:
                    print(f"   âŒ ä¿å­˜æ•…éšœæ’é™¤ #{i+1} å¤±è´¥: {e}")

            print(f"   ğŸ”§ æ•…éšœæ’é™¤å¤„ç†å®Œæˆ: {troubleshooting_count}/{len(node_data['troubleshooting'])} æˆåŠŸ")

            # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
            if self.cache_manager:
                try:
                    current_url = node_data.get('url', '')
                    if current_url and node_data.get('troubleshooting'):
                        troubleshooting_cache_key = f"{current_url}#troubleshooting"
                        self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                            troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)
                        print(f"   ğŸ’¾ å·²ç”Ÿæˆtroubleshooting_cache.jsonæ–‡ä»¶: {node_dir / 'troubleshooting_cache.json'}")
                    else:
                        print(f"   âš ï¸  è·³è¿‡ç”Ÿæˆtroubleshootingç¼“å­˜: URLæˆ–æ•°æ®ä¸ºç©º")
                except Exception as cache_error:
                    print(f"   âš ï¸  ä¿å­˜troubleshootingç¼“å­˜å¤±è´¥: {cache_error}")
                    import traceback
                    print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

        # æ›´æ–°ç¼“å­˜ç´¢å¼•
        try:
            self._update_cache_for_node(node_data, node_dir)
            print(f"   ğŸ“‹ ç¼“å­˜ç´¢å¼•æ›´æ–°æˆåŠŸ")
        except Exception as cache_error:
            print(f"   âš ï¸  æ›´æ–°ç¼“å­˜ç´¢å¼•å¤±è´¥: {cache_error}")
            
        print(f"   âœ… èŠ‚ç‚¹å†…å®¹ä¿å­˜å®Œæˆ: ğŸ“– {guides_count} ä¸ªæŒ‡å—, ğŸ”§ {troubleshooting_count} ä¸ªæ•…éšœæ’é™¤")

    def _validate_node_data_before_save(self, node_data):
        """åœ¨ä¿å­˜å‰éªŒè¯èŠ‚ç‚¹æ•°æ®çš„å®Œæ•´æ€§"""
        if not node_data or not isinstance(node_data, dict):
            print(f"   âŒ èŠ‚ç‚¹æ•°æ®éªŒè¯å¤±è´¥: æ•°æ®ä¸ºç©ºæˆ–ç±»å‹é”™è¯¯ {type(node_data)}")
            return False
            
        # æ£€æŸ¥åŸºæœ¬å­—æ®µ
        required_fields = ['name', 'url']
        missing_fields = [field for field in required_fields if not node_data.get(field)]
        if missing_fields:
            print(f"   âš ï¸  ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
        
        # æ£€æŸ¥guidesæ•°æ®
        guides = node_data.get('guides', [])
        if guides:
            print(f"   ğŸ“– éªŒè¯ {len(guides)} ä¸ªæŒ‡å—æ•°æ®:")
            for i, guide in enumerate(guides):
                if not isinstance(guide, dict):
                    print(f"      âŒ æŒ‡å— #{i+1} æ•°æ®ç±»å‹é”™è¯¯: {type(guide)}")
                    continue
                    
                guide_title = guide.get('title', 'Unknown')[:50]
                guide_steps = guide.get('steps', [])
                print(f"      ğŸ“– [{i+1}] {guide_title} - {len(guide_steps)} æ­¥éª¤")
                
                # æ£€æŸ¥æ­¥éª¤æ•°æ®
                if not guide_steps:
                    print(f"         âš ï¸  æŒ‡å—æ— æ­¥éª¤æ•°æ®")
                else:
                    # æ£€æŸ¥å‰å‡ ä¸ªæ­¥éª¤çš„æ•°æ®å®Œæ•´æ€§
                    for j, step in enumerate(guide_steps[:3]):  # åªæ£€æŸ¥å‰3ä¸ªæ­¥éª¤
                        if isinstance(step, dict):
                            step_text = step.get('text', '')
                            step_images = step.get('images', [])
                            print(f"         æ­¥éª¤ {j+1}: {len(step_text)} å­—ç¬¦, {len(step_images)} å›¾ç‰‡")
        
        # æ£€æŸ¥troubleshootingæ•°æ®
        troubleshooting = node_data.get('troubleshooting', [])
        if troubleshooting:
            print(f"   ğŸ”§ éªŒè¯ {len(troubleshooting)} ä¸ªæ•…éšœæ’é™¤æ•°æ®:")
            for i, ts in enumerate(troubleshooting):
                if not isinstance(ts, dict):
                    print(f"      âŒ æ•…éšœæ’é™¤ #{i+1} æ•°æ®ç±»å‹é”™è¯¯: {type(ts)}")
                    continue
                    
                ts_title = ts.get('title', 'Unknown')[:50]
                ts_causes = ts.get('causes', [])
                print(f"      ğŸ”§ [{i+1}] {ts_title} - {len(ts_causes)} åŸå› ")
                
                # æ£€æŸ¥åŸå› æ•°æ®
                if not ts_causes:
                    print(f"         âš ï¸  æ•…éšœæ’é™¤æ— åŸå› æ•°æ®")
                else:
                    for j, cause in enumerate(ts_causes[:2]):  # åªæ£€æŸ¥å‰2ä¸ªåŸå› 
                        if isinstance(cause, dict):
                            cause_content = cause.get('content', '')
                            cause_images = cause.get('images', [])
                            print(f"         åŸå›  {j+1}: {len(cause_content)} å­—ç¬¦, {len(cause_images)} å›¾ç‰‡")
        
        print(f"   âœ… èŠ‚ç‚¹æ•°æ®éªŒè¯å®Œæˆ")
        return True

    def _find_target_node_in_tree_v2(self, tree_data):
        """åœ¨æ ‘ç»“æ„ä¸­æ‰¾åˆ°åŒ…å«å®é™…å†…å®¹çš„ç›®æ ‡èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not tree_data or not isinstance(tree_data, dict):
            return None

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0

        if has_guides or has_troubleshooting:
            return tree_data

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            for child in tree_data['children']:
                result = self._find_target_node_in_tree_v2(child)
                if result:
                    return result

        return None

    def _find_all_content_nodes_in_tree(self, tree_data):
        """åœ¨æ ‘ç»“æ„ä¸­æ‰¾åˆ°æ‰€æœ‰åŒ…å«å®é™…å†…å®¹çš„èŠ‚ç‚¹"""
        content_nodes = []
        if not tree_data or not isinstance(tree_data, dict):
            return content_nodes

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
        has_guides = tree_data.get('guides') and len(tree_data.get('guides', [])) > 0
        has_troubleshooting = tree_data.get('troubleshooting') and len(tree_data.get('troubleshooting', [])) > 0

        if has_guides or has_troubleshooting:
            content_nodes.append(tree_data)

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            for child in tree_data['children']:
                child_nodes = self._find_all_content_nodes_in_tree(child)
                content_nodes.extend(child_nodes)

        return content_nodes

    def _find_target_node_by_url(self, tree_data, target_url):
        """æ ¹æ®URLåœ¨æ ‘ç»“æ„ä¸­æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹"""
        if not tree_data or not isinstance(tree_data, dict) or not target_url:
            return None

        # æ£€æŸ¥å½“å‰èŠ‚ç‚¹çš„URLæ˜¯å¦åŒ¹é…
        current_url = tree_data.get('url', '')
        if current_url and current_url == target_url:
            return tree_data

        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            for child in tree_data['children']:
                result = self._find_target_node_by_url(child, target_url)
                if result:
                    return result

        return None

    def _save_target_node_children(self, target_node, root_dir):
        """åªä¿å­˜ç›®æ ‡èŠ‚ç‚¹çš„å­ç»“æ„"""
        if not target_node or not isinstance(target_node, dict):
            return

        # ç¡®ä¿æ ¹ç›®å½•å­˜åœ¨
        root_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ç›®æ ‡èŠ‚ç‚¹æœ¬èº«çš„å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if target_node.get('guides') or target_node.get('troubleshooting'):
            self._save_node_content(target_node, root_dir)

        # ä¿å­˜å­èŠ‚ç‚¹ç»“æ„
        if 'children' in target_node and target_node['children']:
            for child in target_node['children']:
                child_name = child.get('name', 'Unknown')
                safe_name = self._clean_directory_name(child_name)
                child_dir = root_dir / safe_name

                # é€’å½’ä¿å­˜å­èŠ‚ç‚¹
                self._save_tree_structure(child, child_dir)

    def _update_cache_for_node(self, node_data, node_dir):
        """ä¸ºèŠ‚ç‚¹æ›´æ–°ç¼“å­˜ç´¢å¼•ï¼Œæ­£ç¡®ç»Ÿè®¡åª’ä½“æ–‡ä»¶æ•°é‡"""
        if not self.cache_manager or not node_data:
            return

        url = node_data.get('url', '')
        if not url:
            return

        try:
            # ç»Ÿè®¡å†…å®¹æ•°é‡
            guides_count = len(node_data.get('guides', []))
            troubleshooting_count = len(node_data.get('troubleshooting', []))

            # ç»Ÿè®¡åª’ä½“æ–‡ä»¶æ•°é‡ - é€’å½’ç»Ÿè®¡æ‰€æœ‰å­ç›®å½•ä¸­çš„åª’ä½“æ–‡ä»¶
            media_count = 0

            # ç»Ÿè®¡guidesç›®å½•ä¸‹çš„åª’ä½“æ–‡ä»¶
            guides_dir = node_dir / "guides"
            if guides_dir.exists():
                for guide_subdir in guides_dir.iterdir():
                    if guide_subdir.is_dir():
                        media_subdir = guide_subdir / "media"
                        if media_subdir.exists():
                            media_count += len(list(media_subdir.glob("*")))

            # ç»Ÿè®¡troubleshootingç›®å½•ä¸‹çš„åª’ä½“æ–‡ä»¶
            ts_dir = node_dir / "troubleshooting"
            if ts_dir.exists():
                for ts_subdir in ts_dir.iterdir():
                    if ts_subdir.is_dir():
                        media_subdir = ts_subdir / "media"
                        if media_subdir.exists():
                            media_count += len(list(media_subdir.glob("*")))

            # ç»Ÿè®¡æ ¹ç›®å½•ä¸‹çš„åª’ä½“æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            root_media_dir = node_dir / "media"
            if root_media_dir.exists():
                media_count += len(list(root_media_dir.glob("*")))

            print(f"   ğŸ“Š ç»Ÿè®¡ç»“æœ - æŒ‡å—: {guides_count}, æ•…éšœæ’é™¤: {troubleshooting_count}, åª’ä½“æ–‡ä»¶: {media_count}")

            # æ·»åŠ åˆ°ç¼“å­˜
            self.cache_manager.add_to_cache(
                url, node_dir,
                guides_count=guides_count,
                troubleshooting_count=troubleshooting_count,
                media_count=media_count
            )

        except Exception as e:
            self.logger.error(f"æ›´æ–°ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def _save_target_content_to_root(self, tree_data, root_dir):
        """å°†ç›®æ ‡äº§å“å†…å®¹ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•ï¼Œä¼˜åŒ–ç›®å½•ç»“æ„"""
        if not tree_data or not isinstance(tree_data, dict):
            return

        # å¤åˆ¶æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = tree_data.copy()
        self._process_media_urls(node_data, root_dir)

        # ä¿å­˜ä¸»è¦ä¿¡æ¯åˆ°æ ¹ç›®å½•çš„info.json
        info_data = {k: v for k, v in node_data.items()
                    if k not in ['children', 'guides', 'troubleshooting']}

        info_file = root_dir / "info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            safe_json_dump(info_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides - ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•çš„guidesæ–‡ä»¶å¤¹
        if 'guides' in node_data and node_data['guides']:
            guides_dir = root_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    safe_json_dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - åªæœ‰å½“å‰èŠ‚ç‚¹æœ‰troubleshootingæ•°æ®æ—¶æ‰ä¿å­˜
        # troubleshootingåº”è¯¥ä¿å­˜åœ¨å®ƒæ‰€å±çš„å…·ä½“é¡µé¢ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯ä¸Šçº§ç›®å½•
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            # æ£€æŸ¥è¿™äº›troubleshootingæ˜¯å¦çœŸçš„å±äºå½“å‰é¡µé¢
            current_url = node_data.get('url', '')
            if current_url and self._troubleshooting_belongs_to_current_page(node_data['troubleshooting'], current_url):
                ts_dir = root_dir / "troubleshooting"
                ts_dir.mkdir(exist_ok=True)
                # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
                ts_media_dir = ts_dir / "media"
                ts_media_dir.mkdir(exist_ok=True)

                for i, ts in enumerate(node_data['troubleshooting']):
                    ts_data = ts.copy()
                    self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                    ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        safe_json_dump(ts_data, f, ensure_ascii=False, indent=2)

                print(f"   ğŸ’¾ ä¿å­˜äº† {len(node_data['troubleshooting'])} ä¸ªtroubleshootingåˆ°: {ts_dir}")

                # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
                if self.cache_manager and current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)
            else:
                print(f"   âš ï¸  è·³è¿‡ä¿å­˜troubleshootingï¼Œå› ä¸ºå®ƒä»¬ä¸å±äºå½“å‰é¡µé¢: {current_url}")

        # å¤„ç†å­ç±»åˆ« - ä¿å­˜åˆ°subcategoriesæ–‡ä»¶å¤¹
        if 'children' in node_data and node_data['children']:
            subcategories_dir = root_dir / "subcategories"
            subcategories_dir.mkdir(exist_ok=True)
            for child in node_data['children']:
                self._save_subcategory_to_filesystem(child, subcategories_dir)

    def _save_subcategory_to_filesystem(self, node, subcategories_dir):
        """ä¿å­˜å­ç±»åˆ«åˆ°subcategoriesç›®å½•"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")

        # åˆ›å»ºå­ç±»åˆ«ç›®å½•
        subcat_dir = subcategories_dir / safe_name
        subcat_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, subcat_dir)

        # ä¿å­˜å­ç±»åˆ«ä¿¡æ¯
        info_file = subcat_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            safe_json_dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†å­ç±»åˆ«çš„guideså’Œtroubleshooting
        if 'guides' in node_data and node_data['guides']:
            guides_dir = subcat_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    safe_json_dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - åªæœ‰å½“å‰å­ç±»åˆ«æœ‰troubleshootingæ•°æ®æ—¶æ‰ä¿å­˜
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            current_url = node_data.get('url', '')
            if current_url and self._troubleshooting_belongs_to_current_page(node_data['troubleshooting'], current_url):
                ts_dir = subcat_dir / "troubleshooting"
                ts_dir.mkdir(exist_ok=True)
                # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
                ts_media_dir = ts_dir / "media"
                ts_media_dir.mkdir(exist_ok=True)

                for i, ts in enumerate(node_data['troubleshooting']):
                    ts_data = ts.copy()
                    self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                    ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        safe_json_dump(ts_data, f, ensure_ascii=False, indent=2)

                # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
                if self.cache_manager and current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)

                print(f"   ğŸ’¾ ä¿å­˜äº† {len(node_data['troubleshooting'])} ä¸ªtroubleshootingåˆ°å­ç±»åˆ«: {subcat_dir}")
            else:
                print(f"   âš ï¸  è·³è¿‡ä¿å­˜troubleshootingåˆ°å­ç±»åˆ«ï¼Œå› ä¸ºå®ƒä»¬ä¸å±äºå½“å‰é¡µé¢: {current_url}")

        # é€’å½’å¤„ç†æ›´æ·±å±‚çš„å­ç±»åˆ«
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_subcategory_to_filesystem(child, subcat_dir)

    def _save_node_to_filesystem(self, node, base_dir, path_prefix):
        """é€’å½’ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶ç³»ç»Ÿï¼Œç¡®ä¿è·¯å¾„ç¬¦åˆçœŸå®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        # ä½¿ç”¨ç»Ÿä¸€çš„ç›®å½•åæ¸…ç†æ–¹æ³•
        safe_name = self._clean_directory_name(node_name)

        # æ„å»ºå½“å‰èŠ‚ç‚¹è·¯å¾„
        if path_prefix:
            current_path = path_prefix + "/" + safe_name
            node_dir = base_dir / current_path
        else:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒ¹é…çš„ç›®å½•
            existing_dir = self._find_matching_directory(base_dir, safe_name)
            if existing_dir:
                node_dir = existing_dir
                print(f"   âœ… ä½¿ç”¨å·²æœ‰ç›®å½•: {node_dir}")
            else:
                node_dir = base_dir / safe_name

        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not node_dir.exists():
            node_dir.mkdir(parents=True, exist_ok=True)
            print(f"   ğŸ“ åˆ›å»ºæ–°ç›®å½•: {node_dir}")

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, node_dir)

        # ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°JSONæ–‡ä»¶
        info_file = node_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            safe_json_dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    safe_json_dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - åªæœ‰å±äºå½“å‰èŠ‚ç‚¹çš„troubleshootingæ‰ä¿å­˜
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            current_url = node_data.get('url', '')
            if current_url and self._troubleshooting_belongs_to_current_page(node_data['troubleshooting'], current_url):
                ts_dir = node_dir / "troubleshooting"
                ts_dir.mkdir(exist_ok=True)
                # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
                ts_media_dir = ts_dir / "media"
                ts_media_dir.mkdir(exist_ok=True)

                for i, ts in enumerate(node_data['troubleshooting']):
                    ts_data = ts.copy()
                    self._process_media_urls(ts_data, ts_dir)
                    ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                    with open(ts_file, 'w', encoding='utf-8') as f:
                        safe_json_dump(ts_data, f, ensure_ascii=False, indent=2)

                # åœ¨Troubleshootingç›®å½•åˆ›å»ºåç«‹å³ä¿å­˜cacheæ–‡ä»¶
                if self.cache_manager and current_url:
                    troubleshooting_cache_key = f"{current_url}#troubleshooting"
                    self.cache_manager.save_troubleshooting_cache_after_directory_creation(
                        troubleshooting_cache_key, current_url, node_data['troubleshooting'], ts_dir)

                print(f"   ğŸ’¾ ä¿å­˜äº† {len(node_data['troubleshooting'])} ä¸ªtroubleshootingåˆ°èŠ‚ç‚¹: {node_dir}")
            else:
                print(f"   âš ï¸  è·³è¿‡ä¿å­˜troubleshootingåˆ°èŠ‚ç‚¹ï¼Œå› ä¸ºå®ƒä»¬ä¸å±äºå½“å‰é¡µé¢: {current_url}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_node_to_filesystem(child, base_dir, current_path)

    def print_combined_tree_structure(self, node, level=0):
        """
        æ‰“å°æ•´åˆåçš„æ ‘å½¢ç»“æ„ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦
        """
        if not node:
            return

        indent = "  " * level
        name = node.get('name', 'Unknown')

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„å†…å®¹
        guides_count = len(node.get('guides', []))
        troubleshooting_count = len(node.get('troubleshooting', []))
        children_count = len(node.get('children', []))

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if 'title' in node and 'steps' in node:
            # ç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹
            steps_count = len(node.get('steps', []))
            print(f"{indent}ğŸ“– {name} [æŒ‡å— - {steps_count}æ­¥éª¤]")
        elif 'causes' in node:
            # ç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹
            causes_count = len(node.get('causes', []))
            print(f"{indent}ğŸ”§ {name} [æ•…éšœæ’é™¤ - {causes_count}åŸå› ]")
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“± {name} [äº§å“{content_str}]")
        elif children_count > 0 or guides_count > 0 or troubleshooting_count > 0:
            # åˆ†ç±»èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“ {name} [åˆ†ç±»{content_str}]")
        else:
            # å…¶ä»–èŠ‚ç‚¹
            print(f"{indent}â“ {name} [æœªçŸ¥ç±»å‹]")

        # æ˜¾ç¤ºguideså†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if guides_count > 0:
            for guide in node.get('guides', []):
                guide_name = guide.get('title', 'Unknown Guide')
                steps_count = len(guide.get('steps', []))
                print(f"{indent}  ğŸ“– {guide_name} [{steps_count}æ­¥éª¤]")

        # æ˜¾ç¤ºtroubleshootingå†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if troubleshooting_count > 0:
            for ts in node.get('troubleshooting', []):
                ts_name = ts.get('title', 'Unknown Troubleshooting')
                causes_count = len(ts.get('causes', []))
                print(f"{indent}  ğŸ”§ {ts_name} [{causes_count}åŸå› ]")

        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if children_count > 0:
            for child in node['children']:
                self.print_combined_tree_structure(child, level + 1)

    def extract_dynamic_sections(self, soup):
        """åŠ¨æ€æå–é¡µé¢ä¸Šçš„çœŸå®å­—æ®µåç§°å’Œå†…å®¹ï¼ŒåŸºäºå®é™…é¡µé¢ç»“æ„ï¼Œç¡®ä¿å­—æ®µåˆ†ç¦»å’Œæ— é‡å¤"""
        sections = {}
        seen_content = set()  # ç”¨äºå»é‡
        processed_elements = set()  # è®°å½•å·²å¤„ç†çš„å…ƒç´ ï¼Œé¿å…é‡å¤å¤„ç†

        try:
            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸï¼Œé¿å…å¯¼èˆªå’Œé¡µè„š
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda x: x and 'content' in str(x).lower())
            if not main_content:
                main_content = soup

            # é¦–å…ˆé€šè¿‡IDå’Œæ ‡é¢˜æ–‡æœ¬ç²¾ç¡®å®šä½å„ä¸ªç‹¬ç«‹éƒ¨åˆ†
            section_mappings = [
                ('introduction', ['#introduction']),
                ('first_steps', ['#Section_First_Steps']),
                ('triage', ['#Section_Triage'])
            ]

            for field_name, selectors in section_mappings:
                found = False
                for selector in selectors:
                    section_element = main_content.select_one(selector)
                    if section_element and id(section_element) not in processed_elements:
                        content = self.extract_section_content_precisely(section_element, main_content, field_name)
                        if content and content.strip():
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸å·²æœ‰å†…å®¹é‡å¤
                            if not self.is_content_duplicate(content, seen_content):
                                sections[field_name] = content.strip()
                                seen_content.add(content.strip())
                                processed_elements.add(id(section_element))
                                print(f"ç²¾ç¡®æå–å­—æ®µ: {field_name} ({len(content)} å­—ç¬¦)")
                                found = True
                                break

                # å¦‚æœé€šè¿‡IDæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡æ ‡é¢˜æ–‡æœ¬æŸ¥æ‰¾
                if not found:
                    target_texts = {
                        'introduction': ['introduction'],
                        'first_steps': ['first steps', 'before undertaking'],
                        'triage': ['triage']
                    }

                    if field_name in target_texts:
                        for heading in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                            heading_text = heading.get_text().strip().lower()
                            if any(target in heading_text for target in target_texts[field_name]):
                                if id(heading) not in processed_elements:
                                    content = self.extract_section_content_precisely(heading, main_content, field_name)
                                    if content and content.strip():
                                        if not self.is_content_duplicate(content, seen_content):
                                            sections[field_name] = content.strip()
                                            seen_content.add(content.strip())
                                            processed_elements.add(id(heading))
                                            print(f"é€šè¿‡æ ‡é¢˜æå–å­—æ®µ: {field_name} ({len(content)} å­—ç¬¦)")
                                            found = True
                                            break

        except Exception as e:
            print(f"åŠ¨æ€æå–sectionsæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

        return sections

    def extract_section_content_precisely(self, element, main_content, field_name):
        """ç²¾ç¡®æå–sectionå†…å®¹"""
        try:
            content_parts = []

            # ä»å…ƒç´ åé¢å¼€å§‹æå–å†…å®¹
            next_elem = element.find_next_sibling()
            while next_elem and next_elem.name not in ['h1', 'h2', 'h3']:
                if next_elem.name in ['p', 'div', 'ul', 'ol']:
                    text = next_elem.get_text().strip()
                    if text and len(text) > 10 and not self.is_commercial_text(text):
                        content_parts.append(text)
                next_elem = next_elem.find_next_sibling()

            return '\n\n'.join(content_parts) if content_parts else ""
        except Exception:
            return ""

    def is_content_duplicate(self, content, seen_content):
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        content_clean = content.strip()
        return content_clean in seen_content

    def extract_causes_sections_with_media(self, soup):
        """æå–Causeséƒ¨åˆ†å†…å®¹ï¼Œå¹¶ä¸ºæ¯ä¸ªcauseæå–å¯¹åº”çš„å›¾ç‰‡å’Œè§†é¢‘"""
        causes = []
        seen_numbers = set()  # é˜²æ­¢é‡å¤

        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¸¦ç¼–å·çš„é“¾æ¥ï¼Œè¿™äº›é€šå¸¸æ˜¯causes
            cause_links = soup.find_all('a', href=lambda x: x and '#Section_' in x)

            for link in cause_links:
                href = link.get('href', '')
                if '#Section_' in href:
                    # æå–ç¼–å·å’Œæ ‡é¢˜
                    link_text = link.get_text().strip()

                    # è·³è¿‡éç¼–å·çš„é“¾æ¥
                    if not any(char.isdigit() for char in link_text):
                        continue

                    # æå–ç¼–å·
                    number_match = re.search(r'^(\d+)', link_text)
                    if number_match:
                        number = number_match.group(1)

                        # é˜²æ­¢é‡å¤
                        if number in seen_numbers:
                            continue
                        seen_numbers.add(number)

                        title = re.sub(r'^\d+\s*', '', link_text).strip()

                        # æŸ¥æ‰¾å¯¹åº”çš„å†…å®¹éƒ¨åˆ†
                        section_id = href.split('#')[-1]
                        content_section = soup.find(id=section_id)

                        content = ""
                        images = []
                        videos = []

                        if content_section:
                            content = self.extract_section_content(content_section)
                            # ä»è¯¥sectionä¸­æå–å›¾ç‰‡å’Œè§†é¢‘
                            images = self._extract_troubleshooting_images_from_section(content_section)
                            videos = self.extract_videos_from_section(content_section)

                        cause_data = {
                            "number": number,
                            "title": title,
                            "content": content
                        }

                        # åªæœ‰å½“æœ‰å›¾ç‰‡æ—¶æ‰æ·»åŠ imageså­—æ®µ
                        if images:
                            cause_data["images"] = images

                        # åªæœ‰å½“æœ‰è§†é¢‘æ—¶æ‰æ·»åŠ videoså­—æ®µ
                        if videos:
                            cause_data["videos"] = videos

                        causes.append(cause_data)

                        if self.verbose:
                            print(f"æå–Cause {number}: {title} ({len(content)} å­—ç¬¦, {len(images)} å›¾ç‰‡, {len(videos)} è§†é¢‘)")

            return causes
        except Exception as e:
            print(f"æå–Causesæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def extract_section_content(self, section):
        """ä»sectionä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            content_parts = []
            p_elements = section.find_all('p')
            for p in p_elements:
                p_text = p.get_text().strip()
                if self.is_commercial_text(p_text):
                    continue
                if p_text and len(p_text) > 10:
                    content_parts.append(p_text)
            return '\n\n'.join(content_parts)
        except Exception:
            return ""

    def extract_videos_from_section(self, section):
        """ä»sectionä¸­æå–è§†é¢‘"""
        videos = []
        try:
            video_elements = section.find_all(['video', 'iframe'])
            for video in video_elements:
                try:
                    from bs4 import Tag
                    if isinstance(video, Tag):
                        video_src = video.get('src') or video.get('data-src')
                        if video_src and isinstance(video_src, str):
                            if 'youtube.com' in video_src or 'youtu.be' in video_src:
                                video_data = {
                                    "url": video_src,
                                    "title": video.get('title', 'Video')
                                }
                                videos.append(video_data)
                except Exception:
                    continue
        except Exception:
            pass
        return videos

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("=" * 60)
    print("iFixité«˜æ€§èƒ½çˆ¬è™«å·¥å…· - ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python auto_crawler.py [URLæˆ–è®¾å¤‡å] [é€‰é¡¹...]")
    print("\nåŸºæœ¬ç”¨æ³•:")
    print("  python auto_crawler.py iMac_M_Series")
    print("  python auto_crawler.py https://www.ifixit.com/Device/Television")
    print("  python auto_crawler.py Television")
    print("\nğŸš€ æ€§èƒ½è°ƒä¼˜é€‰é¡¹:")
    print("  --workers N            è®¾ç½®å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤8ï¼Œæ¨è8-16ï¼‰")
    print("  --max-connections N    è®¾ç½®æœ€å¤§è¿æ¥æ•°ï¼ˆé»˜è®¤80ï¼‰")
    print("  --max-retries N        è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰")
    print("  --timeout N            è®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30ï¼‰")
    print("  --delay N              è®¾ç½®è¯·æ±‚é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤0.5ï¼‰")
    print("  --burst-mode           å¯ç”¨çˆ†å‘æ¨¡å¼ï¼ˆæœ€å¤§æ€§èƒ½ï¼Œå¯èƒ½è¢«é™åˆ¶ï¼‰")
    print("  --conservative         å¯ç”¨ä¿å®ˆæ¨¡å¼ï¼ˆé™ä½å¹¶å‘ï¼Œæ›´ç¨³å®šï¼‰")
    print("\nğŸŒ ä»£ç†å’Œç½‘ç»œé€‰é¡¹:")
    print("  --no-proxy             å…³é—­éš§é“ä»£ç†ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    print("  --proxy-switch N       ä»£ç†åˆ‡æ¢é¢‘ç‡ï¼ˆè¯·æ±‚æ•°ï¼Œé»˜è®¤1=æ¯æ¬¡åˆ‡æ¢ï¼‰")
    print("  --user-agent TEXT      è‡ªå®šä¹‰User-Agent")
    print("\nğŸ’¾ ç¼“å­˜å’Œæ•°æ®é€‰é¡¹:")
    print("  --no-cache             ç¦ç”¨ç¼“å­˜æ£€æŸ¥ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    print("  --force-refresh        å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
    print("  --cache-ttl N          ç¼“å­˜é…ç½®å‚æ•°ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼Œå®é™…ä¸ºé•¿æœŸä¿å­˜ï¼‰")
    print("\nğŸ”„ æ–­ç‚¹ç»­çˆ¬é€‰é¡¹:")
    print("  --no-resume            ç¦ç”¨æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    print("  --reset-progress       é‡ç½®æ ‘æ„å»ºè¿›åº¦ï¼ˆæ¸…é™¤æ–­ç‚¹è®°å½•ï¼‰")
    print("  --reset-only           ä»…é‡ç½®è¿›åº¦åé€€å‡ºï¼ˆä¸å¼€å§‹çˆ¬å–ï¼‰")
    print("  --show-progress        æ˜¾ç¤ºå½“å‰æ ‘æ„å»ºè¿›åº¦")
    print("  --progress-only        ä»…æ˜¾ç¤ºè¿›åº¦åé€€å‡ºï¼ˆä¸å¼€å§‹çˆ¬å–ï¼‰")
    print("\nğŸ“¹ åª’ä½“å¤„ç†é€‰é¡¹:")
    print("  --download-videos      å¯ç”¨è§†é¢‘æ–‡ä»¶ä¸‹è½½ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰")
    print("  --max-video-size N     è®¾ç½®è§†é¢‘æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼Œé»˜è®¤50ï¼‰")
    print("  --skip-images          è·³è¿‡å›¾ç‰‡ä¸‹è½½ï¼ˆä»…ä¿å­˜URLï¼‰")
    print("\nğŸ”§ è°ƒè¯•é€‰é¡¹:")
    print("  --verbose              å¯ç”¨è¯¦ç»†è¾“å‡º")
    print("  --debug                å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆæ›´è¯¦ç»†çš„æ—¥å¿—ï¼‰")
    print("  --stats                æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
    print("\nğŸ“Š é¢„è®¾é…ç½®:")
    print("  --fast                 å¿«é€Ÿæ¨¡å¼ï¼š--workers 12 --burst-mode")
    print("  --stable               ç¨³å®šæ¨¡å¼ï¼š--workers 4 --conservative --delay 1")
    print("  --enterprise           ä¼ä¸šæ¨¡å¼ï¼š--workers 16 --max-connections 160")
    print("\nç¤ºä¾‹:")
    print("  # é»˜è®¤é«˜æ€§èƒ½æ¨¡å¼ï¼ˆ8çº¿ç¨‹+ä»£ç†æ± ï¼‰")
    print("  python auto_crawler.py iMac_M_Series")
    print("")
    print("  # å¿«é€Ÿæ¨¡å¼ï¼ˆ12çº¿ç¨‹+çˆ†å‘æ¨¡å¼ï¼‰")
    print("  python auto_crawler.py iMac_M_Series --fast")
    print("")
    print("  # ä¼ä¸šçº§é«˜å¹¶å‘æ¨¡å¼")
    print("  python auto_crawler.py Television --enterprise --download-videos")
    print("")
    print("  # ç¨³å®šæ¨¡å¼ï¼ˆé€‚åˆç½‘ç»œä¸ç¨³å®šç¯å¢ƒï¼‰")
    print("  python auto_crawler.py iPhone --stable --verbose")
    print("")
    print("  # è‡ªå®šä¹‰é«˜æ€§èƒ½é…ç½®")
    print("  python auto_crawler.py MacBook --workers 20 --max-connections 200 --delay 0.2")
    print("")
    print("  # æ–­ç‚¹ç»­çˆ¬ç›¸å…³æ“ä½œ")
    print("  python auto_crawler.py Television --show-progress  # æŸ¥çœ‹è¿›åº¦")
    print("  python auto_crawler.py Television --reset-progress # é‡ç½®è¿›åº¦")
    print("  python auto_crawler.py Television                  # è‡ªåŠ¨ä»æ–­ç‚¹æ¢å¤")
    print("\nğŸ¯ é»˜è®¤é…ç½®è¯´æ˜:")
    print("- ğŸ”¥ é«˜æ€§èƒ½ï¼šé»˜è®¤8çº¿ç¨‹å¹¶å‘ + éš§é“ä»£ç†æ± ")
    print("- ğŸŒ æ™ºèƒ½ä»£ç†ï¼šHTTPéš§é“ä»£ç†æ± ï¼Œæ¯æ¬¡è¯·æ±‚è‡ªåŠ¨åˆ‡æ¢IP")
    print("- ğŸ’¾ æ™ºèƒ½ç¼“å­˜ï¼šè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„å†…å®¹ï¼Œé•¿æœŸä¿å­˜åˆ°æœ¬åœ°")
    print("- ğŸ”„ æ–­ç‚¹ç»­çˆ¬ï¼šè‡ªåŠ¨è®°å½•æ ‘æ„å»ºè¿›åº¦ï¼Œæ”¯æŒä¸­æ–­æ¢å¤")
    print("- ğŸ”„ é”™è¯¯é‡è¯•ï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿ç­–ç•¥")
    print("- ğŸ“ åª’ä½“ä¸‹è½½ï¼šè‡ªåŠ¨ä¸‹è½½å¹¶æœ¬åœ°åŒ–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶")
    print("- ğŸ“Š å®æ—¶ç»Ÿè®¡ï¼šæ˜¾ç¤ºçˆ¬å–è¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡")
    print("=" * 60)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = sys.argv[1:]

    # æ£€æŸ¥å¸®åŠ©å‚æ•°
    if not args or args[0].lower() in ['--help', '-h', 'help']:
        print_usage()
        return

    # è·å–ç›®æ ‡URL/åç§°
    input_text = args[0]

    # ğŸš€ é»˜è®¤é«˜æ€§èƒ½é…ç½®
    verbose = '--verbose' in args or '--debug' in args
    debug_mode = '--debug' in args
    show_stats = '--stats' in args
    use_proxy = '--no-proxy' not in args  # é»˜è®¤å¯ç”¨ä»£ç†
    use_cache = '--no-cache' not in args  # é»˜è®¤å¯ç”¨ç¼“å­˜
    force_refresh = '--force-refresh' in args
    skip_images = '--skip-images' in args

    # ğŸ¯ é¢„è®¾é…ç½®å¤„ç†
    if '--fast' in args:
        # å¿«é€Ÿæ¨¡å¼ï¼šé«˜å¹¶å‘ + çˆ†å‘æ¨¡å¼
        default_workers = 12
        burst_mode = True
        conservative_mode = False
        default_delay = 0.3
    elif '--stable' in args:
        # ç¨³å®šæ¨¡å¼ï¼šä½å¹¶å‘ + ä¿å®ˆç­–ç•¥
        default_workers = 4
        burst_mode = False
        conservative_mode = True
        default_delay = 1.0
    elif '--enterprise' in args:
        # ä¼ä¸šæ¨¡å¼ï¼šè¶…é«˜å¹¶å‘
        default_workers = 16
        burst_mode = True
        conservative_mode = False
        default_delay = 0.2
    else:
        # é»˜è®¤é«˜æ€§èƒ½æ¨¡å¼
        default_workers = 8
        burst_mode = '--burst-mode' in args
        conservative_mode = '--conservative' in args
        default_delay = 0.5

    # è§£ææ€§èƒ½å‚æ•°
    max_workers = default_workers
    if '--workers' in args:
        try:
            workers_idx = args.index('--workers')
            if workers_idx + 1 < len(args):
                max_workers = int(args[workers_idx + 1])
        except (ValueError, IndexError):
            print(f"è­¦å‘Š: workerså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{default_workers}")

    # è§£æè¿æ¥æ•°å‚æ•°
    max_connections = max_workers * 10  # é»˜è®¤ä¸ºçº¿ç¨‹æ•°çš„10å€
    if '--max-connections' in args:
        try:
            conn_idx = args.index('--max-connections')
            if conn_idx + 1 < len(args):
                max_connections = int(args[conn_idx + 1])
        except (ValueError, IndexError):
            print(f"è­¦å‘Š: max-connectionså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{max_connections}")

    # è§£æé‡è¯•å‚æ•°
    max_retries = 1  # æé€Ÿæ¨¡å¼ï¼šæœ€å°‘é‡è¯•æ¬¡æ•°
    if '--max-retries' in args:
        try:
            retries_idx = args.index('--max-retries')
            if retries_idx + 1 < len(args):
                max_retries = int(args[retries_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-retrieså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼3")

    # è§£æè¶…æ—¶å‚æ•°
    timeout = 30
    if '--timeout' in args:
        try:
            timeout_idx = args.index('--timeout')
            if timeout_idx + 1 < len(args):
                timeout = int(args[timeout_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: timeoutå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼30ç§’")

    # è§£æè¯·æ±‚é—´éš”å‚æ•°
    request_delay = default_delay
    if '--delay' in args:
        try:
            delay_idx = args.index('--delay')
            if delay_idx + 1 < len(args):
                request_delay = float(args[delay_idx + 1])
        except (ValueError, IndexError):
            print(f"è­¦å‘Š: delayå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼{default_delay}ç§’")

    # è§£æä»£ç†åˆ‡æ¢é¢‘ç‡
    proxy_switch_freq = 1  # é»˜è®¤æ¯æ¬¡è¯·æ±‚éƒ½åˆ‡æ¢IP
    if '--proxy-switch' in args:
        try:
            switch_idx = args.index('--proxy-switch')
            if switch_idx + 1 < len(args):
                proxy_switch_freq = int(args[switch_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: proxy-switchå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼1ï¼ˆæ¯æ¬¡è¯·æ±‚åˆ‡æ¢ï¼‰")

    # è§£æç¼“å­˜TTL
    cache_ttl = 24
    if '--cache-ttl' in args:
        try:
            ttl_idx = args.index('--cache-ttl')
            if ttl_idx + 1 < len(args):
                cache_ttl = int(args[ttl_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: cache-ttlå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼24å°æ—¶")

    # è§£æè§†é¢‘ä¸‹è½½å‚æ•°
    download_videos = '--download-videos' in args
    max_video_size_mb = 50
    if '--max-video-size' in args:
        try:
            size_idx = args.index('--max-video-size')
            if size_idx + 1 < len(args):
                max_video_size_mb = int(args[size_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-video-sizeå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼50MB")

    # ğŸ”„ è§£ææ–­ç‚¹ç»­çˆ¬å‚æ•°
    enable_resume = '--no-resume' not in args  # é»˜è®¤å¯ç”¨æ–­ç‚¹ç»­çˆ¬
    reset_progress = '--reset-progress' in args
    show_progress = '--show-progress' in args

    # å¤„ç†è¿›åº¦é‡ç½®
    if reset_progress:
        print("ğŸ”„ é‡ç½®æ ‘æ„å»ºè¿›åº¦...")
        from tree_building_progress import TreeBuildingProgressManager

        # ç¡®å®šç›®æ ‡URL
        url = input_text if input_text.startswith('http') else f"https://www.ifixit.com/Device/{input_text}"
        progress_manager = TreeBuildingProgressManager(url)
        progress_manager.reset_progress()
        print("âœ… è¿›åº¦å·²é‡ç½®")

        if '--reset-only' in args:
            return

    # æ˜¾ç¤ºè¿›åº¦æŠ¥å‘Š
    if show_progress:
        print("ğŸ“Š æ˜¾ç¤ºæ ‘æ„å»ºè¿›åº¦...")
        from tree_building_progress import TreeBuildingProgressManager

        # ç¡®å®šç›®æ ‡URL
        url = input_text if input_text.startswith('http') else f"https://www.ifixit.com/Device/{input_text}"
        progress_manager = TreeBuildingProgressManager(url)
        progress_manager.display_progress_report()

        if '--progress-only' in args:
            return

    # è§£æè‡ªå®šä¹‰User-Agent
    custom_user_agent = None
    if '--user-agent' in args:
        try:
            ua_idx = args.index('--user-agent')
            if ua_idx + 1 < len(args):
                custom_user_agent = args[ua_idx + 1]
        except (ValueError, IndexError):
            print("è­¦å‘Š: user-agentå‚æ•°æ— æ•ˆ")

    if not input_text:
        print_usage()
        return

    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)

    if url:
        # åˆ›å»ºæ•´åˆçˆ¬è™«å®ä¾‹ï¼ˆç”¨äºæ£€æµ‹ï¼‰
        temp_crawler = CombinedIFixitCrawler(verbose=False, use_proxy=False)

        # æ£€æŸ¥ç›®æ ‡æ˜¯å¦å·²ç»å®Œæ•´çˆ¬å–
        is_complete, target_dir = temp_crawler._check_target_completeness(input_text)

        if is_complete and not force_refresh and target_dir:
            print("\n" + "=" * 60)
            print(f"ğŸ¯ æ£€æµ‹åˆ°ç›®æ ‡å·²å®Œæˆçˆ¬å–: {name}")
            print(f"ğŸ“ ç›®å½•ä½ç½®: {target_dir}")
            print("=" * 60)

            # æ£€æŸ¥ç›®å½•å†…å®¹
            info_file = target_dir / "info.json"
            guides_dir = target_dir / "guides"
            troubleshooting_dir = target_dir / "troubleshooting"
            media_dir = target_dir / "media"

            guides_count = len(list(guides_dir.glob("*.json"))) if guides_dir.exists() else 0
            ts_count = len(list(troubleshooting_dir.glob("*.json"))) if troubleshooting_dir.exists() else 0
            media_count = len(list(media_dir.glob("*"))) if media_dir.exists() else 0

            print(f"ğŸ“Š å·²æœ‰å†…å®¹ç»Ÿè®¡:")
            print(f"   åŸºæœ¬ä¿¡æ¯: {'âœ…' if info_file.exists() else 'âŒ'}")
            print(f"   æŒ‡å—æ•°é‡: {guides_count}")
            print(f"   æ•…éšœæ’é™¤æ•°é‡: {ts_count}")
            print(f"   åª’ä½“æ–‡ä»¶æ•°é‡: {media_count}")

            print(f"\nğŸ’¡ é€‰é¡¹:")
            print(f"   1. è·³è¿‡çˆ¬å–ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
            print(f"   2. é‡æ–°çˆ¬å–ï¼ˆè¦†ç›–ç°æœ‰æ•°æ®ï¼‰")
            print(f"   3. é€€å‡ºç¨‹åº")

            while True:
                choice = input("\nè¯·é€‰æ‹© (1/2/3): ").strip()
                if choice == "1":
                    print(f"âœ… è·³è¿‡çˆ¬å–ï¼Œç°æœ‰æ•°æ®ä½ç½®: {target_dir}")
                    return
                elif choice == "2":
                    print("ğŸ”„ å°†é‡æ–°çˆ¬å–å¹¶è¦†ç›–ç°æœ‰æ•°æ®...")
                    force_refresh = True
                    break
                elif choice == "3":
                    print("ğŸ‘‹ ç¨‹åºé€€å‡º")
                    return
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")

        # ğŸš€ æ˜¾ç¤ºé«˜æ€§èƒ½é…ç½®ä¿¡æ¯
        print("\n" + "=" * 80)
        print(f"ğŸ¯ å¼€å§‹é«˜æ€§èƒ½çˆ¬å–: {name}")
        print("=" * 80)
        print(f"ğŸ”¥ æ€§èƒ½é…ç½®:")
        print(f"   å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
        print(f"   æœ€å¤§è¿æ¥æ•°: {max_connections}")
        print(f"   è¯·æ±‚é—´éš”: {request_delay}ç§’")
        print(f"   è¶…æ—¶æ—¶é—´: {timeout}ç§’")
        print(f"   æœ€å¤§é‡è¯•: {max_retries}æ¬¡")

        mode_desc = []
        if burst_mode:
            mode_desc.append("ğŸš€çˆ†å‘æ¨¡å¼")
        if conservative_mode:
            mode_desc.append("ğŸ›¡ï¸ä¿å®ˆæ¨¡å¼")
        if mode_desc:
            print(f"   è¿è¡Œæ¨¡å¼: {' + '.join(mode_desc)}")

        print(f"ğŸŒ ç½‘ç»œé…ç½®:")
        print(f"   éš§é“ä»£ç†: {'âœ…å¯ç”¨' if use_proxy else 'âŒç¦ç”¨'}")
        if use_proxy:
            if proxy_switch_freq == 1:
                print(f"   ä»£ç†åˆ‡æ¢: æ¯æ¬¡è¯·æ±‚åˆ‡æ¢IP")
            else:
                print(f"   ä»£ç†åˆ‡æ¢: æ¯{proxy_switch_freq}æ¬¡è¯·æ±‚")
        print(f"   æ™ºèƒ½ç¼“å­˜: {'âœ…å¯ç”¨' if use_cache else 'âŒç¦ç”¨'}")
        if use_cache:
            print(f"   ç¼“å­˜ç­–ç•¥: é•¿æœŸä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶")

        print(f"ğŸ“ åª’ä½“å¤„ç†:")
        print(f"   å›¾ç‰‡ä¸‹è½½: {'âŒè·³è¿‡' if skip_images else 'âœ…å¯ç”¨'}")
        print(f"   è§†é¢‘ä¸‹è½½: {'âœ…å¯ç”¨' if download_videos else 'âŒç¦ç”¨'}")
        if download_videos:
            print(f"   è§†é¢‘å¤§å°é™åˆ¶: {max_video_size_mb}MB")

        print(f"ğŸ”§ å…¶ä»–é€‰é¡¹:")
        print(f"   è¯¦ç»†è¾“å‡º: {'âœ…å¯ç”¨' if verbose else 'âŒç¦ç”¨'}")
        print(f"   è°ƒè¯•æ¨¡å¼: {'âœ…å¯ç”¨' if debug_mode else 'âŒç¦ç”¨'}")
        print(f"   ç»Ÿè®¡ä¿¡æ¯: {'âœ…å¯ç”¨' if show_stats else 'âŒç¦ç”¨'}")
        print(f"   å¼ºåˆ¶åˆ·æ–°: {'âœ…å¯ç”¨' if force_refresh else 'âŒç¦ç”¨'}")

        print("=" * 80)
        print("ğŸš€ æ‰§è¡Œé˜¶æ®µ:")
        print("   é˜¶æ®µ1: æ„å»ºå®Œæ•´æ ‘å½¢ç»“æ„")
        print("   é˜¶æ®µ2: é«˜å¹¶å‘æå–è¯¦ç»†å†…å®¹")
        print("   é˜¶æ®µ3: å¹¶è¡Œä¸‹è½½åª’ä½“æ–‡ä»¶")
        print("=" * 80)

        # ğŸš€ åˆ›å»ºé«˜æ€§èƒ½æ•´åˆçˆ¬è™«
        crawler = CombinedIFixitCrawler(
            verbose=verbose,
            use_proxy=use_proxy,
            use_cache=use_cache,
            force_refresh=force_refresh,
            max_workers=max_workers,
            max_retries=max_retries,
            download_videos=download_videos,
            max_video_size_mb=max_video_size_mb,
            max_connections=max_connections,
            timeout=timeout,
            request_delay=request_delay,
            proxy_switch_freq=proxy_switch_freq,
            cache_ttl=cache_ttl,
            custom_user_agent=custom_user_agent,
            burst_mode=burst_mode,
            conservative_mode=conservative_mode,
            skip_images=skip_images,
            debug_mode=debug_mode,
            show_stats=show_stats
        )

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œæ•´åˆçˆ¬å–
        try:
            combined_data = crawler.crawl_combined_tree(url, name)

            if combined_data:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = crawler.count_detailed_nodes(combined_data)
                elapsed_time = time.time() - start_time

                # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
                print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
                print("=" * 60)
                crawler.print_combined_tree_structure(combined_data)
                print("=" * 60)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\næ•´åˆçˆ¬å–ç»Ÿè®¡:")
                print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
                print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
                print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
                print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
                print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
                print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

                # ä¿å­˜ç»“æœ
                print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶å¤¹...")
                filename = crawler.save_combined_result(combined_data, target_name=input_text)

                print(f"\nâœ… æ•´åˆçˆ¬å–å®Œæˆ!")
                print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {filename}")

                # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
                crawler._print_performance_stats()

                # ä¿å­˜ç¼“å­˜ç´¢å¼•
                if crawler.cache_manager:
                    crawler.cache_manager.save_cache_index()
                    cache_stats = crawler.cache_manager.get_cache_stats()
                    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
                    print(f"- æ€»å¤„ç†URL: {cache_stats['total_urls']}")
                    print(f"- ç¼“å­˜å‘½ä¸­: {cache_stats['cached_urls']}")
                    print(f"- æ–°å¢å¤„ç†: {cache_stats['new_urls']}")
                    if cache_stats['total_urls'] > 0:
                        hit_rate = (cache_stats['cached_urls'] / cache_stats['total_urls']) * 100
                        print(f"- ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")

                # æä¾›ä½¿ç”¨å»ºè®®
                print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
                print(f"- æŸ¥çœ‹æ–‡ä»¶å¤¹ç»“æ„äº†è§£å®Œæ•´çš„æ•°æ®å±‚çº§")
                print(f"- æ¯ä¸ªç›®å½•åŒ…å«info.jsonã€guides/ã€troubleshooting/å­ç›®å½•")
                print(f"- æ‰€æœ‰åª’ä½“æ–‡ä»¶å·²ä¸‹è½½åˆ°media/ç›®å½•ï¼ŒURLå·²æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„")
                print(f"- ä¸‹æ¬¡çˆ¬å–ç›¸åŒå†…å®¹å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œå¤§å¹…æå‡é€Ÿåº¦")

            else:
                print("\nâœ— æ•´åˆçˆ¬å–å¤±è´¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
            # å³ä½¿ä¸­æ–­ä¹Ÿä¿å­˜ç¼“å­˜ç´¢å¼•
            if crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
        except Exception as e:
            print(f"\nâœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {safe_str(e)}")
            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ç¼“å­˜ç´¢å¼•
            if crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
            if verbose:
                import traceback
                traceback.print_exc()
        finally:
            # ç¡®ä¿ç¼“å­˜ç´¢å¼•è¢«ä¿å­˜å’Œèµ„æºè¢«æ¸…ç†
            if 'crawler' in locals():
                if crawler.cache_manager:
                    crawler.cache_manager.save_cache_index()
                # æ¸…ç†çˆ¬è™«èµ„æº
                crawler.cleanup()


if __name__ == "__main__":
    main()
