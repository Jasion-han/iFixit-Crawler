#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•´åˆçˆ¬è™«å·¥å…· - ç»“åˆæ ‘å½¢ç»“æ„å’Œè¯¦ç»†å†…å®¹æå– + ä»£ç†æ±  + æœ¬åœ°æ–‡ä»¶å­˜å‚¨
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
æ”¯æŒå¤©å¯IPä»£ç†æ± ã€æœ¬åœ°æ–‡ä»¶å¤¹å­˜å‚¨ã€åª’ä½“æ–‡ä»¶ä¸‹è½½ç­‰åŠŸèƒ½
ç”¨æ³•: python auto_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python auto_crawler.py https://www.ifixit.com/Device/Television
      python auto_crawler.py Television
"""

import os
import sys
import json
import time
import random
import re
import requests
from urllib.parse import urlparse, urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
import hashlib
import mimetypes
import threading
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import queue

# å¯¼å…¥ä¸¤ä¸ªåŸºç¡€çˆ¬è™«
from enhanced_crawler import EnhancedIFixitCrawler
from tree_crawler import TreeCrawler


class ProxyManager:
    """ä»£ç†ç®¡ç†å™¨ - ä»æœ¬åœ°proxy.txtæ–‡ä»¶ç®¡ç†ä»£ç†æ± """

    def __init__(self, proxy_file="proxy.txt", username="yjnvjx", password="gkmb3obc"):
        self.proxy_file = proxy_file
        self.username = username
        self.password = password

        self.proxy_pool = []
        self.current_proxy = None
        self.failed_proxies = []
        self.proxy_switch_count = 0
        self.proxy_lock = threading.Lock()

        # åŠ è½½ä»£ç†
        self._load_proxies()

    def _load_proxies(self):
        """ä»proxy.txtæ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨"""
        proxy_list = []

        try:
            if not os.path.exists(self.proxy_file):
                print(f"âŒ ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨: {self.proxy_file}")
                return

            with open(self.proxy_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()
                if line and ':' in line and not line.startswith('#'):
                    try:
                        host, port = line.split(':', 1)
                        proxy_config = {
                            'http': f"http://{self.username}:{self.password}@{host.strip()}:{port.strip()}",
                            'https': f"http://{self.username}:{self.password}@{host.strip()}:{port.strip()}"
                        }
                        proxy_list.append(proxy_config)
                    except ValueError:
                        print(f"âš ï¸  è·³è¿‡æ ¼å¼é”™è¯¯çš„ä»£ç†: {line}")

            self.proxy_pool = proxy_list
            print(f"ğŸ“‹ ä» {self.proxy_file} åŠ è½½äº† {len(proxy_list)} ä¸ªä»£ç†")

        except Exception as e:
            print(f"âŒ è¯»å–ä»£ç†æ–‡ä»¶å¤±è´¥: {e}")

    def get_proxy(self):
        """è·å–ä¸€ä¸ªå¯ç”¨çš„ä»£ç†"""
        with self.proxy_lock:
            # å¦‚æœå½“å‰ä»£ç†å¯ç”¨ï¼Œç›´æ¥è¿”å›
            if self.current_proxy and self.current_proxy not in self.failed_proxies:
                return self.current_proxy

            # ä»ä»£ç†æ± ä¸­é€‰æ‹©ä¸€ä¸ªæœªå¤±æ•ˆçš„ä»£ç†
            available_proxies = []
            for p in self.proxy_pool:
                is_failed = False
                for failed in self.failed_proxies:
                    if p['http'] == failed['http']:
                        is_failed = True
                        break
                if not is_failed:
                    available_proxies.append(p)

            if not available_proxies:
                # å¦‚æœæ‰€æœ‰ä»£ç†éƒ½å¤±æ•ˆäº†ï¼Œé‡ç½®å¤±æ•ˆåˆ—è¡¨ï¼Œé‡æ–°å¼€å§‹
                print("âš ï¸  æ‰€æœ‰ä»£ç†éƒ½å·²å¤±æ•ˆï¼Œé‡ç½®ä»£ç†æ± ...")
                self.failed_proxies.clear()
                available_proxies = self.proxy_pool

            if available_proxies:
                self.current_proxy = random.choice(available_proxies)
                self.proxy_switch_count += 1

                # æå–IPå’Œç«¯å£ç”¨äºæ˜¾ç¤º
                proxy_url = self.current_proxy['http']
                if '@' in proxy_url:
                    ip_port = proxy_url.split('@')[1]
                    if ':' in ip_port:
                        display_ip, display_port = ip_port.rsplit(':', 1)
                    else:
                        display_ip, display_port = ip_port, "Unknown"
                else:
                    display_ip, display_port = "Unknown", "Unknown"

                current_time = time.strftime("%Y-%m-%d %H:%M:%S")
                print(f"ğŸ”„ åˆ‡æ¢ä»£ç† (ç¬¬{self.proxy_switch_count}æ¬¡): {display_ip}:{display_port}")

                return self.current_proxy
            else:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„ä»£ç†")
                return None

    def mark_proxy_failed(self, proxy, reason=""):
        """æ ‡è®°ä»£ç†ä¸ºå¤±æ•ˆ"""
        with self.proxy_lock:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å¤±æ•ˆåˆ—è¡¨ä¸­
            already_failed = False
            for failed in self.failed_proxies:
                if failed['http'] == proxy['http']:
                    already_failed = True
                    break

            if not already_failed:
                self.failed_proxies.append(proxy)

            # æå–IPå’Œç«¯å£ç”¨äºæ˜¾ç¤º
            proxy_url = proxy['http']
            if '@' in proxy_url:
                ip_port = proxy_url.split('@')[1]
                if ':' in ip_port:
                    display_ip, display_port = ip_port.rsplit(':', 1)
                else:
                    display_ip, display_port = ip_port, "Unknown"
            else:
                display_ip, display_port = "Unknown", "Unknown"

            print(f"âŒ ä»£ç†å¤±æ•ˆ: {display_ip}:{display_port} - {reason}")

            # å¦‚æœå¤±æ•ˆçš„æ˜¯å½“å‰ä»£ç†ï¼Œæ¸…é™¤å½“å‰ä»£ç†
            if self.current_proxy == proxy:
                self.current_proxy = None

    def get_stats(self):
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_proxies': len(self.proxy_pool),
            'failed_proxies': len(self.failed_proxies),
            'available_proxies': len(self.proxy_pool) - len(self.failed_proxies),
            'switch_count': self.proxy_switch_count
        }

class CombinedIFixitCrawler(EnhancedIFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", verbose=False, use_proxy=True,
                 use_cache=True, force_refresh=False, max_workers=4, max_retries=5,
                 download_videos=False, max_video_size_mb=50):
        super().__init__(base_url, verbose)

        # ç«‹å³åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œç¡®ä¿loggerå¯ç”¨
        self._setup_logging()

        self.tree_crawler = TreeCrawler(base_url)
        self.processed_nodes = set()
        self.target_url = None

        # ä»£ç†æ± é…ç½®
        self.use_proxy = use_proxy
        self.proxy_manager = ProxyManager() if use_proxy else None
        self.failed_urls = set()  # æ·»åŠ å¤±è´¥URLé›†åˆ

        # è§†é¢‘å¤„ç†é…ç½®
        self.download_videos = download_videos
        self.max_video_size_mb = max_video_size_mb
        self.video_extensions = ['.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv']

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "retry_success": 0,
            "retry_failed": 0,
            "media_downloaded": 0,
            "media_failed": 0,
            "videos_skipped": 0,
            "videos_downloaded": 0
        }

        # æœ¬åœ°å­˜å‚¨é…ç½®
        self.storage_root = "ifixit_data"
        self.media_folder = "media"

        # ç¼“å­˜é…ç½®
        self.use_cache = use_cache
        self.force_refresh = force_refresh

        # å¹¶å‘é…ç½®
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.thread_local = threading.local()

        # é”™è¯¯å¤„ç†é…ç½®
        self.failed_log_file = "failed_urls.log"

        # æ‰“å°è§†é¢‘å¤„ç†é…ç½®
        if self.download_videos:
            self.logger.info(f"âœ… è§†é¢‘ä¸‹è½½å·²å¯ç”¨ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {max_video_size_mb}MB")
        else:
            self.logger.info("âš ï¸ è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå°†ä¿ç•™åŸå§‹URL")

    def _load_proxy_config(self):
        """åŠ è½½ä»£ç†é…ç½®"""
        print("ğŸ“‹ ä½¿ç”¨å¤©å¯IPä»£ç†æœåŠ¡")

        # å¤©å¯IPä»£ç†è®¤è¯ä¿¡æ¯
        self.proxy_username = "yjnvjx"  # æ‚¨çš„å¤©å¯IPç”¨æˆ·å
        self.proxy_password = "gkmb3obc"  # æ‚¨çš„å¤©å¯IPå¯†ç 

        self.current_proxy = None
        self.proxy_switch_count = 0
        self.failed_urls = set()

        # è¿™äº›é…ç½®å·²åœ¨__init__ä¸­è®¾ç½®ï¼Œæ­¤å¤„ä¸å†é‡å¤

        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()

        if self.use_proxy:
            self._init_proxy_pool()

        # å¤åˆ¶æ ‘å½¢çˆ¬è™«çš„é‡è¦æ–¹æ³•åˆ°å½“å‰ç±»
        self.find_exact_path = self.tree_crawler.find_exact_path
        self.extract_breadcrumbs_from_page = self.tree_crawler.extract_breadcrumbs_from_page
        self._build_tree_from_path = self.tree_crawler._build_tree_from_path
        self._find_node_by_url = self.tree_crawler._find_node_by_url
        self.ensure_instruction_url_in_leaf_nodes = self.tree_crawler.ensure_instruction_url_in_leaf_nodes

        # é‡å†™tree_crawlerçš„get_soupæ–¹æ³•ï¼Œè®©å®ƒä½¿ç”¨æˆ‘ä»¬çš„å¢å¼ºç‰ˆæœ¬
        self.tree_crawler.get_soup = self._tree_crawler_get_soup

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO if self.verbose else logging.WARNING,
            format=log_format,
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _check_cache_validity(self, url, local_path):
        """æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not self.use_cache or self.force_refresh:
            return False

        if not local_path.exists():
            return False

        # æ£€æŸ¥info.jsonæ˜¯å¦å­˜åœ¨
        info_file = local_path / "info.json"
        if not info_file.exists():
            self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘info.json - {url}")
            return False

        try:
            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            # æ£€æŸ¥guidesç›®å½•
            if 'guides' in info_data:
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘guidesç›®å½• - {url}")
                    return False

            # æ£€æŸ¥troubleshootingç›®å½•
            if 'troubleshooting' in info_data:
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘troubleshootingç›®å½• - {url}")
                    return False

            # æ£€æŸ¥mediaæ–‡ä»¶
            media_dir = local_path / self.media_folder
            if media_dir.exists():
                # ç®€å•æ£€æŸ¥ï¼šå¦‚æœmediaç›®å½•å­˜åœ¨ä½†ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ä¸‹è½½å¤±è´¥
                media_files = list(media_dir.glob("*"))
                if len(media_files) == 0:
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶
                    if self._should_have_media(info_data):
                        self.logger.info(f"ç¼“å­˜æ— æ•ˆ: mediaç›®å½•ä¸ºç©ºä½†åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶ - {url}")
                        return False

            self.logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡çˆ¬å–: {url}")
            return True

        except Exception as e:
            self.logger.error(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e} - {url}")
            return False

    def _should_have_media(self, data):
        """æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åº”è¯¥åŒ…å«åª’ä½“æ–‡ä»¶"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and value:
                    return True
                elif key == 'images' and isinstance(value, list) and value:
                    return True
                elif isinstance(value, (dict, list)):
                    if self._should_have_media(value):
                        return True
        elif isinstance(data, list):
            for item in data:
                if self._should_have_media(item):
                    return True
        return False

    def _log_failed_url(self, url, error, retry_count=0):
        """è®°å½•å¤±è´¥çš„URLåˆ°æ—¥å¿—æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"{timestamp} | {url} | {error} | retry_count: {retry_count}\n"

        try:
            with open(self.failed_log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
        except Exception as e:
            self.logger.error(f"æ— æ³•å†™å…¥å¤±è´¥æ—¥å¿—: {e}")

    def _exponential_backoff(self, attempt):
        """æŒ‡æ•°é€€é¿å»¶è¿Ÿ"""
        delay = min(300, (2 ** attempt) + random.uniform(0, 1))
        time.sleep(delay)
        return delay

    def _is_temporary_error(self, error):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶æ€§é”™è¯¯"""
        temporary_errors = [
            "timeout", "connection", "network", "502", "503", "504",
            "429", "500", "ConnectionError", "Timeout", "ReadTimeout"
        ]
        error_str = str(error).lower()
        return any(temp_err in error_str for temp_err in temporary_errors)

    def _retry_with_backoff(self, func, *args, **kwargs):
        """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶"""
        last_error = None

        for attempt in range(self.max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    self.stats["retry_success"] += 1
                    self.logger.info(f"é‡è¯•æˆåŠŸ (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                return result

            except Exception as e:
                last_error = e
                self.stats["total_retries"] += 1

                if not self._is_temporary_error(e):
                    self.logger.error(f"æ°¸ä¹…æ€§é”™è¯¯ï¼Œåœæ­¢é‡è¯•: {e}")
                    break

                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    self.logger.warning(f"é‡è¯• {attempt+1}/{self.max_retries} (å»¶è¿Ÿ{delay:.1f}s): {e}")

                    # ç½‘ç»œé”™è¯¯æ—¶åˆ‡æ¢ä»£ç†
                    if self.use_proxy and "network" in str(e).lower():
                        self._switch_proxy(f"é‡è¯•æ—¶ç½‘ç»œé”™è¯¯: {str(e)}")
                else:
                    self.logger.error(f"é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")

        self.stats["retry_failed"] += 1
        if last_error:
            raise last_error
        else:
            raise Exception("é‡è¯•å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯")

    def _init_proxy_pool(self):
        """åˆå§‹åŒ–ä»£ç†æ±  - ä»å¤©å¯IP APIè·å–ä»£ç†"""
        print("=" * 50)
        print("ğŸš€ å¯åŠ¨ä»£ç†æ± ç³»ç»Ÿ")
        print("=" * 50)
        self._get_new_proxy()

    def _get_new_proxy(self):
        """ä»å¤©å¯IP APIè·å–æ–°çš„ä»£ç†IP"""
        api_url = "http://api.tianqiip.com/getip"
        params = {
            'secret': 'c3ljdpezjim64de5',
            'num': '200',
            'type': 'txt',
            'port': '2',
            'time': '3',
            'mr': '1',
            'sign': '2704663b3f023a2e4097093d70f24acb'
        }

        try:
            print(f"ğŸ“¡ æ­£åœ¨ä»å¤©å¯IP APIè·å–ä»£ç†...")
            print(f"ğŸ”— APIåœ°å€: {api_url}")

            response = requests.get(api_url, params=params, timeout=15)

            if response.status_code == 200:
                proxy_text = response.text.strip()
                print(f"ğŸ” APIå“åº”å†…å®¹é¢„è§ˆ: {proxy_text[:200]}...")

                # è§£ææ–‡æœ¬æ ¼å¼çš„ä»£ç†åˆ—è¡¨ (IP:PORTæ ¼å¼)
                proxy_lines = [line.strip() for line in proxy_text.split('\n') if line.strip()]

                if proxy_lines:
                    # éšæœºé€‰æ‹©ä¸€ä¸ªä»£ç†
                    import random
                    selected_proxy = random.choice(proxy_lines)

                    if ':' in selected_proxy:
                        proxy_host, proxy_port = selected_proxy.split(':', 1)

                        # æ„å»ºå¸¦è®¤è¯çš„ä»£ç†é…ç½®
                        proxy_meta = f"http://{self.proxy_username}:{self.proxy_password}@{proxy_host}:{proxy_port}"
                        self.current_proxy = {
                            'http': proxy_meta,
                            'https': proxy_meta
                        }

                        self.proxy_switch_count += 1
                        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

                        print("âœ… å¤©å¯IPä»£ç†è·å–æˆåŠŸ!")
                        print(f"ğŸ•’ æ—¶é—´: {current_time}")
                        print(f"ğŸŒ ä»£ç†IP: {proxy_host}")
                        print(f"ğŸ”Œ ç«¯å£: {proxy_port}")
                        print(f"ğŸ”„ åˆ‡æ¢æ¬¡æ•°: ç¬¬ {self.proxy_switch_count} æ¬¡")
                        print(f"ğŸ“Š å¯ç”¨ä»£ç†æ€»æ•°: {len(proxy_lines)}")
                        print("=" * 50)

                        # ä»£ç†è·å–æˆåŠŸ

                        return True
                    else:
                        print(f"âŒ ä»£ç†æ ¼å¼é”™è¯¯: {selected_proxy}")
                else:
                    print("âŒ æœªè·å–åˆ°æœ‰æ•ˆçš„ä»£ç†åˆ—è¡¨")
            else:
                print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                print(f"âŒ å“åº”å†…å®¹: {response.text[:200]}")

        except Exception as e:
            print(f"âŒ è·å–å¤©å¯IPä»£ç†å¤±è´¥: {e}")

        # APIå¤±è´¥æ—¶ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼
        print("âš ï¸  å¤©å¯IP APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. å¤©å¯IPè´¦å·æ˜¯å¦æœ‰æ•ˆ")
        print("   2. è´¦å·ä½™é¢æ˜¯å¦å……è¶³")
        print("   3. APIå‚æ•°æ˜¯å¦æ­£ç¡®")
        print("   4. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        return self._get_demo_proxy()

    def _get_demo_proxy(self):
        """è·å–æ¨¡æ‹Ÿä»£ç†ç”¨äºæ¼”ç¤º"""
        import random

        # æ¨¡æ‹Ÿä»£ç†IPæ± 
        demo_proxies = [
            {"ip": "192.168.1.100", "port": "8080"},
            {"ip": "10.0.0.50", "port": "3128"},
            {"ip": "172.16.0.25", "port": "8888"},
        ]

        proxy_info = random.choice(demo_proxies)
        proxy_host = proxy_info['ip']
        proxy_port = proxy_info['port']

        # æ¼”ç¤ºæ¨¡å¼ä¹Ÿä½¿ç”¨è®¤è¯ä¿¡æ¯
        proxy_meta = f"http://{self.proxy_username}:{self.proxy_password}@{proxy_host}:{proxy_port}"
        self.current_proxy = {
            'http': proxy_meta,
            'https': proxy_meta
        }

        self.proxy_switch_count += 1
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

        print("âœ… æ¨¡æ‹Ÿä»£ç†è·å–æˆåŠŸ! (æ¼”ç¤ºæ¨¡å¼)")
        print(f"ğŸ•’ æ—¶é—´: {current_time}")
        print(f"ğŸŒ ä»£ç†IP: {proxy_host}")
        print(f"ğŸ”Œ ç«¯å£: {proxy_port}")
        print(f"ğŸ”„ åˆ‡æ¢æ¬¡æ•°: ç¬¬ {self.proxy_switch_count} æ¬¡")
        print("ğŸ’¡ æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œå®é™…ä½¿ç”¨éœ€è¦é…ç½®çœŸå®çš„å¤©å¯IP API")
        print("=" * 50)
        return True



    def _get_next_proxy(self):
        """è·å–å½“å‰ä»£ç†æˆ–åˆ‡æ¢æ–°ä»£ç†"""
        if not self.use_proxy or not self.proxy_manager:
            return None

        return self.proxy_manager.get_proxy()

    def _switch_proxy(self, reason="è¯·æ±‚å¤±è´¥"):
        """åˆ‡æ¢ä»£ç†IP"""
        self.logger.warning(f"âš ï¸  ä»£ç†åˆ‡æ¢åŸå› : {reason}")

        if not self.use_proxy or not self.proxy_manager:
            return False

        # æ ‡è®°å½“å‰ä»£ç†ä¸ºå¤±æ•ˆ
        current_proxy = self.proxy_manager.current_proxy
        if current_proxy:
            self.proxy_manager.mark_proxy_failed(current_proxy, reason)

        # è·å–æ–°ä»£ç†
        new_proxy = self.proxy_manager.get_proxy()
        return new_proxy is not None

    # ä»£ç†æ± ç›¸å…³æ–¹æ³•å·²é›†æˆåˆ°ProxyManagerç±»ä¸­



    def get_soup(self, url, use_playwright=False):
        """é‡å†™get_soupæ–¹æ³•ï¼Œæ”¯æŒä»£ç†æ± ã€é‡è¯•æœºåˆ¶å’ŒPlaywrightæ¸²æŸ“"""
        if not url:
            return None

        if url in self.failed_urls:
            self.logger.warning(f"è·³è¿‡å·²çŸ¥å¤±è´¥URL: {url}")
            return None

        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        if '?' in url:
            url += '&lang=en'
        else:
            url += '?lang=en'

        self.stats["total_requests"] += 1

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return self._retry_with_backoff(self._get_soup_with_playwright, url)

        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„requestsæ–¹æ³•
        return self._retry_with_backoff(self._get_soup_requests, url)

    def _get_soup_requests(self, url):
        """ä½¿ç”¨requestsè·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒæ™ºèƒ½ä»£ç†åˆ‡æ¢"""
        session = requests.Session()
        retry_strategy = Retry(
            total=2,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è·å–ä»£ç†
        proxies = self._get_next_proxy() if self.use_proxy else None

        try:
            response = session.get(
                url,
                headers=self.headers,
                timeout=30,
                proxies=proxies
            )
            response.raise_for_status()

            from bs4 import BeautifulSoup
            return BeautifulSoup(response.content, 'html.parser')

        except (requests.exceptions.ProxyError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ConnectionError) as e:
            # ä»£ç†ç›¸å…³é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†
            if self.use_proxy:
                print(f"ğŸ”„ ä»£ç†é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢: {str(e)[:100]}")
                if self._switch_proxy(f"ä»£ç†é”™è¯¯: {str(e)[:50]}"):
                    # ä½¿ç”¨æ–°ä»£ç†é‡è¯•ä¸€æ¬¡
                    new_proxies = self._get_next_proxy()
                    response = session.get(
                        url,
                        headers=self.headers,
                        timeout=30,
                        proxies=new_proxies
                    )
                    response.raise_for_status()

                    from bs4 import BeautifulSoup
                    return BeautifulSoup(response.content, 'html.parser')
            # å¦‚æœåˆ‡æ¢ä»£ç†å¤±è´¥æˆ–ä¸ä½¿ç”¨ä»£ç†ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸
            raise

    def _get_soup_with_playwright(self, url):
        """ä½¿ç”¨Playwrightè·å–JavaScriptæ¸²æŸ“åçš„é¡µé¢å†…å®¹"""
        try:
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                page.goto(url, wait_until='networkidle')

                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            print(f"Playwrightè·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return None

    def _create_local_directory(self, path):
        """åˆ›å»ºæœ¬åœ°ç›®å½•ç»“æ„"""
        full_path = Path(self.storage_root) / path
        full_path.mkdir(parents=True, exist_ok=True)
        return full_path

    def _get_file_size_from_url(self, url):
        """è·å–URLæ–‡ä»¶çš„å¤§å°ï¼ˆMBï¼‰"""
        try:
            proxies = self._get_next_proxy() if self.use_proxy else None
            response = requests.head(url, headers=self.headers, timeout=10, proxies=proxies)
            if response.status_code == 200:
                content_length = response.headers.get('content-length')
                if content_length:
                    size_bytes = int(content_length)
                    size_mb = size_bytes / (1024 * 1024)
                    return size_mb
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° {url}: {e}")
        return None

    def _is_video_file(self, url):
        """æ£€æŸ¥URLæ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
        if not url:
            return False
        url_path = url.split('?')[0].lower()
        return any(url_path.endswith(ext) for ext in self.video_extensions)

    def _download_media_file(self, url, local_dir, filename=None):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œè§†é¢‘å¤„ç†ç­–ç•¥ï¼‰"""
        if not url or not url.startswith('http'):
            return None

        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
        if self._is_video_file(url):
            if not self.download_videos:
                self.logger.info(f"è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰: {url}")
                self.stats["videos_skipped"] += 1
                return url

            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size_mb = self._get_file_size_from_url(url)
            if file_size_mb and file_size_mb > self.max_video_size_mb:
                self.logger.warning(f"è·³è¿‡å¤§è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB > {self.max_video_size_mb}MB): {url}")
                self.stats["videos_skipped"] += 1
                return url

            self.logger.info(f"å‡†å¤‡ä¸‹è½½è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB): {url}")

        try:
            result = self._retry_with_backoff(self._download_media_file_impl, url, local_dir, filename)
            if self._is_video_file(url) and result != url:
                self.stats["videos_downloaded"] += 1
            return result
        except Exception as e:
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æœ€ç»ˆå¤±è´¥ {url}: {e}")
            self.stats["media_failed"] += 1
            self._log_failed_url(url, f"åª’ä½“ä¸‹è½½å¤±è´¥: {str(e)}")
            return url

    def _download_media_file_impl(self, url, local_dir, filename=None):
        """åª’ä½“æ–‡ä»¶ä¸‹è½½çš„å…·ä½“å®ç°"""
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        local_path = local_dir / self.media_folder / filename
        local_path.parent.mkdir(parents=True, exist_ok=True)

        if local_path.exists():
            self.stats["media_downloaded"] += 1
            return str(local_path.relative_to(Path(self.storage_root)))

        proxies = self._get_next_proxy() if self.use_proxy else None
        response = requests.get(url, headers=self.headers, timeout=30, proxies=proxies)
        response.raise_for_status()

        with open(local_path, 'wb') as f:
            f.write(response.content)

        self.stats["media_downloaded"] += 1
        self.logger.info(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename}")
        return str(local_path.relative_to(Path(self.storage_root)))

    def _process_media_urls(self, data, local_dir):
        """é€’å½’å¤„ç†æ•°æ®ä¸­çš„åª’ä½“URLï¼Œä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                    data[key] = self._download_media_file(value, local_dir)
                elif key == 'images' and isinstance(value, list):
                    for i, img_url in enumerate(value):
                        if isinstance(img_url, str) and img_url.startswith('http'):
                            value[i] = self._download_media_file(img_url, local_dir)
                elif isinstance(value, (dict, list)):
                    self._process_media_urls(value, local_dir)
        elif isinstance(data, list):
            for item in data:
                self._process_media_urls(item, local_dir)

    def _tree_crawler_get_soup(self, url):
        """ä¸ºtree_crawleræä¾›çš„get_soupæ–¹æ³•ï¼Œåœ¨éœ€è¦é¢åŒ…å±‘å¯¼èˆªæ—¶ä½¿ç”¨Playwright"""
        # å¯¹äºéœ€è¦æå–é¢åŒ…å±‘å¯¼èˆªçš„é¡µé¢ï¼Œä½¿ç”¨Playwright
        if '/Teardown/' in url or '/Guide/' in url or '/Device/' in url:
            return self.get_soup(url, use_playwright=True)
        else:
            return self.get_soup(url, use_playwright=False)

    def extract_real_name_from_url(self, url):
        """
        ä»URLä¸­æå–çœŸå®çš„nameå­—æ®µï¼ˆURLè·¯å¾„çš„æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰
        """
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_url = url.split('?')[0].split('#')[0]

        # ç§»é™¤æœ«å°¾çš„æ–œæ 
        clean_url = clean_url.rstrip('/')

        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µ
        path_parts = clean_url.split('/')
        if path_parts:
            name = path_parts[-1]
            # URLè§£ç 
            import urllib.parse
            name = urllib.parse.unquote(name)
            # ä¿®å¤å¼•å·ç¼–ç é—®é¢˜ï¼šå°† \" è½¬æ¢ä¸º "
            name = name.replace('\\"', '"')
            return name

        return ""

    def extract_real_title_from_page(self, soup):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ
        """
        if not soup:
            return ""

        # ä¼˜å…ˆä»h1æ ‡ç­¾æå–
        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        # å…¶æ¬¡ä»titleæ ‡ç­¾æå–
        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    def extract_real_title_from_page(self, soup):
        """ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ"""
        if not soup:
            return ""

        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    # ç¬¬ä¸€ä¸ªextract_real_view_statisticsæ–¹æ³•å·²ç§»é™¤ï¼Œä½¿ç”¨ä¸‹é¢çš„å®ç°

    def _is_specified_target_url(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        """
        if not self.target_url or not url:
            return False

        # æ ‡å‡†åŒ–URLè¿›è¡Œæ¯”è¾ƒ
        def normalize_url(u):
            import urllib.parse
            # URLè§£ç ï¼Œç„¶åæ ‡å‡†åŒ–
            decoded = urllib.parse.unquote(u)
            return decoded.rstrip('/').lower()

        return normalize_url(url) == normalize_url(self.target_url)

    def discover_subcategories_from_page(self, soup, base_url):
        """
        åŠ¨æ€å‘ç°é¡µé¢ä¸­çš„å­ç±»åˆ«é“¾æ¥
        """
        if not soup:
            return []

        subcategories = []

        # æŸ¥æ‰¾è®¾å¤‡ç±»åˆ«é“¾æ¥çš„å¤šç§é€‰æ‹©å™¨
        category_selectors = [
            'a[href*="/Device/"]',  # åŒ…å«/Device/çš„é“¾æ¥
            '.category-link',       # ç±»åˆ«é“¾æ¥ç±»
            '.device-link',         # è®¾å¤‡é“¾æ¥ç±»
            '.subcategory',         # å­ç±»åˆ«ç±»
            'a[href^="/Device/"]'   # ä»¥/Device/å¼€å¤´çš„é“¾æ¥
        ]

        found_links = set()

        for selector in category_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/Device/' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
                    skip_patterns = [
                        '/Guide/', '/Troubleshooting/', '/Edit/', '/History/',
                        '/Answers/', '?revision', 'action=edit', 'action=create'
                    ]

                    if not any(pattern in full_url for pattern in skip_patterns):
                        # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå­ç±»åˆ«ï¼ˆURLæ¯”å½“å‰é¡µé¢æ›´æ·±ä¸€å±‚ï¼‰
                        if full_url != base_url and full_url not in found_links:
                            # æå–é“¾æ¥æ–‡æœ¬ä½œä¸ºåç§°
                            link_text = link.get_text().strip()
                            if link_text:
                                subcategories.append({
                                    'name': self.extract_real_name_from_url(full_url),
                                    'url': full_url,
                                    'title': link_text
                                })
                                found_links.add(full_url)

        return subcategories

    def extract_real_view_statistics(self, soup, url):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®
        """
        if not soup:
            return {}

        stats = {}

        # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®çš„å„ç§å¯èƒ½ä½ç½®
        stat_selectors = [
            '.view-count',
            '.stats-item',
            '.view-stats',
            '[data-stats]',
            '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # è§£æç»Ÿè®¡æ•°æ®æ ¼å¼
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»APIæˆ–å…¶ä»–ä½ç½®è·å–
        if not stats:
            # å°è¯•ä»é¡µé¢çš„JavaScriptæ•°æ®ä¸­æå–
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        # æŸ¥æ‰¾å¯èƒ½çš„ç»Ÿè®¡æ•°æ®
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def build_correct_url_from_path(self, base_url, path_segments):
        """
        æ ¹æ®è·¯å¾„æ®µæ­£ç¡®æ„å»ºURLï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
        """
        if not path_segments:
            return base_url

        # ä»base_urlå¼€å§‹ï¼Œé€æ­¥æ›¿æ¢è·¯å¾„æ®µ
        current_url = base_url

        for segment in path_segments:
            # è·å–å½“å‰URLçš„è·¯å¾„éƒ¨åˆ†
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(current_url)
            path_parts = [p for p in str(parsed.path).split('/') if p]

            # æ·»åŠ æ–°çš„è·¯å¾„æ®µ
            path_parts.append(str(segment))

            # é‡æ–°æ„å»ºURL
            new_path = '/' + '/'.join(path_parts)
            new_parsed = parsed._replace(path=new_path)
            current_url = urlunparse(new_parsed)

        return current_url

    def restructure_target_content(self, node, is_target_page=False):
        """
        é‡æ„ç›®æ ‡æ–‡ä»¶çš„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨guides[]å’Œtroubleshooting[]è€Œéchildren[]
        """
        if not node or not isinstance(node, dict):
            return node

        # å¦‚æœè¿™æ˜¯ç›®æ ‡é¡µé¢ä¸”æœ‰childrenåŒ…å«guideså’Œtroubleshooting
        if is_target_page and 'children' in node and node['children']:
            guides = []
            troubleshooting = []
            real_children = []

            for child in node['children']:
                if 'steps' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæŒ‡å—
                    guide_data = {k: v for k, v in child.items() if k not in ['children']}
                    guides.append(guide_data)
                elif 'causes' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæ•…éšœæ’é™¤
                    ts_data = {k: v for k, v in child.items() if k not in ['children']}
                    troubleshooting.append(ts_data)
                else:
                    # è¿™æ˜¯çœŸæ­£çš„å­ç±»åˆ«
                    real_children.append(child)

            # é‡æ„èŠ‚ç‚¹
            if guides:
                node['guides'] = guides
            if troubleshooting:
                node['troubleshooting'] = troubleshooting

            # åªæœ‰çœŸæ­£çš„å­ç±»åˆ«æ‰ä¿ç•™åœ¨childrenä¸­
            if real_children:
                node['children'] = real_children
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„å­ç±»åˆ«ï¼Œç§»é™¤childrenå­—æ®µ
                if 'children' in node:
                    del node['children']

        return node

    def fix_node_data(self, node, soup=None):
        """ä¿®å¤èŠ‚ç‚¹çš„nameå­—æ®µï¼Œåªåœ¨æœ€ç»ˆäº§å“é¡µé¢æ·»åŠ å…¶ä»–å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_root_device = url == f"{self.base_url}/Device"

        # æ‰€æœ‰èŠ‚ç‚¹éƒ½ä¿®å¤nameå­—æ®µ
        real_name = self.extract_real_name_from_url(url)
        if real_name:
            node['name'] = real_name

        # åªæœ‰åœ¨deep_crawl_product_contentä¸­è¢«è¯†åˆ«ä¸ºç›®æ ‡é¡µé¢æ—¶æ‰æ·»åŠ å…¶ä»–å­—æ®µ
        # è¿™é‡Œä¸æ·»åŠ titleã€instruction_urlã€view_statisticsç­‰å­—æ®µ
        # è¿™äº›å­—æ®µå°†åœ¨deep_crawl_product_contentæ–¹æ³•ä¸­æ·»åŠ 

        return node
        
    def crawl_combined_tree(self, start_url, category_name=None):
        """
        æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # è®¾ç½®ç›®æ ‡URL
        self.target_url = start_url

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
        print("1. æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
        base_tree = self.tree_crawler.crawl_tree(start_url, category_name)

        if not base_tree:
            print("æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
            return None

        print(f"åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆï¼Œå¼€å§‹æå–è¯¦ç»†å†…å®¹...")
        print("=" * 60)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        print("2. ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
        enriched_tree = self.enrich_tree_with_detailed_content(base_tree)

        # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
        print("3. æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
        final_tree = self.deep_crawl_product_content(enriched_tree)

        return final_tree
        
    def enrich_tree_with_detailed_content(self, node):
        """ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ­£ç¡®çš„å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            return node

        # æ£€æŸ¥ç¼“å­˜
        node_name = node.get('name', 'unknown')
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")
        local_path = Path(self.storage_root) / safe_name

        if self._check_cache_validity(url, local_path):
            self.stats["cache_hits"] += 1
            return node

        self.processed_nodes.add(url)
        self.stats["cache_misses"] += 1

        time.sleep(random.uniform(0.5, 1.0))
        soup = self.get_soup(url)

        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½ç»è¿‡fix_node_dataå¤„ç†
        node = self.fix_node_data(node, soup)

        if self.verbose:
            self.logger.info(f"å¤„ç†èŠ‚ç‚¹: {node.get('name', '')} - {url}")
        else:
            print(f"å¤„ç†: {node.get('name', '')}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            enriched_children = []
            for child in node['children']:
                enriched_child = self.enrich_tree_with_detailed_content(child)
                if enriched_child:
                    enriched_children.append(enriched_child)
            node['children'] = enriched_children

        return node

    def _process_content_concurrently(self, guide_links, device_url, skip_troubleshooting, soup):
        """å¹¶å‘å¤„ç†guideså’Œtroubleshootingå†…å®¹"""
        guides_data = []
        troubleshooting_data = []

        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []

        # æ·»åŠ guideä»»åŠ¡
        for guide_link in guide_links:
            tasks.append(('guide', guide_link))

        # æ·»åŠ troubleshootingä»»åŠ¡
        if not skip_troubleshooting:
            troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, device_url)
            print(f"  æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢")

            for ts_link in troubleshooting_links:
                if isinstance(ts_link, dict):
                    ts_url = ts_link.get('url', '')
                else:
                    ts_url = ts_link
                if ts_url:
                    tasks.append(('troubleshooting', ts_url))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        if self.max_workers > 1 and len(tasks) > 1:
            with ThreadPoolExecutor(max_workers=min(self.max_workers, len(tasks))) as executor:
                future_to_task = {}

                for task_type, url in tasks:
                    if task_type == 'guide':
                        future = executor.submit(self._process_guide_task, url)
                    else:  # troubleshooting
                        future = executor.submit(self._process_troubleshooting_task, url)
                    future_to_task[future] = (task_type, url)

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task_type, url = future_to_task[future]
                    try:
                        result = future.result()
                        if result:
                            if task_type == 'guide':
                                guides_data.append(result)
                                print(f"    âœ“ æŒ‡å—: {result.get('title', '')}")
                            else:
                                troubleshooting_data.append(result)
                                print(f"    âœ“ æ•…éšœæ’é™¤: {result.get('title', '')}")
                    except Exception as e:
                        self.logger.error(f"å¹¶å‘ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")
        else:
            # å•çº¿ç¨‹å¤„ç†
            for task_type, url in tasks:
                try:
                    if task_type == 'guide':
                        result = self._process_guide_task(url)
                        if result:
                            guides_data.append(result)
                            print(f"    âœ“ æŒ‡å—: {result.get('title', '')}")
                    else:
                        result = self._process_troubleshooting_task(url)
                        if result:
                            troubleshooting_data.append(result)
                            print(f"    âœ“ æ•…éšœæ’é™¤: {result.get('title', '')}")
                except Exception as e:
                    self.logger.error(f"ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")

        return guides_data, troubleshooting_data

    def _process_guide_task(self, guide_url):
        """å¤„ç†å•ä¸ªguideä»»åŠ¡"""
        try:
            guide_content = self.extract_guide_content(guide_url)
            if guide_content:
                guide_content['url'] = guide_url
                return guide_content
        except Exception as e:
            self.logger.error(f"å¤„ç†guideå¤±è´¥ {guide_url}: {e}")
            self._log_failed_url(guide_url, f"Guideå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_troubleshooting_task(self, ts_url):
        """å¤„ç†å•ä¸ªtroubleshootingä»»åŠ¡"""
        try:
            ts_content = self.extract_troubleshooting_content(ts_url)
            if ts_content:
                ts_content['url'] = ts_url
                return ts_content
        except Exception as e:
            self.logger.error(f"å¤„ç†troubleshootingå¤±è´¥ {ts_url}: {e}")
            self._log_failed_url(ts_url, f"Troubleshootingå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)

        total_requests = self.stats["total_requests"]
        cache_hits = self.stats["cache_hits"]
        cache_misses = self.stats["cache_misses"]

        if total_requests > 0:
            cache_hit_rate = (cache_hits / (cache_hits + cache_misses)) * 100 if (cache_hits + cache_misses) > 0 else 0
            print(f"ğŸŒ æ€»è¯·æ±‚æ•°: {total_requests}")
            print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_hits} ({cache_hit_rate:.1f}%)")
            print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­: {cache_misses}")

        retry_success = self.stats["retry_success"]
        retry_failed = self.stats["retry_failed"]
        total_retries = retry_success + retry_failed

        if total_retries > 0:
            retry_success_rate = (retry_success / total_retries) * 100
            print(f"ğŸ” é‡è¯•æˆåŠŸ: {retry_success}/{total_retries} ({retry_success_rate:.1f}%)")
            print(f"âŒ é‡è¯•å¤±è´¥: {retry_failed}")

        media_downloaded = self.stats["media_downloaded"]
        media_failed = self.stats["media_failed"]
        total_media = media_downloaded + media_failed

        if total_media > 0:
            media_success_rate = (media_downloaded / total_media) * 100
            print(f"ğŸ“ åª’ä½“ä¸‹è½½æˆåŠŸ: {media_downloaded}/{total_media} ({media_success_rate:.1f}%)")
            print(f"ğŸ“ åª’ä½“ä¸‹è½½å¤±è´¥: {media_failed}")

        # è§†é¢‘å¤„ç†ç»Ÿè®¡
        videos_downloaded = self.stats.get("videos_downloaded", 0)
        videos_skipped = self.stats.get("videos_skipped", 0)
        total_videos = videos_downloaded + videos_skipped

        if total_videos > 0:
            print(f"ğŸ¥ è§†é¢‘æ–‡ä»¶å¤„ç†:")
            print(f"   âœ… å·²ä¸‹è½½: {videos_downloaded}")
            print(f"   â­ï¸ å·²è·³è¿‡: {videos_skipped}")
            if not self.download_videos:
                print(f"   ğŸ’¡ æç¤º: è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå¦‚éœ€ä¸‹è½½è¯·è®¾ç½® download_videos=True")
            else:
                print(f"   ğŸ“ å¤§å°é™åˆ¶: {self.max_video_size_mb}MB")

        if self.use_proxy and hasattr(self, 'proxy_switch_count'):
            print(f"ğŸ”„ ä»£ç†åˆ‡æ¢æ¬¡æ•°: {self.proxy_switch_count}")

        print("=" * 60)
        


    def count_detailed_nodes(self, node):
        """
        ç»Ÿè®¡æ ‘ä¸­åŒ…å«è¯¦ç»†å†…å®¹çš„èŠ‚ç‚¹æ•°é‡
        """
        count = {'guides': 0, 'troubleshooting': 0, 'products': 0, 'categories': 0}

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„guideså’Œtroubleshootingæ•°ç»„
        if 'guides' in node and isinstance(node['guides'], list):
            count['guides'] += len(node['guides'])

        if 'troubleshooting' in node and isinstance(node['troubleshooting'], list):
            count['troubleshooting'] += len(node['troubleshooting'])

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹
        if 'title' in node and 'steps' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹ï¼ˆä¸åœ¨guidesæ•°ç»„ä¸­ï¼‰
            count['guides'] += 1
        elif 'causes' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹ï¼ˆä¸åœ¨troubleshootingæ•°ç»„ä¸­ï¼‰
            count['troubleshooting'] += 1
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            count['products'] += 1
        else:
            # åˆ†ç±»èŠ‚ç‚¹
            count['categories'] += 1

        # é€’å½’ç»Ÿè®¡å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for child in node['children']:
                child_count = self.count_detailed_nodes(child)
                for key in count:
                    count[key] += child_count[key]

        return count

    def extract_guides_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æŒ‡å—é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§é¡µé¢å¸ƒå±€
        """
        guides = []
        seen_guide_urls = set()

        if not soup:
            return guides

        # æŸ¥æ‰¾æ‰€æœ‰æŒ‡å—é“¾æ¥ï¼ˆåŒ…æ‹¬Guideå’ŒTeardownï¼‰
        guide_links = soup.select("a[href*='/Guide/'], a[href*='/Teardown/']")

        for link in guide_links:
            href = link.get("href")
            text = link.get_text().strip()

            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
            skip_patterns = [
                "Create Guide", "åˆ›å»ºæŒ‡å—", "Guide/new", "Guide/edit", "/edit/",
                "Guide/history", "/history/", "Edit Guide", "ç¼–è¾‘æŒ‡å—",
                "Version History", "ç‰ˆæœ¬å†å²", "Teardown/edit", "#comment", "permalink=comment"
            ]

            if href and text and not any(pattern in href or pattern in text for pattern in skip_patterns):
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_guide_urls:
                    seen_guide_urls.add(href)
                    guides.append({
                        "title": text,
                        "url": href
                    })

        return guides

    def extract_troubleshooting_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æ•…éšœæ’é™¤é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬
        """
        troubleshooting_links = []
        seen_ts_urls = set()

        if not soup:
            return troubleshooting_links

        # æŸ¥æ‰¾æ‰€æœ‰æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
        # 1. ç›´æ¥çš„Troubleshootingé“¾æ¥
        ts_links = soup.select("a[href*='/Troubleshooting/']")

        # 2. æŸ¥æ‰¾åŒ…å«æ•…éšœæ’é™¤æ–‡æœ¬çš„é“¾æ¥
        all_links = soup.select("a[href]")
        for link in all_links:
            text = link.get_text().strip().lower()
            href = link.get("href", "")

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
            troubleshooting_keywords = [
                'troubleshooting', 'æ•…éšœæ’é™¤', 'æ•…éšœ', 'problems', 'issues',
                'won\'t turn on', 'not working', 'broken'
            ]

            if any(keyword in text for keyword in troubleshooting_keywords) or '/Troubleshooting/' in href:
                ts_links.append(link)

        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        for link in ts_links:
            href = link.get("href")
            text = link.get_text().strip()

            if href and text:
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_ts_urls and '/Troubleshooting/' in href:
                    seen_ts_urls.add(href)
                    troubleshooting_links.append({
                        "title": text,
                        "url": href
                    })

        return troubleshooting_links

    def extract_troubleshooting_content(self, troubleshooting_url):
        """
        é‡å†™troubleshootingå†…å®¹æå–æ–¹æ³•ï¼Œä¿®å¤causesæå–é—®é¢˜
        """
        if not troubleshooting_url:
            return None

        # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in troubleshooting_url:
            troubleshooting_url = troubleshooting_url.replace('zh.ifixit.com', 'www.ifixit.com')

        # æ·»åŠ lang=enå‚æ•°
        if '?' in troubleshooting_url:
            if 'lang=' not in troubleshooting_url:
                troubleshooting_url += '&lang=en'
        else:
            troubleshooting_url += '?lang=en'

        print(f"Crawling troubleshooting content: {troubleshooting_url}")

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(random.uniform(1, 2))

        soup = self.get_soup(troubleshooting_url)
        if not soup:
            return None

        troubleshooting_data = {
            "url": troubleshooting_url,
            "title": "",
            "causes": []
        }

        try:
            # æå–æ ‡é¢˜
            title_elem = soup.select_one("h1, h2[data-testid='troubleshooting-title']")
            if title_elem:
                title_text = title_elem.get_text().strip()
                title_text = re.sub(r'\s+', ' ', title_text)
                troubleshooting_data["title"] = title_text
                print(f"æå–æ ‡é¢˜: {title_text}")

            # æå–causes - æŸ¥æ‰¾æ‰€æœ‰Section_å¼€å¤´çš„div
            causes = []
            section_divs = soup.find_all('div', id=re.compile(r'^Section_'))

            for i, section_div in enumerate(section_divs, 1):
                from bs4 import Tag
                if not isinstance(section_div, Tag):
                    continue

                cause_data = {
                    "number": str(i),
                    "title": "",
                    "content": ""
                }

                # æå–æ ‡é¢˜
                title_elem = section_div.find('h2')
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    cause_data["title"] = title_text

                # æå–å†…å®¹ - æŸ¥æ‰¾æ‰€æœ‰på…ƒç´ 
                content_parts = []
                p_elements = section_div.find_all('p')
                for p in p_elements:
                    p_text = p.get_text().strip()
                    if self.is_commercial_text(p_text):
                        continue

                    if p_text and len(p_text) > 10:
                        content_parts.append(p_text)

                if content_parts:
                    cause_data["content"] = '\n\n'.join(content_parts)

                # åªæ·»åŠ æœ‰å†…å®¹çš„cause
                if cause_data["title"] or cause_data["content"]:
                    causes.append(cause_data)
                    print(f"  æå–Cause {i}: {cause_data['title']}")

            troubleshooting_data["causes"] = causes

            # æå–ç»Ÿè®¡æ•°æ®
            statistics = self.extract_page_statistics(soup)
            if statistics:
                # åˆ›å»ºview_statisticså¯¹è±¡
                view_stats = {}
                for key in ['past_24_hours', 'past_7_days', 'past_30_days', 'all_time']:
                    if key in statistics:
                        view_stats[key] = statistics[key]

                if view_stats:
                    troubleshooting_data["view_statistics"] = view_stats

                for key in ['completed', 'favorites']:
                    if key in statistics:
                        troubleshooting_data[key] = statistics[key]

            print(f"  æ€»è®¡æå–åˆ° {len(causes)} ä¸ªcauses")
            return troubleshooting_data

        except Exception as e:
            print(f"æå–æ•…éšœæ’é™¤å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def deep_crawl_product_content(self, node, skip_troubleshooting=False):
        """æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶æ­£ç¡®æ„å»ºæ•°æ®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_specified_target = self._is_specified_target_url(url)

        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (is_specified_target or not node.get('children') or len(node.get('children', [])) == 0)
        )

        if is_target_page:
            print(f"æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")

            try:
                soup = self.get_soup(url)
                if soup:
                    # åŸºæœ¬æ•°æ®ä¿®å¤
                    node = self.fix_node_data(node, soup)

                    # åªåœ¨ç›®æ ‡äº§å“é¡µé¢æ·»åŠ è¿™äº›å­—æ®µ
                    is_root_device = url == f"{self.base_url}/Device"
                    if not is_root_device:
                        # æ·»åŠ titleå­—æ®µ
                        real_title = self.extract_real_title_from_page(soup)
                        if real_title:
                            node['title'] = real_title

                        # æ·»åŠ instruction_urlå­—æ®µ
                        node['instruction_url'] = ""

                        # æ·»åŠ view_statisticså­—æ®µ
                        real_stats = self.extract_real_view_statistics(soup, url)
                        if real_stats:
                            node['view_statistics'] = real_stats

                    guides = self.extract_guides_from_device_page(soup, url)
                    guide_links = [guide["url"] for guide in guides]
                    print(f"  æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

                    # ä½¿ç”¨å¹¶å‘å¤„ç†guideså’Œtroubleshooting
                    guides_data, troubleshooting_data = self._process_content_concurrently(
                        guide_links, url, skip_troubleshooting, soup
                    )

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                    # å¤„ç†å­ç±»åˆ« - æ— è®ºæ˜¯å¦ä¸ºæŒ‡å®šç›®æ ‡éƒ½è¦ä¿æŒå±‚çº§ç»“æ„
                    if is_specified_target:
                        # å¦‚æœæ˜¯æŒ‡å®šç›®æ ‡ï¼Œå°è¯•å‘ç°æ–°çš„å­ç±»åˆ«
                        subcategories = self.discover_subcategories_from_page(soup, url)
                        if subcategories:
                            print(f"  å‘ç° {len(subcategories)} ä¸ªå­ç±»åˆ«ï¼Œå¼€å§‹å¤„ç†...")

                            subcategory_children = []
                            for subcat in subcategories:
                                print(f"    å¤„ç†å­ç±»åˆ«: {subcat['name']}")

                                subcat_node = {
                                    'name': subcat['name'],
                                    'url': subcat['url'],
                                    'title': subcat['title']
                                }

                                processed_subcat = self.deep_crawl_product_content(subcat_node, skip_troubleshooting=True)
                                if processed_subcat:
                                    subcategory_children.append(processed_subcat)

                            if subcategory_children:
                                node['children'] = subcategory_children
                                print(f"  æˆåŠŸå¤„ç† {len(subcategory_children)} ä¸ªå­ç±»åˆ«")

                    # ä¿æŒç°æœ‰çš„childrenç»“æ„ï¼Œä¸è¦åˆ é™¤å±‚çº§å…³ç³»

                    print(f"  æ€»è®¡: {len(guides_data)} ä¸ªæŒ‡å—, {len(troubleshooting_data)} ä¸ªæ•…éšœæ’é™¤")

            except Exception as e:
                print(f"  âœ— æ·±å…¥çˆ¬å–æ—¶å‡ºé”™: {str(e)}")

        elif 'children' in node and node['children']:
            for i, child in enumerate(node['children']):
                node['children'][i] = self.deep_crawl_product_content(child, skip_troubleshooting)

        return node
        
    def save_combined_result(self, tree_data, target_name=None):
        """ä¿å­˜æ•´åˆç»“æœåˆ°æœ¬åœ°æ–‡ä»¶å¤¹ç»“æ„"""
        # åˆ›å»ºæ ¹ç›®å½•
        if target_name:
            safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
            root_dir = self._create_local_directory(f"auto_{safe_name}")
        else:
            root_name = tree_data.get("name", "auto")
            if " > " in root_name:
                root_name = root_name.split(" > ")[-1]
            root_name = root_name.replace("/", "_").replace("\\", "_").replace(":", "_")
            root_dir = self._create_local_directory(f"auto_{root_name}")

        # å¤„ç†åª’ä½“æ–‡ä»¶å¹¶ä¿å­˜æ•°æ®ç»“æ„
        self._save_node_to_filesystem(tree_data, root_dir, "")

        print(f"\næ•´åˆç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {root_dir}")
        return str(root_dir)

    def _save_node_to_filesystem(self, node, base_dir, path_prefix):
        """é€’å½’ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")

        # åˆ›å»ºå½“å‰èŠ‚ç‚¹ç›®å½•
        current_path = path_prefix + "/" + safe_name if path_prefix else safe_name
        node_dir = base_dir / current_path
        node_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, node_dir)

        # ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°JSONæ–‡ä»¶
        info_file = node_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = node_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)
            for i, ts in enumerate(node_data['troubleshooting']):
                ts_data = ts.copy()
                self._process_media_urls(ts_data, ts_dir)
                ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                with open(ts_file, 'w', encoding='utf-8') as f:
                    json.dump(ts_data, f, ensure_ascii=False, indent=2)

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_node_to_filesystem(child, base_dir, current_path)
        
    def print_combined_tree_structure(self, node, level=0):
        """
        æ‰“å°æ•´åˆåçš„æ ‘å½¢ç»“æ„ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦
        """
        if not node:
            return

        indent = "  " * level
        name = node.get('name', 'Unknown')

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„å†…å®¹
        guides_count = len(node.get('guides', []))
        troubleshooting_count = len(node.get('troubleshooting', []))
        children_count = len(node.get('children', []))

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if 'title' in node and 'steps' in node:
            # ç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹
            steps_count = len(node.get('steps', []))
            print(f"{indent}ğŸ“– {name} [æŒ‡å— - {steps_count}æ­¥éª¤]")
        elif 'causes' in node:
            # ç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹
            causes_count = len(node.get('causes', []))
            print(f"{indent}ğŸ”§ {name} [æ•…éšœæ’é™¤ - {causes_count}åŸå› ]")
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“± {name} [äº§å“{content_str}]")
        elif children_count > 0 or guides_count > 0 or troubleshooting_count > 0:
            # åˆ†ç±»èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“ {name} [åˆ†ç±»{content_str}]")
        else:
            # å…¶ä»–èŠ‚ç‚¹
            print(f"{indent}â“ {name} [æœªçŸ¥ç±»å‹]")

        # æ˜¾ç¤ºguideså†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if guides_count > 0:
            for guide in node.get('guides', []):
                guide_name = guide.get('title', 'Unknown Guide')
                steps_count = len(guide.get('steps', []))
                print(f"{indent}  ğŸ“– {guide_name} [{steps_count}æ­¥éª¤]")

        # æ˜¾ç¤ºtroubleshootingå†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if troubleshooting_count > 0:
            for ts in node.get('troubleshooting', []):
                ts_name = ts.get('title', 'Unknown Troubleshooting')
                causes_count = len(ts.get('causes', []))
                print(f"{indent}  ğŸ”§ {ts_name} [{causes_count}åŸå› ]")

        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if children_count > 0:
            for child in node['children']:
                self.print_combined_tree_structure(child, level + 1)

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("=" * 60)
    print("iFixitæ•´åˆçˆ¬è™«å·¥å…· - ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python auto_crawler.py [URLæˆ–è®¾å¤‡å] [é€‰é¡¹...]")
    print("\nåŸºæœ¬ç”¨æ³•:")
    print("  python auto_crawler.py iMac_M_Series")
    print("  python auto_crawler.py https://www.ifixit.com/Device/Television")
    print("  python auto_crawler.py Television")
    print("\nå¸¸ç”¨é€‰é¡¹:")
    print("  --verbose              å¯ç”¨è¯¦ç»†è¾“å‡º")
    print("  --no-proxy             å…³é—­ä»£ç†æ± ")
    print("  --no-cache             ç¦ç”¨ç¼“å­˜æ£€æŸ¥")
    print("  --force-refresh        å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
    print("  --workers N            è®¾ç½®å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰")
    print("  --max-retries N        è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤5ï¼‰")
    print("\nè§†é¢‘å¤„ç†é€‰é¡¹:")
    print("  --download-videos      å¯ç”¨è§†é¢‘æ–‡ä»¶ä¸‹è½½ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰")
    print("  --max-video-size N     è®¾ç½®è§†é¢‘æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼Œé»˜è®¤50ï¼‰")
    print("\nç¤ºä¾‹:")
    print("  # åŸºæœ¬çˆ¬å–ï¼ˆä¸ä¸‹è½½è§†é¢‘ï¼‰")
    print("  python auto_crawler.py iMac_M_Series")
    print("")
    print("  # å¯ç”¨è§†é¢‘ä¸‹è½½ï¼Œé™åˆ¶10MB")
    print("  python auto_crawler.py iMac_M_Series --download-videos --max-video-size 10")
    print("")
    print("  # è¯¦ç»†è¾“å‡ºï¼Œç¦ç”¨ä»£ç†")
    print("  python auto_crawler.py Television --verbose --no-proxy")
    print("\nåŠŸèƒ½è¯´æ˜:")
    print("- æ™ºèƒ½ç¼“å­˜ï¼šè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„å†…å®¹")
    print("- ä»£ç†æ± ï¼šå¤©å¯IPè‡ªåŠ¨åˆ‡æ¢ï¼Œé¿å…å°ç¦")
    print("- å¹¶å‘çˆ¬å–ï¼šå¤šçº¿ç¨‹æå‡çˆ¬å–é€Ÿåº¦")
    print("- é”™è¯¯é‡è¯•ï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•")
    print("- åª’ä½“ä¸‹è½½ï¼šè‡ªåŠ¨ä¸‹è½½å¹¶æœ¬åœ°åŒ–æ‰€æœ‰åª’ä½“æ–‡ä»¶")
    print("- è§†é¢‘å¤„ç†ï¼šå¯é€‰æ‹©æ€§ä¸‹è½½è§†é¢‘æ–‡ä»¶ï¼Œæ”¯æŒå¤§å°é™åˆ¶")
    print("=" * 60)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = sys.argv[1:]

    # æ£€æŸ¥å¸®åŠ©å‚æ•°
    if not args or args[0].lower() in ['--help', '-h', 'help']:
        print_usage()
        return

    # è·å–ç›®æ ‡URL/åç§°
    input_text = args[0]

    # è§£æé€‰é¡¹
    verbose = '--verbose' in args
    use_proxy = '--no-proxy' not in args  # é»˜è®¤å¯ç”¨ä»£ç†
    use_cache = '--no-cache' not in args  # é»˜è®¤å¯ç”¨ç¼“å­˜
    force_refresh = '--force-refresh' in args

    # è§£æworkerså‚æ•°
    max_workers = 4
    if '--workers' in args:
        try:
            workers_idx = args.index('--workers')
            if workers_idx + 1 < len(args):
                max_workers = int(args[workers_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: workerså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼4")

    # è§£æmax-retrieså‚æ•°
    max_retries = 5
    if '--max-retries' in args:
        try:
            retries_idx = args.index('--max-retries')
            if retries_idx + 1 < len(args):
                max_retries = int(args[retries_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-retrieså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼5")

    # è§£æè§†é¢‘ä¸‹è½½å‚æ•°
    download_videos = '--download-videos' in args
    max_video_size_mb = 50
    if '--max-video-size' in args:
        try:
            size_idx = args.index('--max-video-size')
            if size_idx + 1 < len(args):
                max_video_size_mb = int(args[size_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-video-sizeå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼50MB")

    if not input_text:
        print_usage()
        return

    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)

    if url:
        print("\n" + "=" * 60)
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {name}")
        print("é˜¶æ®µ1: æ„å»ºå®Œæ•´æ ‘å½¢ç»“æ„")
        print("é˜¶æ®µ2: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹")
        print("=" * 60)

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"ğŸ“‹ çˆ¬å–é…ç½®:")
        print(f"   ç›®æ ‡: {name}")
        print(f"   è¯¦ç»†è¾“å‡º: {'æ˜¯' if verbose else 'å¦'}")
        print(f"   ä½¿ç”¨ä»£ç†: {'æ˜¯' if use_proxy else 'å¦'}")
        print(f"   ä½¿ç”¨ç¼“å­˜: {'æ˜¯' if use_cache else 'å¦'}")
        print(f"   å¼ºåˆ¶åˆ·æ–°: {'æ˜¯' if force_refresh else 'å¦'}")
        print(f"   å¹¶å‘æ•°: {max_workers}")
        print(f"   æœ€å¤§é‡è¯•: {max_retries}")
        print(f"   ä¸‹è½½è§†é¢‘: {'æ˜¯' if download_videos else 'å¦'}")
        if download_videos:
            print(f"   è§†é¢‘å¤§å°é™åˆ¶: {max_video_size_mb}MB")

        # åˆ›å»ºæ•´åˆçˆ¬è™«
        crawler = CombinedIFixitCrawler(
            verbose=verbose,
            use_proxy=use_proxy,
            use_cache=use_cache,
            force_refresh=force_refresh,
            max_workers=max_workers,
            max_retries=max_retries,
            download_videos=download_videos,
            max_video_size_mb=max_video_size_mb
        )

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œæ•´åˆçˆ¬å–
        try:
            combined_data = crawler.crawl_combined_tree(url, name)

            if combined_data:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = crawler.count_detailed_nodes(combined_data)
                elapsed_time = time.time() - start_time

                # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
                print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
                print("=" * 60)
                crawler.print_combined_tree_structure(combined_data)
                print("=" * 60)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\næ•´åˆçˆ¬å–ç»Ÿè®¡:")
                print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
                print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
                print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
                print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
                print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
                print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

                # ä¿å­˜ç»“æœ
                filename = crawler.save_combined_result(combined_data, target_name=input_text)

                print(f"\nâœ… æ•´åˆçˆ¬å–å®Œæˆ!")
                print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {filename}")

                # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
                crawler._print_performance_stats()

                # æä¾›ä½¿ç”¨å»ºè®®
                print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
                print(f"- æŸ¥çœ‹æ–‡ä»¶å¤¹ç»“æ„äº†è§£å®Œæ•´çš„æ•°æ®å±‚çº§")
                print(f"- æ¯ä¸ªç›®å½•åŒ…å«info.jsonã€guides/ã€troubleshooting/å­ç›®å½•")
                print(f"- æ‰€æœ‰åª’ä½“æ–‡ä»¶å·²ä¸‹è½½åˆ°media/ç›®å½•ï¼ŒURLå·²æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„")
                print(f"- ä¸‹æ¬¡çˆ¬å–ç›¸åŒå†…å®¹å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œå¤§å¹…æå‡é€Ÿåº¦")

            else:
                print("\nâœ— æ•´åˆçˆ¬å–å¤±è´¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            print(f"\nâœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            if verbose:
                import traceback
                traceback.print_exc()
    else:
        print_usage()

def test_tianqi_proxy():
    """æµ‹è¯•å¤©å¯IPä»£ç†æ± åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•å¤©å¯IPä»£ç†æ± åŠŸèƒ½")
    print("=" * 60)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œå¯ç”¨ä»£ç†
    crawler = CombinedIFixitCrawler(
        verbose=True,
        use_proxy=True,
        use_cache=False,
        max_workers=1
    )

    # æµ‹è¯•ä»£ç†è·å–
    print("\n1. æµ‹è¯•ä»£ç†è·å–...")
    if crawler.proxy_manager:
        proxy = crawler.proxy_manager.get_proxy()
        if proxy:
            print("âœ… ä»£ç†è·å–æˆåŠŸ")
            print(f"å½“å‰ä»£ç†: {proxy}")
            stats = crawler.proxy_manager.get_stats()
            print(f"ä»£ç†æ± ç»Ÿè®¡: {stats}")
        else:
            print("âŒ ä»£ç†è·å–å¤±è´¥")
            return
    else:
        print("âŒ ä»£ç†ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return

    # æµ‹è¯•ä»£ç†åˆ‡æ¢
    print("\n2. æµ‹è¯•ä»£ç†åˆ‡æ¢...")
    old_proxy = crawler.current_proxy.copy() if crawler.current_proxy else None
    if crawler._switch_proxy("æµ‹è¯•åˆ‡æ¢"):
        new_proxy = crawler.current_proxy
        if new_proxy != old_proxy:
            print("âœ… ä»£ç†åˆ‡æ¢æˆåŠŸ")
            print(f"æ–°ä»£ç†: {new_proxy}")
        else:
            print("âš ï¸  ä»£ç†æœªå˜åŒ–ï¼ˆå¯èƒ½æ˜¯ä»£ç†æ± ä¸ºç©ºï¼‰")
    else:
        print("âŒ ä»£ç†åˆ‡æ¢å¤±è´¥")

    # æµ‹è¯•å®é™…ç½‘ç»œè¯·æ±‚
    print("\n3. æµ‹è¯•å®é™…ç½‘ç»œè¯·æ±‚...")
    test_url = "https://www.ifixit.com/Device"
    try:
        soup = crawler.get_soup(test_url)
        if soup:
            title = soup.find('title')
            print(f"âœ… ç½‘ç»œè¯·æ±‚æˆåŠŸ")
            print(f"é¡µé¢æ ‡é¢˜: {title.get_text() if title else 'No title'}")
        else:
            print("âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œæœªè·å–åˆ°é¡µé¢å†…å®¹")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")

    print("\n" + "=" * 60)
    print("ğŸ ä»£ç†æ± æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "--test-proxy":
        test_tianqi_proxy()
        sys.exit(0)

    main()
