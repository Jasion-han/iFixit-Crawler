#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•´åˆçˆ¬è™«å·¥å…· - ç»“åˆæ ‘å½¢ç»“æ„å’Œè¯¦ç»†å†…å®¹æå– + ä¼ä¸šçº§åŠŸèƒ½
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
æ”¯æŒä»£ç†æ± ã€MySQLå­˜å‚¨ã€å¤±è´¥é‡è¯•ç­‰ä¼ä¸šçº§åŠŸèƒ½
ç”¨æ³•: python auto_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python auto_crawler.py https://www.ifixit.com/Device/Television
      python auto_crawler.py Television
"""

import os
import sys
import json
import time
import random
import re
import requests
import pymysql
from urllib.parse import urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å¯¼å…¥ä¸¤ä¸ªåŸºç¡€çˆ¬è™«
from enhanced_crawler import EnhancedIFixitCrawler
from tree_crawler import TreeCrawler

class CombinedIFixitCrawler(EnhancedIFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", verbose=False, use_proxy=False, use_database=False):
        super().__init__(base_url, verbose)
        self.tree_crawler = TreeCrawler(base_url)
        self.processed_nodes = set()
        self.target_url = None

        # ä¼ä¸šçº§åŠŸèƒ½
        self.use_proxy = use_proxy
        self.use_database = use_database
        self.proxy_pool = []
        self.current_proxy_index = 0
        self.db_connection = None
        self.failed_urls = set()

        if use_proxy:
            self._init_proxy_pool()
        if use_database:
            self._init_database()

        # å¤åˆ¶æ ‘å½¢çˆ¬è™«çš„é‡è¦æ–¹æ³•åˆ°å½“å‰ç±»
        self.find_exact_path = self.tree_crawler.find_exact_path
        self.extract_breadcrumbs_from_page = self.tree_crawler.extract_breadcrumbs_from_page
        self._build_tree_from_path = self.tree_crawler._build_tree_from_path
        self._find_node_by_url = self.tree_crawler._find_node_by_url
        self.ensure_instruction_url_in_leaf_nodes = self.tree_crawler.ensure_instruction_url_in_leaf_nodes

        # é‡å†™tree_crawlerçš„get_soupæ–¹æ³•ï¼Œè®©å®ƒä½¿ç”¨æˆ‘ä»¬çš„å¢å¼ºç‰ˆæœ¬
        self.tree_crawler.get_soup = self._tree_crawler_get_soup

    def _init_proxy_pool(self):
        """åˆå§‹åŒ–ä»£ç†æ± """
        self.proxy_pool = [
            {'http': 'http://proxy1:8080', 'https': 'https://proxy1:8080'},
            {'http': 'http://proxy2:8080', 'https': 'https://proxy2:8080'},
        ]

    def _init_database(self):
        """åˆå§‹åŒ–MySQLæ•°æ®åº“è¿æ¥"""
        try:
            self.db_connection = pymysql.connect(
                host='localhost',
                user='root',
                password='password',
                database='ifixit_crawler',
                charset='utf8mb4'
            )
            self._create_tables()
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            self.use_database = False

    def _create_tables(self):
        """åˆ›å»ºæ•°æ®åº“è¡¨"""
        if not self.db_connection:
            return

        cursor = self.db_connection.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS crawl_data (
                id INT AUTO_INCREMENT PRIMARY KEY,
                url VARCHAR(500) UNIQUE,
                name VARCHAR(200),
                title VARCHAR(500),
                content LONGTEXT,
                status ENUM('pending', 'success', 'failed') DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
            )
        """)
        self.db_connection.commit()

    def _get_next_proxy(self):
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†"""
        if not self.proxy_pool:
            return None
        proxy = self.proxy_pool[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_pool)
        return proxy

    def _save_to_database(self, data, url):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        if not self.use_database or not self.db_connection:
            return

        try:
            cursor = self.db_connection.cursor()
            cursor.execute("""
                INSERT INTO crawl_data (url, name, title, content, status)
                VALUES (%s, %s, %s, %s, 'success')
                ON DUPLICATE KEY UPDATE
                name = VALUES(name), title = VALUES(title),
                content = VALUES(content), status = 'success'
            """, (url, data.get('name', ''), data.get('title', ''), json.dumps(data, ensure_ascii=False)))
            self.db_connection.commit()
        except Exception as e:
            print(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")

    def get_soup(self, url, use_playwright=False):
        """é‡å†™get_soupæ–¹æ³•ï¼Œæ”¯æŒä»£ç†æ± ã€é‡è¯•æœºåˆ¶å’ŒPlaywrightæ¸²æŸ“"""
        if not url:
            return None

        if url in self.failed_urls:
            return None

        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        if '?' in url:
            url += '&lang=en'
        else:
            url += '?lang=en'

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return self._get_soup_with_playwright(url)

        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„requestsæ–¹æ³•
        max_retries = 3
        for attempt in range(max_retries):
            try:
                session = requests.Session()
                retry_strategy = Retry(
                    total=3,
                    backoff_factor=1,
                    status_forcelist=[429, 500, 502, 503, 504],
                )
                adapter = HTTPAdapter(max_retries=retry_strategy)
                session.mount("http://", adapter)
                session.mount("https://", adapter)

                proxies = self._get_next_proxy() if self.use_proxy else None

                response = session.get(
                    url,
                    headers=self.headers,
                    timeout=30,
                    proxies=proxies
                )
                response.raise_for_status()

                from bs4 import BeautifulSoup
                return BeautifulSoup(response.content, 'html.parser')

            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
                    self.failed_urls.add(url)
                    return None
                time.sleep(random.uniform(1, 3))

        return None

    def _get_soup_with_playwright(self, url):
        """ä½¿ç”¨Playwrightè·å–JavaScriptæ¸²æŸ“åçš„é¡µé¢å†…å®¹"""
        try:
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                page.goto(url, wait_until='networkidle')

                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            print(f"Playwrightè·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return None

    def _tree_crawler_get_soup(self, url):
        """ä¸ºtree_crawleræä¾›çš„get_soupæ–¹æ³•ï¼Œåœ¨éœ€è¦é¢åŒ…å±‘å¯¼èˆªæ—¶ä½¿ç”¨Playwright"""
        # å¯¹äºéœ€è¦æå–é¢åŒ…å±‘å¯¼èˆªçš„é¡µé¢ï¼Œä½¿ç”¨Playwright
        if '/Teardown/' in url or '/Guide/' in url or '/Device/' in url:
            return self.get_soup(url, use_playwright=True)
        else:
            return self.get_soup(url, use_playwright=False)

    def extract_real_name_from_url(self, url):
        """
        ä»URLä¸­æå–çœŸå®çš„nameå­—æ®µï¼ˆURLè·¯å¾„çš„æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰
        """
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_url = url.split('?')[0].split('#')[0]

        # ç§»é™¤æœ«å°¾çš„æ–œæ 
        clean_url = clean_url.rstrip('/')

        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µ
        path_parts = clean_url.split('/')
        if path_parts:
            name = path_parts[-1]
            # URLè§£ç 
            import urllib.parse
            name = urllib.parse.unquote(name)
            # ä¿®å¤å¼•å·ç¼–ç é—®é¢˜ï¼šå°† \" è½¬æ¢ä¸º "
            name = name.replace('\\"', '"')
            return name

        return ""

    def extract_real_title_from_page(self, soup):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ
        """
        if not soup:
            return ""

        # ä¼˜å…ˆä»h1æ ‡ç­¾æå–
        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        # å…¶æ¬¡ä»titleæ ‡ç­¾æå–
        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    def extract_real_title_from_page(self, soup):
        """ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ"""
        if not soup:
            return ""

        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    def extract_real_view_statistics(self, soup, url):
        """ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®"""
        if not soup:
            return {}

        stats = {}
        stat_selectors = [
            '.view-count', '.stats-item', '.view-stats', '[data-stats]', '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        if not stats:
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def _is_specified_target_url(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        """
        if not self.target_url or not url:
            return False

        # æ ‡å‡†åŒ–URLè¿›è¡Œæ¯”è¾ƒ
        def normalize_url(u):
            import urllib.parse
            # URLè§£ç ï¼Œç„¶åæ ‡å‡†åŒ–
            decoded = urllib.parse.unquote(u)
            return decoded.rstrip('/').lower()

        return normalize_url(url) == normalize_url(self.target_url)

    def discover_subcategories_from_page(self, soup, base_url):
        """
        åŠ¨æ€å‘ç°é¡µé¢ä¸­çš„å­ç±»åˆ«é“¾æ¥
        """
        if not soup:
            return []

        subcategories = []

        # æŸ¥æ‰¾è®¾å¤‡ç±»åˆ«é“¾æ¥çš„å¤šç§é€‰æ‹©å™¨
        category_selectors = [
            'a[href*="/Device/"]',  # åŒ…å«/Device/çš„é“¾æ¥
            '.category-link',       # ç±»åˆ«é“¾æ¥ç±»
            '.device-link',         # è®¾å¤‡é“¾æ¥ç±»
            '.subcategory',         # å­ç±»åˆ«ç±»
            'a[href^="/Device/"]'   # ä»¥/Device/å¼€å¤´çš„é“¾æ¥
        ]

        found_links = set()

        for selector in category_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/Device/' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
                    skip_patterns = [
                        '/Guide/', '/Troubleshooting/', '/Edit/', '/History/',
                        '/Answers/', '?revision', 'action=edit', 'action=create'
                    ]

                    if not any(pattern in full_url for pattern in skip_patterns):
                        # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå­ç±»åˆ«ï¼ˆURLæ¯”å½“å‰é¡µé¢æ›´æ·±ä¸€å±‚ï¼‰
                        if full_url != base_url and full_url not in found_links:
                            # æå–é“¾æ¥æ–‡æœ¬ä½œä¸ºåç§°
                            link_text = link.get_text().strip()
                            if link_text:
                                subcategories.append({
                                    'name': self.extract_real_name_from_url(full_url),
                                    'url': full_url,
                                    'title': link_text
                                })
                                found_links.add(full_url)

        return subcategories

    def extract_real_view_statistics(self, soup, url):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®
        """
        if not soup:
            return {}

        stats = {}

        # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®çš„å„ç§å¯èƒ½ä½ç½®
        stat_selectors = [
            '.view-count',
            '.stats-item',
            '.view-stats',
            '[data-stats]',
            '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # è§£æç»Ÿè®¡æ•°æ®æ ¼å¼
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»APIæˆ–å…¶ä»–ä½ç½®è·å–
        if not stats:
            # å°è¯•ä»é¡µé¢çš„JavaScriptæ•°æ®ä¸­æå–
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        # æŸ¥æ‰¾å¯èƒ½çš„ç»Ÿè®¡æ•°æ®
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def build_correct_url_from_path(self, base_url, path_segments):
        """
        æ ¹æ®è·¯å¾„æ®µæ­£ç¡®æ„å»ºURLï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
        """
        if not path_segments:
            return base_url

        # ä»base_urlå¼€å§‹ï¼Œé€æ­¥æ›¿æ¢è·¯å¾„æ®µ
        current_url = base_url

        for segment in path_segments:
            # è·å–å½“å‰URLçš„è·¯å¾„éƒ¨åˆ†
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(current_url)
            path_parts = [p for p in parsed.path.split('/') if p]

            # æ·»åŠ æ–°çš„è·¯å¾„æ®µ
            path_parts.append(segment)

            # é‡æ–°æ„å»ºURL
            new_path = '/' + '/'.join(path_parts)
            new_parsed = parsed._replace(path=new_path)
            current_url = urlunparse(new_parsed)

        return current_url

    def restructure_target_content(self, node, is_target_page=False):
        """
        é‡æ„ç›®æ ‡æ–‡ä»¶çš„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨guides[]å’Œtroubleshooting[]è€Œéchildren[]
        """
        if not node or not isinstance(node, dict):
            return node

        # å¦‚æœè¿™æ˜¯ç›®æ ‡é¡µé¢ä¸”æœ‰childrenåŒ…å«guideså’Œtroubleshooting
        if is_target_page and 'children' in node and node['children']:
            guides = []
            troubleshooting = []
            real_children = []

            for child in node['children']:
                if 'steps' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæŒ‡å—
                    guide_data = {k: v for k, v in child.items() if k not in ['children']}
                    guides.append(guide_data)
                elif 'causes' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæ•…éšœæ’é™¤
                    ts_data = {k: v for k, v in child.items() if k not in ['children']}
                    troubleshooting.append(ts_data)
                else:
                    # è¿™æ˜¯çœŸæ­£çš„å­ç±»åˆ«
                    real_children.append(child)

            # é‡æ„èŠ‚ç‚¹
            if guides:
                node['guides'] = guides
            if troubleshooting:
                node['troubleshooting'] = troubleshooting

            # åªæœ‰çœŸæ­£çš„å­ç±»åˆ«æ‰ä¿ç•™åœ¨childrenä¸­
            if real_children:
                node['children'] = real_children
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„å­ç±»åˆ«ï¼Œç§»é™¤childrenå­—æ®µ
                if 'children' in node:
                    del node['children']

        return node

    def fix_node_data(self, node, soup=None):
        """ä¿®å¤èŠ‚ç‚¹çš„nameå­—æ®µï¼Œåªåœ¨æœ€ç»ˆäº§å“é¡µé¢æ·»åŠ å…¶ä»–å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_root_device = url == f"{self.base_url}/Device"

        # æ‰€æœ‰èŠ‚ç‚¹éƒ½ä¿®å¤nameå­—æ®µ
        real_name = self.extract_real_name_from_url(url)
        if real_name:
            node['name'] = real_name

        # åªæœ‰åœ¨deep_crawl_product_contentä¸­è¢«è¯†åˆ«ä¸ºç›®æ ‡é¡µé¢æ—¶æ‰æ·»åŠ å…¶ä»–å­—æ®µ
        # è¿™é‡Œä¸æ·»åŠ titleã€instruction_urlã€view_statisticsç­‰å­—æ®µ
        # è¿™äº›å­—æ®µå°†åœ¨deep_crawl_product_contentæ–¹æ³•ä¸­æ·»åŠ 

        return node
        
    def crawl_combined_tree(self, start_url, category_name=None):
        """
        æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # è®¾ç½®ç›®æ ‡URL
        self.target_url = start_url

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
        print("1. æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
        base_tree = self.tree_crawler.crawl_tree(start_url, category_name)

        if not base_tree:
            print("æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
            return None

        print(f"åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆï¼Œå¼€å§‹æå–è¯¦ç»†å†…å®¹...")
        print("=" * 60)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        print("2. ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
        enriched_tree = self.enrich_tree_with_detailed_content(base_tree)

        # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
        print("3. æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
        final_tree = self.deep_crawl_product_content(enriched_tree)

        return final_tree
        
    def enrich_tree_with_detailed_content(self, node):
        """ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ­£ç¡®çš„å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            return node

        self.processed_nodes.add(url)

        time.sleep(random.uniform(0.5, 1.0))
        soup = self.get_soup(url)

        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½ç»è¿‡fix_node_dataå¤„ç†
        node = self.fix_node_data(node, soup)

        if self.verbose:
            print(f"å¤„ç†èŠ‚ç‚¹: {node.get('name', '')} - {url}")
        else:
            print(f"å¤„ç†: {node.get('name', '')}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            enriched_children = []
            for child in node['children']:
                enriched_child = self.enrich_tree_with_detailed_content(child)
                if enriched_child:
                    enriched_children.append(enriched_child)
            node['children'] = enriched_children

        return node
        


    def count_detailed_nodes(self, node):
        """
        ç»Ÿè®¡æ ‘ä¸­åŒ…å«è¯¦ç»†å†…å®¹çš„èŠ‚ç‚¹æ•°é‡
        """
        count = {'guides': 0, 'troubleshooting': 0, 'products': 0, 'categories': 0}

        if 'title' in node and 'steps' in node:
            count['guides'] += 1
        elif 'causes' in node:
            count['troubleshooting'] += 1
        elif 'product_name' in node:
            count['products'] += 1
        else:
            count['categories'] += 1

        if 'children' in node and node['children']:
            for child in node['children']:
                child_count = self.count_detailed_nodes(child)
                for key in count:
                    count[key] += child_count[key]

        return count

    def extract_guides_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æŒ‡å—é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§é¡µé¢å¸ƒå±€
        """
        guides = []
        seen_guide_urls = set()

        if not soup:
            return guides

        # æŸ¥æ‰¾æ‰€æœ‰æŒ‡å—é“¾æ¥ï¼ˆåŒ…æ‹¬Guideå’ŒTeardownï¼‰
        guide_links = soup.select("a[href*='/Guide/'], a[href*='/Teardown/']")

        for link in guide_links:
            href = link.get("href")
            text = link.get_text().strip()

            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
            skip_patterns = [
                "Create Guide", "åˆ›å»ºæŒ‡å—", "Guide/new", "Guide/edit", "/edit/",
                "Guide/history", "/history/", "Edit Guide", "ç¼–è¾‘æŒ‡å—",
                "Version History", "ç‰ˆæœ¬å†å²", "Teardown/edit", "#comment", "permalink=comment"
            ]

            if href and text and not any(pattern in href or pattern in text for pattern in skip_patterns):
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_guide_urls:
                    seen_guide_urls.add(href)
                    guides.append({
                        "title": text,
                        "url": href
                    })

        return guides

    def extract_troubleshooting_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æ•…éšœæ’é™¤é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬
        """
        troubleshooting_links = []
        seen_ts_urls = set()

        if not soup:
            return troubleshooting_links

        # æŸ¥æ‰¾æ‰€æœ‰æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
        # 1. ç›´æ¥çš„Troubleshootingé“¾æ¥
        ts_links = soup.select("a[href*='/Troubleshooting/']")

        # 2. æŸ¥æ‰¾åŒ…å«æ•…éšœæ’é™¤æ–‡æœ¬çš„é“¾æ¥
        all_links = soup.select("a[href]")
        for link in all_links:
            text = link.get_text().strip().lower()
            href = link.get("href", "")

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
            troubleshooting_keywords = [
                'troubleshooting', 'æ•…éšœæ’é™¤', 'æ•…éšœ', 'problems', 'issues',
                'won\'t turn on', 'not working', 'broken'
            ]

            if any(keyword in text for keyword in troubleshooting_keywords) or '/Troubleshooting/' in href:
                ts_links.append(link)

        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        for link in ts_links:
            href = link.get("href")
            text = link.get_text().strip()

            if href and text:
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_ts_urls and '/Troubleshooting/' in href:
                    seen_ts_urls.add(href)
                    troubleshooting_links.append({
                        "title": text,
                        "url": href
                    })

        return troubleshooting_links

    def extract_troubleshooting_content(self, troubleshooting_url):
        """
        é‡å†™troubleshootingå†…å®¹æå–æ–¹æ³•ï¼Œä¿®å¤causesæå–é—®é¢˜
        """
        if not troubleshooting_url:
            return None

        # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in troubleshooting_url:
            troubleshooting_url = troubleshooting_url.replace('zh.ifixit.com', 'www.ifixit.com')

        # æ·»åŠ lang=enå‚æ•°
        if '?' in troubleshooting_url:
            if 'lang=' not in troubleshooting_url:
                troubleshooting_url += '&lang=en'
        else:
            troubleshooting_url += '?lang=en'

        print(f"Crawling troubleshooting content: {troubleshooting_url}")

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(random.uniform(1, 2))

        soup = self.get_soup(troubleshooting_url)
        if not soup:
            return None

        troubleshooting_data = {
            "url": troubleshooting_url,
            "title": "",
            "causes": []
        }

        try:
            # æå–æ ‡é¢˜
            title_elem = soup.select_one("h1, h2[data-testid='troubleshooting-title']")
            if title_elem:
                title_text = title_elem.get_text().strip()
                title_text = re.sub(r'\s+', ' ', title_text)
                troubleshooting_data["title"] = title_text
                print(f"æå–æ ‡é¢˜: {title_text}")

            # æå–causes - æŸ¥æ‰¾æ‰€æœ‰Section_å¼€å¤´çš„div
            causes = []
            section_divs = soup.find_all('div', id=re.compile(r'^Section_'))

            for i, section_div in enumerate(section_divs, 1):
                from bs4 import Tag
                if not isinstance(section_div, Tag):
                    continue

                cause_data = {
                    "number": str(i),
                    "title": "",
                    "content": ""
                }

                # æå–æ ‡é¢˜
                title_elem = section_div.find('h2')
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    cause_data["title"] = title_text

                # æå–å†…å®¹ - æŸ¥æ‰¾æ‰€æœ‰på…ƒç´ 
                content_parts = []
                p_elements = section_div.find_all('p')
                for p in p_elements:
                    p_text = p.get_text().strip()
                    if self.is_commercial_text(p_text):
                        continue

                    if p_text and len(p_text) > 10:
                        content_parts.append(p_text)

                if content_parts:
                    cause_data["content"] = '\n\n'.join(content_parts)

                # åªæ·»åŠ æœ‰å†…å®¹çš„cause
                if cause_data["title"] or cause_data["content"]:
                    causes.append(cause_data)
                    print(f"  æå–Cause {i}: {cause_data['title']}")

            troubleshooting_data["causes"] = causes

            # æå–ç»Ÿè®¡æ•°æ®
            statistics = self.extract_page_statistics(soup)
            if statistics:
                # åˆ›å»ºview_statisticså¯¹è±¡
                view_stats = {}
                for key in ['past_24_hours', 'past_7_days', 'past_30_days', 'all_time']:
                    if key in statistics:
                        view_stats[key] = statistics[key]

                if view_stats:
                    troubleshooting_data["view_statistics"] = view_stats

                for key in ['completed', 'favorites']:
                    if key in statistics:
                        troubleshooting_data[key] = statistics[key]

            print(f"  æ€»è®¡æå–åˆ° {len(causes)} ä¸ªcauses")
            return troubleshooting_data

        except Exception as e:
            print(f"æå–æ•…éšœæ’é™¤å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def deep_crawl_product_content(self, node, skip_troubleshooting=False):
        """æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶æ­£ç¡®æ„å»ºæ•°æ®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_specified_target = self._is_specified_target_url(url)

        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (is_specified_target or not node.get('children') or len(node.get('children', [])) == 0)
        )

        if is_target_page:
            print(f"æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")

            try:
                soup = self.get_soup(url)
                if soup:
                    # åŸºæœ¬æ•°æ®ä¿®å¤
                    node = self.fix_node_data(node, soup)

                    # åªåœ¨ç›®æ ‡äº§å“é¡µé¢æ·»åŠ è¿™äº›å­—æ®µ
                    is_root_device = url == f"{self.base_url}/Device"
                    if not is_root_device:
                        # æ·»åŠ titleå­—æ®µ
                        real_title = self.extract_real_title_from_page(soup)
                        if real_title:
                            node['title'] = real_title

                        # æ·»åŠ instruction_urlå­—æ®µ
                        node['instruction_url'] = ""

                        # æ·»åŠ view_statisticså­—æ®µ
                        real_stats = self.extract_real_view_statistics(soup, url)
                        if real_stats:
                            node['view_statistics'] = real_stats

                    guides = self.extract_guides_from_device_page(soup, url)
                    guide_links = [guide["url"] for guide in guides]
                    print(f"  æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

                    guides_data = []
                    troubleshooting_data = []

                    for guide_link in guide_links:
                        guide_content = self.extract_guide_content(guide_link)
                        if guide_content:
                            guide_content['url'] = guide_link
                            guides_data.append(guide_content)
                            print(f"    âœ“ æŒ‡å—: {guide_content.get('title', '')}")

                    if not skip_troubleshooting:
                        troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, url)
                        print(f"  æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢")

                        for ts_link in troubleshooting_links:
                            if isinstance(ts_link, dict):
                                ts_url = ts_link.get('url', '')
                            else:
                                ts_url = ts_link

                            if ts_url:
                                ts_content = self.extract_troubleshooting_content(ts_url)
                                if ts_content:
                                    ts_content['url'] = ts_url
                                    troubleshooting_data.append(ts_content)
                                    print(f"    âœ“ æ•…éšœæ’é™¤: {ts_content.get('title', '')}")

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                    # å¤„ç†å­ç±»åˆ« - æ— è®ºæ˜¯å¦ä¸ºæŒ‡å®šç›®æ ‡éƒ½è¦ä¿æŒå±‚çº§ç»“æ„
                    if is_specified_target:
                        # å¦‚æœæ˜¯æŒ‡å®šç›®æ ‡ï¼Œå°è¯•å‘ç°æ–°çš„å­ç±»åˆ«
                        subcategories = self.discover_subcategories_from_page(soup, url)
                        if subcategories:
                            print(f"  å‘ç° {len(subcategories)} ä¸ªå­ç±»åˆ«ï¼Œå¼€å§‹å¤„ç†...")

                            subcategory_children = []
                            for subcat in subcategories:
                                print(f"    å¤„ç†å­ç±»åˆ«: {subcat['name']}")

                                subcat_node = {
                                    'name': subcat['name'],
                                    'url': subcat['url'],
                                    'title': subcat['title']
                                }

                                processed_subcat = self.deep_crawl_product_content(subcat_node, skip_troubleshooting=True)
                                if processed_subcat:
                                    subcategory_children.append(processed_subcat)

                            if subcategory_children:
                                node['children'] = subcategory_children
                                print(f"  æˆåŠŸå¤„ç† {len(subcategory_children)} ä¸ªå­ç±»åˆ«")

                    # ä¿æŒç°æœ‰çš„childrenç»“æ„ï¼Œä¸è¦åˆ é™¤å±‚çº§å…³ç³»

                    print(f"  æ€»è®¡: {len(guides_data)} ä¸ªæŒ‡å—, {len(troubleshooting_data)} ä¸ªæ•…éšœæ’é™¤")

                    if self.use_database:
                        self._save_to_database(node, url)

            except Exception as e:
                print(f"  âœ— æ·±å…¥çˆ¬å–æ—¶å‡ºé”™: {str(e)}")

        elif 'children' in node and node['children']:
            for i, child in enumerate(node['children']):
                node['children'][i] = self.deep_crawl_product_content(child, skip_troubleshooting)

        return node
        
    def save_combined_result(self, tree_data, filename=None, target_name=None):
        """ä¿å­˜æ•´åˆç»“æœåˆ°JSONæ–‡ä»¶æˆ–æ•°æ®åº“"""
        if self.use_database:
            self._save_to_database(tree_data, tree_data.get('url', ''))
            print(f"\næ•´åˆç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")
            return "database"

        os.makedirs("results", exist_ok=True)

        if not filename:
            if target_name:
                safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                filename = f"results/auto_{safe_name}.json"
            else:
                root_name = tree_data.get("name", "auto")
                if " > " in root_name:
                    root_name = root_name.split(" > ")[-1]
                root_name = root_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                filename = f"results/auto_{root_name}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(tree_data, f, ensure_ascii=False, indent=2)

        print(f"\næ•´åˆç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename
        
    def print_combined_tree_structure(self, node, level=0):
        """
        æ‰“å°æ•´åˆåçš„æ ‘å½¢ç»“æ„ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦
        """
        indent = "  " * level
        name = node.get('name', 'Unknown')
        
        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if 'title' in node and 'steps' in node:
            # æŒ‡å—èŠ‚ç‚¹
            steps_count = len(node.get('steps', []))
            print(f"{indent}ğŸ“– {name} [æŒ‡å— - {steps_count}æ­¥éª¤]")
        elif 'causes' in node:
            # æ•…éšœæ’é™¤èŠ‚ç‚¹  
            causes_count = len(node.get('causes', []))
            print(f"{indent}ğŸ”§ {name} [æ•…éšœæ’é™¤ - {causes_count}åŸå› ]")
        elif 'product_name' in node:
            # äº§å“èŠ‚ç‚¹
            print(f"{indent}ğŸ“± {name} [äº§å“]")
        elif 'children' in node and node['children']:
            # åˆ†ç±»èŠ‚ç‚¹
            children_count = len(node['children'])
            print(f"{indent}ğŸ“ {name} [åˆ†ç±» - {children_count}å­é¡¹]")
        else:
            # å…¶ä»–èŠ‚ç‚¹
            print(f"{indent}â“ {name} [æœªçŸ¥ç±»å‹]")
            
        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for child in node['children']:
                self.print_combined_tree_structure(child, level + 1)

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python auto_crawler.py [URLæˆ–è®¾å¤‡å] [verbose] [use_proxy] [use_database]")
    print("ä¾‹å¦‚: python auto_crawler.py https://www.ifixit.com/Device/Television")
    print("      python auto_crawler.py Television verbose false false")
    print("      python auto_crawler.py --retry  # é‡è¯•å¤±è´¥çš„URL")

def main():
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†å‚æ•°
    if len(sys.argv) > 1:
        input_text = sys.argv[1]

        # æ£€æŸ¥å¸®åŠ©å‚æ•°
        if input_text.lower() in ['--help', '-h', 'help']:
            print_usage()
            return

        verbose = len(sys.argv) > 2 and sys.argv[2].lower() in ('verbose', 'debug', 'true', '1')
        use_proxy = len(sys.argv) > 3 and sys.argv[3].lower() in ('true', '1', 'yes')
        use_database = len(sys.argv) > 4 and sys.argv[4].lower() in ('true', '1', 'yes')
    else:
        print("=" * 60)
        print("iFixitæ•´åˆçˆ¬è™«å·¥å…· - æ ‘å½¢ç»“æ„ + è¯¦ç»†å†…å®¹ + ä¼ä¸šçº§åŠŸèƒ½")
        print("=" * 60)
        print("\nè¯·è¾“å…¥è¦çˆ¬å–çš„iFixitäº§å“åç§°æˆ–URL:")
        print("ä¾‹å¦‚: 70UK6570PUB            - æŒ‡å®šäº§å“å‹å·")
        print("      https://www.ifixit.com/Device/70UK6570PUB - æŒ‡å®šäº§å“URL")
        print("      Television             - ç”µè§†ç±»åˆ«")
        print("      LG_Television          - å“ç‰Œç”µè§†ç±»åˆ«")

        input_text = input("\n> ").strip()
        verbose = input("\næ˜¯å¦å¼€å¯è¯¦ç»†è¾“å‡º? (y/n): ").strip().lower() == 'y'
        use_proxy = input("æ˜¯å¦ä½¿ç”¨ä»£ç†æ± ? (y/n): ").strip().lower() == 'y'
        use_database = input("æ˜¯å¦ä½¿ç”¨MySQLå­˜å‚¨? (y/n): ").strip().lower() == 'y'

    if not input_text:
        print_usage()
        return

    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)

    if url:
        print("\n" + "=" * 60)
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {name}")
        print("é˜¶æ®µ1: æ„å»ºå®Œæ•´æ ‘å½¢ç»“æ„")
        print("é˜¶æ®µ2: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹")
        print("=" * 60)

        # åˆ›å»ºæ•´åˆçˆ¬è™«
        crawler = CombinedIFixitCrawler(verbose=verbose, use_proxy=use_proxy, use_database=use_database)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œæ•´åˆçˆ¬å–
        try:
            combined_data = crawler.crawl_combined_tree(url, name)

            if combined_data:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = crawler.count_detailed_nodes(combined_data)
                elapsed_time = time.time() - start_time

                # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
                print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
                print("=" * 60)
                crawler.print_combined_tree_structure(combined_data)
                print("=" * 60)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\næ•´åˆçˆ¬å–ç»Ÿè®¡:")
                print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
                print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
                print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
                print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
                print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
                print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

                # ä¿å­˜ç»“æœ
                filename = crawler.save_combined_result(combined_data, target_name=input_text)

                print(f"\nâœ“ æ•´åˆçˆ¬å–å®Œæˆ!")
                print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

                # æä¾›ä½¿ç”¨å»ºè®®
                print(f"\nä½¿ç”¨å»ºè®®:")
                print(f"- æŸ¥çœ‹JSONæ–‡ä»¶äº†è§£å®Œæ•´çš„æ•°æ®ç»“æ„")
                print(f"- æ ‘å½¢ç»“æ„ä¿æŒäº†ä»æ ¹ç›®å½•åˆ°ç›®æ ‡çš„å®Œæ•´è·¯å¾„")
                print(f"- æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«äº†ç›¸åº”çš„è¯¦ç»†å†…å®¹æ•°æ®")

            else:
                print("\nâœ— æ•´åˆçˆ¬å–å¤±è´¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            print(f"\nâœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            if verbose:
                import traceback
                traceback.print_exc()
    else:
        print_usage()

if __name__ == "__main__":
    main()
