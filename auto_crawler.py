#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•´åˆçˆ¬è™«å·¥å…· - ç»“åˆæ ‘å½¢ç»“æ„å’Œè¯¦ç»†å†…å®¹æå– + ä»£ç†æ±  + æœ¬åœ°æ–‡ä»¶å­˜å‚¨
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
æ”¯æŒå¤©å¯IPä»£ç†æ± ã€æœ¬åœ°æ–‡ä»¶å¤¹å­˜å‚¨ã€åª’ä½“æ–‡ä»¶ä¸‹è½½ç­‰åŠŸèƒ½
ç”¨æ³•: python auto_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python auto_crawler.py https://www.ifixit.com/Device/Television
      python auto_crawler.py Television
"""

import os
import sys
import json
import time
import random
import re
import requests
from urllib.parse import urlparse, urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
import hashlib
import mimetypes
import threading
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import queue

# å¯¼å…¥ä¸¤ä¸ªåŸºç¡€çˆ¬è™«
from enhanced_crawler import EnhancedIFixitCrawler
from tree_crawler import TreeCrawler


class ProxyManager:
    """ä»£ç†ç®¡ç†å™¨ - ä»æœ¬åœ°proxy.txtæ–‡ä»¶ç®¡ç†ä»£ç†æ± """

    def __init__(self, proxy_file="proxy.txt", username=None, password=None, max_reset_cycles=3):
        self.proxy_file = proxy_file
        self.username = username
        self.password = password

        self.proxy_pool = []
        self.current_proxy = None
        self.failed_proxies = []
        self.proxy_switch_count = 0
        self.proxy_lock = threading.Lock()

        # æ·»åŠ é‡ç½®å¾ªç¯é™åˆ¶
        self.max_reset_cycles = max_reset_cycles
        self.reset_cycle_count = 0

        # åŠ è½½ä»£ç†
        self._load_proxies()

    def _load_proxies(self):
        """ä»proxy.txtæ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨"""
        proxy_list = []

        try:
            if not os.path.exists(self.proxy_file):
                print(f"âŒ ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨: {self.proxy_file}")
                return

            with open(self.proxy_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()
                if line and ':' in line and not line.startswith('#'):
                    try:
                        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                        # 1. username:password@host:port (å¸¦è®¤è¯)
                        # 2. host:port (æ— è®¤è¯)
                        if '@' in line:
                            # å¸¦è®¤è¯çš„ä»£ç†æ ¼å¼
                            auth_part, host_port = line.split('@', 1)
                            username, password = auth_part.split(':', 1)
                            host, port = host_port.split(':', 1)
                            proxy_config = {
                                'http': f"http://{username}:{password}@{host.strip()}:{port.strip()}",
                                'https': f"http://{username}:{password}@{host.strip()}:{port.strip()}"
                            }
                        else:
                            # æ— è®¤è¯çš„ä»£ç†æ ¼å¼
                            host, port = line.split(':', 1)
                            if self.username and self.password:
                                # å¦‚æœè®¾ç½®äº†é»˜è®¤è®¤è¯ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤è®¤è¯
                                proxy_config = {
                                    'http': f"http://{self.username}:{self.password}@{host.strip()}:{port.strip()}",
                                    'https': f"http://{self.username}:{self.password}@{host.strip()}:{port.strip()}"
                                }
                            else:
                                # æ— è®¤è¯ä»£ç†
                                proxy_config = {
                                    'http': f"http://{host.strip()}:{port.strip()}",
                                    'https': f"http://{host.strip()}:{port.strip()}"
                                }
                        proxy_list.append(proxy_config)
                    except ValueError:
                        print(f"âš ï¸  è·³è¿‡æ ¼å¼é”™è¯¯çš„ä»£ç†: {line}")

            self.proxy_pool = proxy_list
            print(f"ğŸ“‹ ä» {self.proxy_file} åŠ è½½äº† {len(proxy_list)} ä¸ªä»£ç†")

        except Exception as e:
            print(f"âŒ è¯»å–ä»£ç†æ–‡ä»¶å¤±è´¥: {e}")

    def get_proxy(self):
        """è·å–ä¸€ä¸ªå¯ç”¨çš„ä»£ç†"""
        with self.proxy_lock:
            # å¦‚æœå½“å‰ä»£ç†å¯ç”¨ï¼Œç›´æ¥è¿”å›
            if self.current_proxy and self.current_proxy not in self.failed_proxies:
                return self.current_proxy

            # ä»ä»£ç†æ± ä¸­é€‰æ‹©ä¸€ä¸ªæœªå¤±æ•ˆçš„ä»£ç†
            available_proxies = []
            for p in self.proxy_pool:
                is_failed = False
                for failed in self.failed_proxies:
                    if p['http'] == failed['http']:
                        is_failed = True
                        break
                if not is_failed:
                    available_proxies.append(p)

            if not available_proxies:
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§é‡ç½®æ¬¡æ•°
                if self.reset_cycle_count >= self.max_reset_cycles:
                    print(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡ç½®æ¬¡æ•°({self.max_reset_cycles})ï¼Œæ‰€æœ‰ä»£ç†å‡ä¸å¯ç”¨")
                    return None

                # å¦‚æœæ‰€æœ‰ä»£ç†éƒ½å¤±æ•ˆäº†ï¼Œé‡ç½®å¤±æ•ˆåˆ—è¡¨ï¼Œé‡æ–°å¼€å§‹
                self.reset_cycle_count += 1
                print(f"âš ï¸  æ‰€æœ‰ä»£ç†éƒ½å·²å¤±æ•ˆï¼Œé‡ç½®ä»£ç†æ± ... (ç¬¬{self.reset_cycle_count}/{self.max_reset_cycles}æ¬¡)")
                self.failed_proxies.clear()
                available_proxies = self.proxy_pool

            if available_proxies:
                self.current_proxy = random.choice(available_proxies)
                self.proxy_switch_count += 1

                # æå–IPå’Œç«¯å£ç”¨äºæ˜¾ç¤º
                proxy_url = self.current_proxy['http']
                if '@' in proxy_url:
                    ip_port = proxy_url.split('@')[1]
                    if ':' in ip_port:
                        display_ip, display_port = ip_port.rsplit(':', 1)
                    else:
                        display_ip, display_port = ip_port, "Unknown"
                else:
                    display_ip, display_port = "Unknown", "Unknown"

                print(f"ğŸ”„ åˆ‡æ¢ä»£ç† (ç¬¬{self.proxy_switch_count}æ¬¡): {display_ip}:{display_port} [é‡ç½®å‘¨æœŸ: {self.reset_cycle_count}/{self.max_reset_cycles}]")

                return self.current_proxy
            else:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„ä»£ç†")
                return None

    def mark_proxy_failed(self, proxy, reason=""):
        """æ ‡è®°ä»£ç†ä¸ºå¤±æ•ˆ"""
        with self.proxy_lock:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å¤±æ•ˆåˆ—è¡¨ä¸­
            already_failed = False
            for failed in self.failed_proxies:
                if failed['http'] == proxy['http']:
                    already_failed = True
                    break

            if not already_failed:
                self.failed_proxies.append(proxy)

            # æå–IPå’Œç«¯å£ç”¨äºæ˜¾ç¤º
            proxy_url = proxy['http']
            if '@' in proxy_url:
                ip_port = proxy_url.split('@')[1]
                if ':' in ip_port:
                    display_ip, display_port = ip_port.rsplit(':', 1)
                else:
                    display_ip, display_port = ip_port, "Unknown"
            else:
                display_ip, display_port = "Unknown", "Unknown"

            print(f"âŒ ä»£ç†å¤±æ•ˆ: {display_ip}:{display_port} - {reason}")

            # å¦‚æœå¤±æ•ˆçš„æ˜¯å½“å‰ä»£ç†ï¼Œæ¸…é™¤å½“å‰ä»£ç†
            if self.current_proxy == proxy:
                self.current_proxy = None

    def get_stats(self):
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_proxies': len(self.proxy_pool),
            'failed_proxies': len(self.failed_proxies),
            'available_proxies': len(self.proxy_pool) - len(self.failed_proxies),
            'switch_count': self.proxy_switch_count,
            'reset_cycles': self.reset_cycle_count,
            'max_reset_cycles': self.max_reset_cycles
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.proxy_switch_count = 0
        self.reset_cycle_count = 0
        self.failed_proxies.clear()


class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - ä¼˜åŒ–çˆ¬è™«é‡å¤æ‰§è¡Œæ•ˆç‡"""

    def __init__(self, storage_root, logger=None):
        self.storage_root = Path(storage_root)
        self.logger = logger or logging.getLogger(__name__)
        self.cache_index_file = self.storage_root / "cache_index.json"
        self.cache_index = {}
        self.stats = {
            'total_urls': 0,
            'cached_urls': 0,
            'new_urls': 0,
            'invalid_cache': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        self.load_cache_index()

    def load_cache_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            if self.cache_index_file.exists():
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.cache_index = data.get('entries', {})
                    self.logger.info(f"å·²åŠ è½½ç¼“å­˜ç´¢å¼•ï¼ŒåŒ…å« {len(self.cache_index)} ä¸ªæ¡ç›®")
            else:
                self.cache_index = {}
                self.logger.info("ç¼“å­˜ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ç´¢å¼•")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
            self.cache_index = {}

    def save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            self.storage_root.mkdir(parents=True, exist_ok=True)
            cache_data = {
                'version': '1.0',
                'last_updated': datetime.now(timezone.utc).isoformat(),
                'entries': self.cache_index
            }
            with open(self.cache_index_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ç¼“å­˜ç´¢å¼•å·²ä¿å­˜ï¼ŒåŒ…å« {len(self.cache_index)} ä¸ªæ¡ç›®")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def get_url_hash(self, url):
        """ç”ŸæˆURLçš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

    def is_url_cached_and_valid(self, url, local_path):
        """æ£€æŸ¥URLæ˜¯å¦å·²ç¼“å­˜ä¸”æ•°æ®æœ‰æ•ˆ"""
        url_hash = self.get_url_hash(url)
        self.stats['total_urls'] += 1

        self.logger.info(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
        self.logger.info(f"   URLå“ˆå¸Œ: {url_hash}")
        self.logger.info(f"   æœ¬åœ°è·¯å¾„: {local_path}")

        # æ£€æŸ¥ç¼“å­˜ç´¢å¼•ä¸­æ˜¯å¦å­˜åœ¨
        if url_hash not in self.cache_index:
            self.logger.info(f"   âŒ ç¼“å­˜ç´¢å¼•ä¸­ä¸å­˜åœ¨æ­¤URL")
            self.stats['new_urls'] += 1
            self.stats['cache_misses'] += 1
            return False

        cache_entry = self.cache_index[url_hash]
        self.logger.info(f"   ğŸ“‹ æ‰¾åˆ°ç¼“å­˜æ¡ç›®: {cache_entry.get('processed_time', 'N/A')}")

        # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not local_path.exists():
            self.logger.info(f"   âŒ æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_path}")
            self.stats['invalid_cache'] += 1
            self.stats['cache_misses'] += 1
            return False

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if not self._validate_cached_data(local_path, cache_entry):
            self.logger.info(f"   âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            self.stats['invalid_cache'] += 1
            self.stats['cache_misses'] += 1
            return False

        self.logger.info(f"   âœ… ç¼“å­˜æœ‰æ•ˆï¼Œå‘½ä¸­!")
        self.stats['cached_urls'] += 1
        self.stats['cache_hits'] += 1
        return True

    def _validate_cached_data(self, local_path, cache_entry):
        """éªŒè¯ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§"""
        try:
            self.logger.info(f"   ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")

            # æ£€æŸ¥info.jsonæ–‡ä»¶
            info_file = local_path / "info.json"
            if not info_file.exists():
                self.logger.info(f"   âŒ info.jsonæ–‡ä»¶ä¸å­˜åœ¨")
                return False

            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            structure = cache_entry.get('structure', {})
            self.logger.info(f"   ğŸ“Š é¢„æœŸç»“æ„: {structure}")

            # éªŒè¯guidesç›®å½•å’Œæ–‡ä»¶
            if structure.get('has_guides', False):
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"   âŒ guidesç›®å½•ä¸å­˜åœ¨")
                    return False

                guide_files = list(guides_dir.glob("*.json"))
                expected_count = structure.get('guides_count', 0)
                if len(guide_files) != expected_count:
                    self.logger.info(f"   âŒ æŒ‡å—æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {len(guide_files)}")
                    return False
                else:
                    self.logger.info(f"   âœ… æŒ‡å—æ–‡ä»¶éªŒè¯é€šè¿‡: {len(guide_files)} ä¸ª")

            # éªŒè¯troubleshootingç›®å½•å’Œæ–‡ä»¶
            if structure.get('has_troubleshooting', False):
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"   âŒ troubleshootingç›®å½•ä¸å­˜åœ¨")
                    return False

                ts_files = list(ts_dir.glob("*.json"))
                expected_count = structure.get('troubleshooting_count', 0)
                if len(ts_files) != expected_count:
                    self.logger.info(f"   âŒ æ•…éšœæ’é™¤æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {len(ts_files)}")
                    return False
                else:
                    self.logger.info(f"   âœ… æ•…éšœæ’é™¤æ–‡ä»¶éªŒè¯é€šè¿‡: {len(ts_files)} ä¸ª")

            # éªŒè¯åª’ä½“æ–‡ä»¶
            if structure.get('has_media', False):
                media_dir = local_path / "media"
                if media_dir.exists():
                    media_files = list(media_dir.glob("*"))
                    if len(media_files) == 0 and structure.get('media_count', 0) > 0:
                        self.logger.info(f"   âŒ åª’ä½“æ–‡ä»¶ç¼ºå¤±")
                        return False
                    else:
                        self.logger.info(f"   âœ… åª’ä½“æ–‡ä»¶éªŒè¯é€šè¿‡: {len(media_files)} ä¸ª")

            self.logger.info(f"   âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            self.logger.error(f"   âŒ éªŒè¯ç¼“å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def add_to_cache(self, url, local_path, guides_count=0, troubleshooting_count=0, media_count=0):
        """æ·»åŠ URLåˆ°ç¼“å­˜ç´¢å¼•"""
        url_hash = self.get_url_hash(url)

        # åˆ†ææœ¬åœ°æ•°æ®ç»“æ„
        structure = self._analyze_local_structure(local_path, guides_count, troubleshooting_count, media_count)

        cache_entry = {
            'url': url,
            'local_path': str(local_path.relative_to(self.storage_root)),
            'processed_time': datetime.now(timezone.utc).isoformat(),
            'content_hash': self._generate_content_hash(local_path),
            'structure': structure,
            'status': 'complete'
        }

        self.cache_index[url_hash] = cache_entry
        self.logger.info(f"å·²æ·»åŠ åˆ°ç¼“å­˜: {url}")

    def _analyze_local_structure(self, local_path, guides_count=0, troubleshooting_count=0, media_count=0):
        """åˆ†ææœ¬åœ°æ•°æ®ç»“æ„"""
        structure = {
            'has_guides': False,
            'guides_count': 0,
            'has_troubleshooting': False,
            'troubleshooting_count': 0,
            'has_media': False,
            'media_count': 0
        }

        try:
            # æ£€æŸ¥guidesç›®å½•
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                guide_files = list(guides_dir.glob("*.json"))
                structure['has_guides'] = len(guide_files) > 0
                structure['guides_count'] = len(guide_files)
            elif guides_count > 0:
                structure['has_guides'] = True
                structure['guides_count'] = guides_count

            # æ£€æŸ¥troubleshootingç›®å½•
            ts_dir = local_path / "troubleshooting"
            if ts_dir.exists():
                ts_files = list(ts_dir.glob("*.json"))
                structure['has_troubleshooting'] = len(ts_files) > 0
                structure['troubleshooting_count'] = len(ts_files)
            elif troubleshooting_count > 0:
                structure['has_troubleshooting'] = True
                structure['troubleshooting_count'] = troubleshooting_count

            # æ£€æŸ¥mediaç›®å½•
            media_dir = local_path / "media"
            if media_dir.exists():
                media_files = list(media_dir.glob("*"))
                structure['has_media'] = len(media_files) > 0
                structure['media_count'] = len(media_files)
            elif media_count > 0:
                structure['has_media'] = True
                structure['media_count'] = media_count

        except Exception as e:
            self.logger.error(f"åˆ†ææœ¬åœ°ç»“æ„æ—¶å‡ºé”™: {e}")

        return structure

    def _generate_content_hash(self, local_path):
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼"""
        try:
            info_file = local_path / "info.json"
            if info_file.exists():
                with open(info_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œæ—¶å‡ºé”™: {e}")
        return ""

    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def display_cache_report(self):
        """æ˜¾ç¤ºç¼“å­˜æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æ™ºèƒ½ç¼“å­˜æŠ¥å‘Š")
        print("="*60)
        print(f"æ€»URLæ•°é‡: {self.stats['total_urls']}")
        print(f"ç¼“å­˜å‘½ä¸­: {self.stats['cached_urls']} (è·³è¿‡å¤„ç†)")
        print(f"éœ€è¦å¤„ç†: {self.stats['new_urls']} (æ–°å¢æˆ–æ›´æ–°)")
        print(f"æ— æ•ˆç¼“å­˜: {self.stats['invalid_cache']} (éœ€è¦é‡æ–°å¤„ç†)")

        if self.stats['total_urls'] > 0:
            hit_rate = (self.stats['cached_urls'] / self.stats['total_urls']) * 100
            print(f"ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")

        print("="*60)

    def clean_invalid_cache(self):
        """æ¸…ç†æ— æ•ˆçš„ç¼“å­˜æ¡ç›®"""
        invalid_keys = []

        for url_hash, cache_entry in self.cache_index.items():
            local_path = self.storage_root / cache_entry['local_path']
            if not local_path.exists() or not self._validate_cached_data(local_path, cache_entry):
                invalid_keys.append(url_hash)

        for key in invalid_keys:
            del self.cache_index[key]

        if invalid_keys:
            self.logger.info(f"å·²æ¸…ç† {len(invalid_keys)} ä¸ªæ— æ•ˆç¼“å­˜æ¡ç›®")
            self.save_cache_index()

        return len(invalid_keys)


class CombinedIFixitCrawler(EnhancedIFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", verbose=False, use_proxy=True,
                 use_cache=True, force_refresh=False, max_workers=4, max_retries=3,
                 download_videos=False, max_video_size_mb=50):
        super().__init__(base_url, verbose)

        # ç«‹å³åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œç¡®ä¿loggerå¯ç”¨
        self._setup_logging()

        self.tree_crawler = TreeCrawler(base_url)
        self.processed_nodes = set()
        self.target_url = None

        # æœ¬åœ°å­˜å‚¨é…ç½®ï¼ˆéœ€è¦åœ¨ç¼“å­˜ç®¡ç†å™¨ä¹‹å‰è®¾ç½®ï¼‰
        self.storage_root = "ifixit_data"
        self.media_folder = "media"

        # ç¼“å­˜é…ç½®
        self.use_cache = use_cache
        self.force_refresh = force_refresh
        self.cache_manager = CacheManager(self.storage_root, self.logger) if use_cache else None

        # ä»£ç†æ± é…ç½®
        self.use_proxy = use_proxy
        self.proxy_manager = ProxyManager() if use_proxy else None
        self.failed_urls = set()  # æ·»åŠ å¤±è´¥URLé›†åˆ

        # è§†é¢‘å¤„ç†é…ç½®
        self.download_videos = download_videos
        self.max_video_size_mb = max_video_size_mb
        self.video_extensions = ['.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv']

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "retry_success": 0,
            "retry_failed": 0,
            "total_retries": 0,  # æ·»åŠ ç¼ºå¤±çš„total_retriesé”®
            "media_downloaded": 0,
            "media_failed": 0,
            "videos_skipped": 0,
            "videos_downloaded": 0
        }

        # å¹¶å‘é…ç½®
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.thread_local = threading.local()

        # é”™è¯¯å¤„ç†é…ç½®
        self.failed_log_file = "failed_urls.log"

        # æ‰“å°è§†é¢‘å¤„ç†é…ç½®
        if self.download_videos:
            self.logger.info(f"âœ… è§†é¢‘ä¸‹è½½å·²å¯ç”¨ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {max_video_size_mb}MB")
        else:
            self.logger.info("âš ï¸ è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå°†ä¿ç•™åŸå§‹URL")

    def _load_proxy_config(self):
        """åŠ è½½ä»£ç†é…ç½®"""
        print("ğŸ“‹ ä½¿ç”¨å¤©å¯IPä»£ç†æœåŠ¡")

        # å¤©å¯IPä»£ç†è®¤è¯ä¿¡æ¯
        self.proxy_username = "yjnvjx"  # æ‚¨çš„å¤©å¯IPç”¨æˆ·å
        self.proxy_password = "gkmb3obc"  # æ‚¨çš„å¤©å¯IPå¯†ç 

        self.current_proxy = None
        self.proxy_switch_count = 0
        self.failed_urls = set()

        # è¿™äº›é…ç½®å·²åœ¨__init__ä¸­è®¾ç½®ï¼Œæ­¤å¤„ä¸å†é‡å¤

        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()

        if self.use_proxy:
            self._init_proxy_pool()

        # å¤åˆ¶æ ‘å½¢çˆ¬è™«çš„é‡è¦æ–¹æ³•åˆ°å½“å‰ç±»
        self.find_exact_path = self.tree_crawler.find_exact_path
        self.extract_breadcrumbs_from_page = self.tree_crawler.extract_breadcrumbs_from_page
        self._build_tree_from_path = self.tree_crawler._build_tree_from_path
        self._find_node_by_url = self.tree_crawler._find_node_by_url
        self.ensure_instruction_url_in_leaf_nodes = self.tree_crawler.ensure_instruction_url_in_leaf_nodes

        # é‡å†™tree_crawlerçš„get_soupæ–¹æ³•ï¼Œè®©å®ƒä½¿ç”¨æˆ‘ä»¬çš„å¢å¼ºç‰ˆæœ¬
        self.tree_crawler.get_soup = self._tree_crawler_get_soup

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO if self.verbose else logging.WARNING,
            format=log_format,
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _check_cache_validity(self, url, local_path):
        """æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ - ä½¿ç”¨æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
        if not self.use_cache or self.force_refresh:
            return False

        # ä½¿ç”¨CacheManagerè¿›è¡Œæ™ºèƒ½ç¼“å­˜æ£€æŸ¥
        if self.cache_manager:
            return self.cache_manager.is_url_cached_and_valid(url, local_path)

        # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰çš„ç®€å•ç¼“å­˜æ£€æŸ¥
        return self._legacy_cache_check(url, local_path)

    def _legacy_cache_check(self, url, local_path):
        """åŸæœ‰çš„ç¼“å­˜æ£€æŸ¥é€»è¾‘ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        if not local_path.exists():
            return False

        # æ£€æŸ¥info.jsonæ˜¯å¦å­˜åœ¨
        info_file = local_path / "info.json"
        if not info_file.exists():
            self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘info.json - {url}")
            return False

        try:
            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)

            # æ£€æŸ¥guidesç›®å½•
            if 'guides' in info_data:
                guides_dir = local_path / "guides"
                if not guides_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘guidesç›®å½• - {url}")
                    return False

            # æ£€æŸ¥troubleshootingç›®å½•
            if 'troubleshooting' in info_data:
                ts_dir = local_path / "troubleshooting"
                if not ts_dir.exists():
                    self.logger.info(f"ç¼“å­˜æ— æ•ˆ: ç¼ºå°‘troubleshootingç›®å½• - {url}")
                    return False

            # æ£€æŸ¥mediaæ–‡ä»¶
            media_dir = local_path / self.media_folder
            if media_dir.exists():
                # ç®€å•æ£€æŸ¥ï¼šå¦‚æœmediaç›®å½•å­˜åœ¨ä½†ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ä¸‹è½½å¤±è´¥
                media_files = list(media_dir.glob("*"))
                if len(media_files) == 0:
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶
                    if self._should_have_media(info_data):
                        self.logger.info(f"ç¼“å­˜æ— æ•ˆ: mediaç›®å½•ä¸ºç©ºä½†åº”è¯¥æœ‰åª’ä½“æ–‡ä»¶ - {url}")
                        return False

            self.logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡çˆ¬å–: {url}")
            return True

        except Exception as e:
            self.logger.error(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e} - {url}")
            return False

    def _should_have_media(self, data):
        """æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åº”è¯¥åŒ…å«åª’ä½“æ–‡ä»¶"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and value:
                    return True
                elif key == 'images' and isinstance(value, list) and value:
                    return True
                elif isinstance(value, (dict, list)):
                    if self._should_have_media(value):
                        return True
        elif isinstance(data, list):
            for item in data:
                if self._should_have_media(item):
                    return True
        return False

    def _log_failed_url(self, url, error, retry_count=0):
        """è®°å½•å¤±è´¥çš„URLåˆ°æ—¥å¿—æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"{timestamp} | {url} | {error} | retry_count: {retry_count}\n"

        try:
            with open(self.failed_log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
        except Exception as e:
            self.logger.error(f"æ— æ³•å†™å…¥å¤±è´¥æ—¥å¿—: {e}")

    def _log_failed_media(self, url, error_msg):
        """è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—"""
        try:
            failed_media_log = "failed_media.log"
            with open(failed_media_log, 'a', encoding='utf-8') as f:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{timestamp}] {url} - {error_msg}\n")
        except Exception as e:
            self.logger.error(f"æ— æ³•å†™å…¥åª’ä½“å¤±è´¥æ—¥å¿—: {e}")

    def _exponential_backoff(self, attempt):
        """æŒ‡æ•°é€€é¿å»¶è¿Ÿ"""
        delay = min(300, (2 ** attempt) + random.uniform(0, 1))
        time.sleep(delay)
        return delay

    def _is_temporary_error(self, error):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶æ€§é”™è¯¯"""
        temporary_errors = [
            "timeout", "connection", "network", "502", "503", "504",
            "429", "500", "ConnectionError", "Timeout", "ReadTimeout"
        ]
        error_str = str(error).lower()
        return any(temp_err in error_str for temp_err in temporary_errors)

    def _retry_with_backoff(self, func, *args, **kwargs):
        """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶"""
        last_error = None

        for attempt in range(self.max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    self.stats["retry_success"] += 1
                    self.logger.info(f"é‡è¯•æˆåŠŸ (ç¬¬{attempt+1}æ¬¡å°è¯•)")
                return result

            except Exception as e:
                last_error = e
                self.stats["total_retries"] += 1

                if not self._is_temporary_error(e):
                    self.logger.error(f"æ°¸ä¹…æ€§é”™è¯¯ï¼Œåœæ­¢é‡è¯•: {e}")
                    break

                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    self.logger.warning(f"é‡è¯• {attempt+1}/{self.max_retries} (å»¶è¿Ÿ{delay:.1f}s): {e}")

                    # ç½‘ç»œé”™è¯¯æ—¶åˆ‡æ¢ä»£ç†
                    if self.use_proxy and "network" in str(e).lower():
                        self._switch_proxy(f"é‡è¯•æ—¶ç½‘ç»œé”™è¯¯: {str(e)}")
                else:
                    self.logger.error(f"é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")

        self.stats["retry_failed"] += 1
        if last_error:
            raise last_error
        else:
            raise Exception("é‡è¯•å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯")

    def _init_proxy_pool(self):
        """åˆå§‹åŒ–ä»£ç†æ±  - ä»å¤©å¯IP APIè·å–ä»£ç†"""
        print("=" * 50)
        print("ğŸš€ å¯åŠ¨ä»£ç†æ± ç³»ç»Ÿ")
        print("=" * 50)
        self._get_new_proxy()

    def _get_new_proxy(self):
        """ä»å¤©å¯IP APIè·å–æ–°çš„ä»£ç†IP"""
        api_url = "http://api.tianqiip.com/getip"
        params = {
            'secret': 'c3ljdpezjim64de5',
            'num': '200',
            'type': 'txt',
            'port': '2',
            'time': '3',
            'mr': '1',
            'sign': '2704663b3f023a2e4097093d70f24acb'
        }

        try:
            print(f"ğŸ“¡ æ­£åœ¨ä»å¤©å¯IP APIè·å–ä»£ç†...")
            print(f"ğŸ”— APIåœ°å€: {api_url}")

            response = requests.get(api_url, params=params, timeout=15)

            if response.status_code == 200:
                proxy_text = response.text.strip()
                print(f"ğŸ” APIå“åº”å†…å®¹é¢„è§ˆ: {proxy_text[:200]}...")

                # è§£ææ–‡æœ¬æ ¼å¼çš„ä»£ç†åˆ—è¡¨ (IP:PORTæ ¼å¼)
                proxy_lines = [line.strip() for line in proxy_text.split('\n') if line.strip()]

                if proxy_lines:
                    # éšæœºé€‰æ‹©ä¸€ä¸ªä»£ç†
                    import random
                    selected_proxy = random.choice(proxy_lines)

                    if ':' in selected_proxy:
                        proxy_host, proxy_port = selected_proxy.split(':', 1)

                        # æ„å»ºå¸¦è®¤è¯çš„ä»£ç†é…ç½®
                        proxy_meta = f"http://{self.proxy_username}:{self.proxy_password}@{proxy_host}:{proxy_port}"
                        self.current_proxy = {
                            'http': proxy_meta,
                            'https': proxy_meta
                        }

                        self.proxy_switch_count += 1
                        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

                        print("âœ… å¤©å¯IPä»£ç†è·å–æˆåŠŸ!")
                        print(f"ğŸ•’ æ—¶é—´: {current_time}")
                        print(f"ğŸŒ ä»£ç†IP: {proxy_host}")
                        print(f"ğŸ”Œ ç«¯å£: {proxy_port}")
                        print(f"ğŸ”„ åˆ‡æ¢æ¬¡æ•°: ç¬¬ {self.proxy_switch_count} æ¬¡")
                        print(f"ğŸ“Š å¯ç”¨ä»£ç†æ€»æ•°: {len(proxy_lines)}")
                        print("=" * 50)

                        # ä»£ç†è·å–æˆåŠŸ

                        return True
                    else:
                        print(f"âŒ ä»£ç†æ ¼å¼é”™è¯¯: {selected_proxy}")
                else:
                    print("âŒ æœªè·å–åˆ°æœ‰æ•ˆçš„ä»£ç†åˆ—è¡¨")
            else:
                print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                print(f"âŒ å“åº”å†…å®¹: {response.text[:200]}")

        except Exception as e:
            print(f"âŒ è·å–å¤©å¯IPä»£ç†å¤±è´¥: {e}")

        # APIå¤±è´¥æ—¶ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼
        print("âš ï¸  å¤©å¯IP APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. å¤©å¯IPè´¦å·æ˜¯å¦æœ‰æ•ˆ")
        print("   2. è´¦å·ä½™é¢æ˜¯å¦å……è¶³")
        print("   3. APIå‚æ•°æ˜¯å¦æ­£ç¡®")
        print("   4. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        return self._get_demo_proxy()

    def _get_demo_proxy(self):
        """è·å–æ¨¡æ‹Ÿä»£ç†ç”¨äºæ¼”ç¤º"""
        import random

        # æ¨¡æ‹Ÿä»£ç†IPæ± 
        demo_proxies = [
            {"ip": "192.168.1.100", "port": "8080"},
            {"ip": "10.0.0.50", "port": "3128"},
            {"ip": "172.16.0.25", "port": "8888"},
        ]

        proxy_info = random.choice(demo_proxies)
        proxy_host = proxy_info['ip']
        proxy_port = proxy_info['port']

        # æ¼”ç¤ºæ¨¡å¼ä¹Ÿä½¿ç”¨è®¤è¯ä¿¡æ¯
        proxy_meta = f"http://{self.proxy_username}:{self.proxy_password}@{proxy_host}:{proxy_port}"
        self.current_proxy = {
            'http': proxy_meta,
            'https': proxy_meta
        }

        self.proxy_switch_count += 1
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

        print("âœ… æ¨¡æ‹Ÿä»£ç†è·å–æˆåŠŸ! (æ¼”ç¤ºæ¨¡å¼)")
        print(f"ğŸ•’ æ—¶é—´: {current_time}")
        print(f"ğŸŒ ä»£ç†IP: {proxy_host}")
        print(f"ğŸ”Œ ç«¯å£: {proxy_port}")
        print(f"ğŸ”„ åˆ‡æ¢æ¬¡æ•°: ç¬¬ {self.proxy_switch_count} æ¬¡")
        print("ğŸ’¡ æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œå®é™…ä½¿ç”¨éœ€è¦é…ç½®çœŸå®çš„å¤©å¯IP API")
        print("=" * 50)
        return True



    def _get_next_proxy(self):
        """è·å–å½“å‰ä»£ç†æˆ–åˆ‡æ¢æ–°ä»£ç†"""
        if not self.use_proxy or not self.proxy_manager:
            return None

        return self.proxy_manager.get_proxy()

    def _switch_proxy(self, reason="è¯·æ±‚å¤±è´¥"):
        """åˆ‡æ¢ä»£ç†IP"""
        self.logger.warning(f"âš ï¸  ä»£ç†åˆ‡æ¢åŸå› : {reason}")

        if not self.use_proxy or not self.proxy_manager:
            return False

        # æ ‡è®°å½“å‰ä»£ç†ä¸ºå¤±æ•ˆ
        current_proxy = self.proxy_manager.current_proxy
        if current_proxy:
            self.proxy_manager.mark_proxy_failed(current_proxy, reason)

        # è·å–æ–°ä»£ç†
        new_proxy = self.proxy_manager.get_proxy()
        return new_proxy is not None

    # ä»£ç†æ± ç›¸å…³æ–¹æ³•å·²é›†æˆåˆ°ProxyManagerç±»ä¸­



    def get_soup(self, url, use_playwright=False):
        """é‡å†™get_soupæ–¹æ³•ï¼Œæ”¯æŒä»£ç†æ± ã€é‡è¯•æœºåˆ¶å’ŒPlaywrightæ¸²æŸ“"""
        if not url:
            return None

        if url in self.failed_urls:
            self.logger.warning(f"è·³è¿‡å·²çŸ¥å¤±è´¥URL: {url}")
            return None

        if 'zh.ifixit.com' in url:
            url = url.replace('zh.ifixit.com', 'www.ifixit.com')

        if '?' in url:
            url += '&lang=en'
        else:
            url += '?lang=en'

        self.stats["total_requests"] += 1

        # å¦‚æœéœ€è¦JavaScriptæ¸²æŸ“ï¼Œä½¿ç”¨Playwright
        if use_playwright:
            return self._retry_with_backoff(self._get_soup_with_playwright, url)

        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„requestsæ–¹æ³•
        return self._retry_with_backoff(self._get_soup_requests, url)

    def _get_soup_requests(self, url):
        """ä½¿ç”¨requestsè·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒæ™ºèƒ½ä»£ç†åˆ‡æ¢"""
        session = requests.Session()
        retry_strategy = Retry(
            total=1,  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œé¿å…ä¸ä¸Šå±‚é‡è¯•æœºåˆ¶å†²çª
            backoff_factor=1,  # å‡å°‘é€€é¿å› å­
            status_forcelist=[429, 500, 502, 503, 504],
            raise_on_status=False  # ä¸åœ¨çŠ¶æ€ç é”™è¯¯æ—¶ç«‹å³æŠ›å‡ºå¼‚å¸¸
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è·å–ä»£ç†
        proxies = self._get_next_proxy() if self.use_proxy else None

        try:
            response = session.get(
                url,
                headers=self.headers,
                timeout=30,
                proxies=proxies
            )
            response.raise_for_status()

            from bs4 import BeautifulSoup
            return BeautifulSoup(response.content, 'html.parser')

        except (requests.exceptions.ProxyError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ConnectionError) as e:
            # ä»£ç†ç›¸å…³é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†
            if self.use_proxy:
                print(f"ğŸ”„ ä»£ç†é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢: {str(e)[:100]}")
                if self._switch_proxy(f"ä»£ç†é”™è¯¯: {str(e)[:50]}"):
                    # ä½¿ç”¨æ–°ä»£ç†é‡è¯•ä¸€æ¬¡
                    new_proxies = self._get_next_proxy()
                    response = session.get(
                        url,
                        headers=self.headers,
                        timeout=30,
                        proxies=new_proxies
                    )
                    response.raise_for_status()

                    from bs4 import BeautifulSoup
                    return BeautifulSoup(response.content, 'html.parser')
            # å¦‚æœåˆ‡æ¢ä»£ç†å¤±è´¥æˆ–ä¸ä½¿ç”¨ä»£ç†ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸
            raise

    def _get_soup_with_playwright(self, url):
        """ä½¿ç”¨Playwrightè·å–JavaScriptæ¸²æŸ“åçš„é¡µé¢å†…å®¹"""
        try:
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…åŠ è½½
                page.goto(url, wait_until='networkidle')

                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                browser.close()

                from bs4 import BeautifulSoup
                return BeautifulSoup(content, 'html.parser')

        except Exception as e:
            print(f"Playwrightè·å–é¡µé¢å¤±è´¥ {url}: {e}")
            self.failed_urls.add(url)
            return None

    def _create_local_directory(self, path):
        """åˆ›å»ºæœ¬åœ°ç›®å½•ç»“æ„"""
        full_path = Path(self.storage_root) / path
        full_path.mkdir(parents=True, exist_ok=True)
        return full_path

    def _get_file_size_from_url(self, url):
        """è·å–URLæ–‡ä»¶çš„å¤§å°ï¼ˆMBï¼‰"""
        try:
            # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
            if '.thumbnail.medium' in url:
                url = url.replace('.thumbnail.medium', '.medium')

            # ä½¿ç”¨ä¸ä¸‹è½½ç›¸åŒçš„è¯·æ±‚å¤´
            media_headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://www.ifixit.com/",
                "Origin": "https://www.ifixit.com",
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "same-site",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }

            proxies = self._get_next_proxy() if self.use_proxy else None
            response = requests.head(url, headers=media_headers, timeout=10, proxies=proxies)
            if response.status_code == 200:
                content_length = response.headers.get('content-length')
                if content_length:
                    size_bytes = int(content_length)
                    size_mb = size_bytes / (1024 * 1024)
                    return size_mb
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° {url}: {e}")
        return None

    def _is_video_file(self, url):
        """æ£€æŸ¥URLæ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
        if not url:
            return False
        url_path = url.split('?')[0].lower()
        return any(url_path.endswith(ext) for ext in self.video_extensions)

    def _download_media_file(self, url, local_dir, filename=None):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œè§†é¢‘å¤„ç†ç­–ç•¥ï¼Œæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰"""
        if not url or not url.startswith('http'):
            return None

        # å…ˆè®¡ç®—æ–‡ä»¶åå’Œæœ¬åœ°è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            url_path = url.split('?')[0]
            if '.' in url_path:
                ext = '.' + url_path.split('.')[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                    ext = '.jpg'
            else:
                ext = '.jpg'
            filename = f"{url_hash}{ext}"

        local_path = local_dir / self.media_folder / filename

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸ä½¿ç”¨è·¨é¡µé¢å»é‡
        is_troubleshooting = "troubleshooting" in str(local_dir)

        if not is_troubleshooting:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ”¯æŒè·¨é¡µé¢å»é‡ï¼‰
            existing_file_path = self._find_existing_media_file(url, filename)
            if existing_file_path:
                # æ–‡ä»¶å·²å­˜åœ¨äºå…¶ä»–ä½ç½®ï¼Œè¿”å›ç›¸å¯¹äºå½“å‰ç›®å½•çš„è·¯å¾„
                if self.verbose:
                    self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                else:
                    # é™é»˜è·³è¿‡ï¼Œä¸æ˜¾ç¤ºæ¶ˆæ¯é¿å…æ—¥å¿—è¿‡å¤š
                    pass
                self.stats["media_downloaded"] += 1
                return existing_file_path

        # å¦‚æœæ–‡ä»¶åœ¨å½“å‰ç›®å½•å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„
        if local_path.exists():
            if self.verbose:
                self.logger.info(f"åª’ä½“æ–‡ä»¶å·²å­˜åœ¨äºå½“å‰ç›®å½•: {filename}")
            self.stats["media_downloaded"] += 1
            # å¯¹äºtroubleshootingç›®å½•ï¼Œè¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
            if is_troubleshooting:
                return str(local_path.relative_to(local_dir))
            else:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
                try:
                    return str(local_path.relative_to(Path(self.storage_root)))
                except ValueError:
                    # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                    return str(local_path.relative_to(local_dir))

        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
        if self._is_video_file(url):
            if not self.download_videos:
                self.logger.info(f"è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰: {url}")
                self.stats["videos_skipped"] += 1
                return url

            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size_mb = self._get_file_size_from_url(url)
            if file_size_mb and file_size_mb > self.max_video_size_mb:
                self.logger.warning(f"è·³è¿‡å¤§è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB > {self.max_video_size_mb}MB): {url}")
                self.stats["videos_skipped"] += 1
                return url

            self.logger.info(f"å‡†å¤‡ä¸‹è½½è§†é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB): {url}")

        try:
            result = self._retry_with_backoff(self._download_media_file_impl, url, local_dir, filename)
            if self._is_video_file(url) and result != url:
                self.stats["videos_downloaded"] += 1
            return result
        except Exception as e:
            self.logger.error(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æœ€ç»ˆå¤±è´¥ {url}: {e}")
            self.stats["media_failed"] += 1
            self._log_failed_url(url, f"åª’ä½“ä¸‹è½½å¤±è´¥: {str(e)}")
            # è®°å½•å¤±è´¥çš„åª’ä½“æ–‡ä»¶åˆ°ä¸“é—¨çš„æ—¥å¿—
            self._log_failed_media(url, str(e))
            return url

    def _find_existing_media_file(self, url, filename):
        """æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„åª’ä½“æ–‡ä»¶ï¼ˆè·¨é¡µé¢å»é‡ï¼‰"""
        try:
            # åŸºäºURLçš„MD5å“ˆå¸Œæ¥æŸ¥æ‰¾æ–‡ä»¶
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]

            # åœ¨æ•´ä¸ªstorage_rootç›®å½•ä¸‹æœç´¢å…·æœ‰ç›¸åŒå“ˆå¸Œçš„æ–‡ä»¶
            storage_path = Path(self.storage_root)
            if not storage_path.exists():
                return None

            # æœç´¢æ‰€æœ‰mediaç›®å½•ä¸‹çš„æ–‡ä»¶
            for media_dir in storage_path.rglob("media"):
                if media_dir.is_dir():
                    # æŸ¥æ‰¾å…·æœ‰ç›¸åŒå“ˆå¸Œå‰ç¼€çš„æ–‡ä»¶
                    for existing_file in media_dir.glob(f"{url_hash}.*"):
                        if existing_file.is_file():
                            # è¿”å›ç›¸å¯¹äºstorage_rootçš„è·¯å¾„
                            return str(existing_file.relative_to(storage_path))

            return None

        except Exception as e:
            if self.verbose:
                self.logger.warning(f"æŸ¥æ‰¾å·²å­˜åœ¨åª’ä½“æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def _download_media_file_impl(self, url, local_dir, filename):
        """åª’ä½“æ–‡ä»¶ä¸‹è½½çš„å…·ä½“å®ç°"""
        local_path = local_dir / self.media_folder / filename
        local_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿®å¤URLæ ¼å¼ï¼šå°†.thumbnail.mediumæ›¿æ¢ä¸º.mediumï¼Œè§£å†³403 Forbiddené—®é¢˜
        if '.thumbnail.medium' in url:
            url = url.replace('.thumbnail.medium', '.medium')
            if self.verbose:
                self.logger.info(f"URLæ ¼å¼ä¿®å¤: .thumbnail.medium -> .medium")

        # ä¸ºåª’ä½“æ–‡ä»¶ä¸‹è½½åˆ›å»ºä¸“é—¨çš„è¯·æ±‚å¤´
        media_headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://www.ifixit.com/",
            "Origin": "https://www.ifixit.com",
            "Sec-Fetch-Dest": "image",
            "Sec-Fetch-Mode": "no-cors",
            "Sec-Fetch-Site": "same-site",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }

        proxies = self._get_next_proxy() if self.use_proxy else None
        response = requests.get(url, headers=media_headers, timeout=30, proxies=proxies)
        response.raise_for_status()

        with open(local_path, 'wb') as f:
            f.write(response.content)

        self.stats["media_downloaded"] += 1
        if self.verbose:
            self.logger.info(f"åª’ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename}")
        else:
            # ç®€åŒ–çš„è¿›åº¦æç¤º
            if self.stats["media_downloaded"] % 10 == 0:
                print(f"      ğŸ“¥ å·²ä¸‹è½½ {self.stats['media_downloaded']} ä¸ªåª’ä½“æ–‡ä»¶...")

        # æ£€æŸ¥æ˜¯å¦æ˜¯troubleshootingç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™è¿”å›ç›¸å¯¹äºtroubleshootingç›®å½•çš„è·¯å¾„
        is_troubleshooting = "troubleshooting" in str(local_dir)
        if is_troubleshooting:
            return str(local_path.relative_to(local_dir))
        else:
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœlocal_dirä¸æ˜¯storage_rootçš„å­ç›®å½•ï¼Œåˆ™ä½¿ç”¨local_dirä½œä¸ºåŸºå‡†
            try:
                return str(local_path.relative_to(Path(self.storage_root)))
            except ValueError:
                # å¦‚æœä¸åœ¨storage_rootä¸‹ï¼Œè¿”å›ç›¸å¯¹äºlocal_dirçš„è·¯å¾„
                return str(local_path.relative_to(local_dir))

    def _process_media_urls(self, data, local_dir):
        """é€’å½’å¤„ç†æ•°æ®ä¸­çš„åª’ä½“URLï¼Œä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key in ['image', 'video', 'thumbnail', 'photo'] and isinstance(value, str) and value.startswith('http'):
                    data[key] = self._download_media_file(value, local_dir)
                elif key == 'images' and isinstance(value, list):
                    for i, img_item in enumerate(value):
                        if isinstance(img_item, str) and img_item.startswith('http'):
                            # ç›´æ¥æ˜¯URLå­—ç¬¦ä¸²
                            value[i] = self._download_media_file(img_item, local_dir)
                        elif isinstance(img_item, dict) and 'url' in img_item:
                            # æ˜¯åŒ…å«urlå­—æ®µçš„å­—å…¸
                            img_url = img_item['url']
                            if isinstance(img_url, str) and img_url.startswith('http'):
                                img_item['url'] = self._download_media_file(img_url, local_dir)
                elif key == 'videos' and isinstance(value, list):
                    for i, video_item in enumerate(value):
                        if isinstance(video_item, str) and video_item.startswith('http'):
                            # ç›´æ¥æ˜¯URLå­—ç¬¦ä¸²
                            if self.download_videos:
                                value[i] = self._download_media_file(video_item, local_dir)
                        elif isinstance(video_item, dict) and 'url' in video_item:
                            # æ˜¯åŒ…å«urlå­—æ®µçš„å­—å…¸
                            video_url = video_item['url']
                            if isinstance(video_url, str) and video_url.startswith('http'):
                                if self.download_videos:
                                    video_item['url'] = self._download_media_file(video_url, local_dir)
                elif isinstance(value, (dict, list)):
                    self._process_media_urls(value, local_dir)
        elif isinstance(data, list):
            for item in data:
                self._process_media_urls(item, local_dir)

    def _tree_crawler_get_soup(self, url):
        """ä¸ºtree_crawleræä¾›çš„get_soupæ–¹æ³•ï¼Œåœ¨éœ€è¦é¢åŒ…å±‘å¯¼èˆªæ—¶ä½¿ç”¨Playwright"""
        # å¯¹äºéœ€è¦æå–é¢åŒ…å±‘å¯¼èˆªçš„é¡µé¢ï¼Œä½¿ç”¨Playwright
        if '/Teardown/' in url or '/Guide/' in url or '/Device/' in url:
            return self.get_soup(url, use_playwright=True)
        else:
            return self.get_soup(url, use_playwright=False)

    def extract_real_name_from_url(self, url):
        """
        ä»URLä¸­æå–çœŸå®çš„nameå­—æ®µï¼ˆURLè·¯å¾„çš„æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰
        """
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_url = url.split('?')[0].split('#')[0]

        # ç§»é™¤æœ«å°¾çš„æ–œæ 
        clean_url = clean_url.rstrip('/')

        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µ
        path_parts = clean_url.split('/')
        if path_parts:
            name = path_parts[-1]
            # URLè§£ç 
            import urllib.parse
            name = urllib.parse.unquote(name)
            # ä¿®å¤å¼•å·ç¼–ç é—®é¢˜ï¼šå°† \" è½¬æ¢ä¸º "
            name = name.replace('\\"', '"')
            return name

        return ""

    def extract_real_title_from_page(self, soup):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ
        """
        if not soup:
            return ""

        # ä¼˜å…ˆä»h1æ ‡ç­¾æå–
        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        # å…¶æ¬¡ä»titleæ ‡ç­¾æå–
        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    def extract_real_title_from_page(self, soup):
        """ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ"""
        if not soup:
            return ""

        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    # ç¬¬ä¸€ä¸ªextract_real_view_statisticsæ–¹æ³•å·²ç§»é™¤ï¼Œä½¿ç”¨ä¸‹é¢çš„å®ç°

    def _is_specified_target_url(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        """
        if not self.target_url or not url:
            return False

        # æ ‡å‡†åŒ–URLè¿›è¡Œæ¯”è¾ƒ
        def normalize_url(u):
            import urllib.parse
            # URLè§£ç ï¼Œç„¶åæ ‡å‡†åŒ–
            decoded = urllib.parse.unquote(u)
            return decoded.rstrip('/').lower()

        return normalize_url(url) == normalize_url(self.target_url)

    def discover_subcategories_from_page(self, soup, base_url):
        """
        åŠ¨æ€å‘ç°é¡µé¢ä¸­çš„å­ç±»åˆ«é“¾æ¥
        """
        if not soup:
            return []

        subcategories = []

        # æŸ¥æ‰¾è®¾å¤‡ç±»åˆ«é“¾æ¥çš„å¤šç§é€‰æ‹©å™¨
        category_selectors = [
            'a[href*="/Device/"]',  # åŒ…å«/Device/çš„é“¾æ¥
            '.category-link',       # ç±»åˆ«é“¾æ¥ç±»
            '.device-link',         # è®¾å¤‡é“¾æ¥ç±»
            '.subcategory',         # å­ç±»åˆ«ç±»
            'a[href^="/Device/"]'   # ä»¥/Device/å¼€å¤´çš„é“¾æ¥
        ]

        found_links = set()

        for selector in category_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/Device/' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue

                    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
                    skip_patterns = [
                        '/Guide/', '/Troubleshooting/', '/Edit/', '/History/',
                        '/Answers/', '?revision', 'action=edit', 'action=create'
                    ]

                    if not any(pattern in full_url for pattern in skip_patterns):
                        # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå­ç±»åˆ«ï¼ˆURLæ¯”å½“å‰é¡µé¢æ›´æ·±ä¸€å±‚ï¼‰
                        if full_url != base_url and full_url not in found_links:
                            # æå–é“¾æ¥æ–‡æœ¬ä½œä¸ºåç§°
                            link_text = link.get_text().strip()
                            if link_text:
                                subcategories.append({
                                    'name': self.extract_real_name_from_url(full_url),
                                    'url': full_url,
                                    'title': link_text
                                })
                                found_links.add(full_url)

        return subcategories

    def extract_real_view_statistics(self, soup, url):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®
        """
        if not soup:
            return {}

        stats = {}

        # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®çš„å„ç§å¯èƒ½ä½ç½®
        stat_selectors = [
            '.view-count',
            '.stats-item',
            '.view-stats',
            '[data-stats]',
            '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # è§£æç»Ÿè®¡æ•°æ®æ ¼å¼
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»APIæˆ–å…¶ä»–ä½ç½®è·å–
        if not stats:
            # å°è¯•ä»é¡µé¢çš„JavaScriptæ•°æ®ä¸­æå–
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        # æŸ¥æ‰¾å¯èƒ½çš„ç»Ÿè®¡æ•°æ®
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def build_correct_url_from_path(self, base_url, path_segments):
        """
        æ ¹æ®è·¯å¾„æ®µæ­£ç¡®æ„å»ºURLï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
        """
        if not path_segments:
            return base_url

        # ä»base_urlå¼€å§‹ï¼Œé€æ­¥æ›¿æ¢è·¯å¾„æ®µ
        current_url = base_url

        for segment in path_segments:
            # è·å–å½“å‰URLçš„è·¯å¾„éƒ¨åˆ†
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(current_url)
            path_parts = [p for p in str(parsed.path).split('/') if p]

            # æ·»åŠ æ–°çš„è·¯å¾„æ®µ
            path_parts.append(str(segment))

            # é‡æ–°æ„å»ºURL
            new_path = '/' + '/'.join(path_parts)
            new_parsed = parsed._replace(path=new_path)
            current_url = urlunparse(new_parsed)

        return current_url

    def restructure_target_content(self, node, is_target_page=False):
        """
        é‡æ„ç›®æ ‡æ–‡ä»¶çš„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨guides[]å’Œtroubleshooting[]è€Œéchildren[]
        """
        if not node or not isinstance(node, dict):
            return node

        # å¦‚æœè¿™æ˜¯ç›®æ ‡é¡µé¢ä¸”æœ‰childrenåŒ…å«guideså’Œtroubleshooting
        if is_target_page and 'children' in node and node['children']:
            guides = []
            troubleshooting = []
            real_children = []

            for child in node['children']:
                if 'steps' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæŒ‡å—
                    guide_data = {k: v for k, v in child.items() if k not in ['children']}
                    guides.append(guide_data)
                elif 'causes' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæ•…éšœæ’é™¤
                    ts_data = {k: v for k, v in child.items() if k not in ['children']}
                    troubleshooting.append(ts_data)
                else:
                    # è¿™æ˜¯çœŸæ­£çš„å­ç±»åˆ«
                    real_children.append(child)

            # é‡æ„èŠ‚ç‚¹
            if guides:
                node['guides'] = guides
            if troubleshooting:
                node['troubleshooting'] = troubleshooting

            # åªæœ‰çœŸæ­£çš„å­ç±»åˆ«æ‰ä¿ç•™åœ¨childrenä¸­
            if real_children:
                node['children'] = real_children
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„å­ç±»åˆ«ï¼Œç§»é™¤childrenå­—æ®µ
                if 'children' in node:
                    del node['children']

        return node

    def fix_node_data(self, node, soup=None):
        """ä¿®å¤èŠ‚ç‚¹çš„nameå­—æ®µï¼Œåªåœ¨æœ€ç»ˆäº§å“é¡µé¢æ·»åŠ å…¶ä»–å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_root_device = url == f"{self.base_url}/Device"

        # æ‰€æœ‰èŠ‚ç‚¹éƒ½ä¿®å¤nameå­—æ®µ
        real_name = self.extract_real_name_from_url(url)
        if real_name:
            node['name'] = real_name

        # åªæœ‰åœ¨deep_crawl_product_contentä¸­è¢«è¯†åˆ«ä¸ºç›®æ ‡é¡µé¢æ—¶æ‰æ·»åŠ å…¶ä»–å­—æ®µ
        # è¿™é‡Œä¸æ·»åŠ titleã€instruction_urlã€view_statisticsç­‰å­—æ®µ
        # è¿™äº›å­—æ®µå°†åœ¨deep_crawl_product_contentæ–¹æ³•ä¸­æ·»åŠ 

        return node

    def extract_what_you_need_enhanced(self, guide_url):
        """
        å¢å¼ºç‰ˆ"What You Need"éƒ¨åˆ†æå–æ–¹æ³•ï¼Œç¡®ä¿çœŸå®é¡µé¢è®¿é—®å’Œæ•°æ®å®Œæ•´æ€§
        """
        if not guide_url:
            return {}

        print(f"    å¢å¼ºæå–What You Need: {guide_url}")

        try:
            # ä½¿ç”¨Playwrightè·å–å®Œæ•´æ¸²æŸ“çš„é¡µé¢
            from playwright.sync_api import sync_playwright

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                # è®¾ç½®è¯·æ±‚å¤´ç¡®ä¿è‹±æ–‡å†…å®¹
                page.set_extra_http_headers({
                    'Accept-Language': 'en-US,en;q=0.9',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                })

                # å¯¼èˆªåˆ°é¡µé¢å¹¶ç­‰å¾…å®Œå…¨åŠ è½½
                page.goto(guide_url, wait_until='networkidle')
                page.wait_for_timeout(5000)  # å¢åŠ ç­‰å¾…æ—¶é—´ç¡®ä¿å®Œå…¨åŠ è½½

                # è·å–é¡µé¢HTML
                html_content = page.content()
                browser.close()

                # è§£æHTML
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')

                # å°è¯•å¤šç§æ–¹æ³•æå–"What You Need"æ•°æ®
                what_you_need = {}

                # æ–¹æ³•1ï¼šä»Reactç»„ä»¶çš„data-propsä¸­æå–ï¼ˆæœ€å‡†ç¡®ï¼‰
                what_you_need = self._extract_from_react_props_enhanced(soup)

                # æ–¹æ³•2ï¼šå¦‚æœReactæ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢çš„What You NeedåŒºåŸŸæå–
                if not what_you_need:
                    what_you_need = self._extract_from_what_you_need_section(soup)

                # æ–¹æ³•3ï¼šå¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢çš„äº§å“é“¾æ¥æå–
                if not what_you_need:
                    what_you_need = self._extract_from_product_links(soup)

                # æ–¹æ³•4ï¼šæœ€åå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
                if not what_you_need:
                    what_you_need = self._extract_from_page_text(soup)

                if what_you_need:
                    print(f"    æˆåŠŸæå–åˆ°: {list(what_you_need.keys())}")
                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    total_items = sum(len(items) if isinstance(items, list) else 1
                                    for items in what_you_need.values())
                    print(f"    æ€»è®¡é¡¹ç›®æ•°: {total_items}")
                else:
                    print(f"    æœªæ‰¾åˆ°What You Needæ•°æ®")

                return what_you_need

        except Exception as e:
            print(f"    å¢å¼ºæå–å¤±è´¥: {str(e)}")
            return {}

    def _extract_from_react_props_enhanced(self, soup):
        """ä»Reactç»„ä»¶çš„data-propsä¸­æå–What you needæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾å¤šç§å¯èƒ½çš„Reactç»„ä»¶
            component_selectors = [
                'div[data-name="GuideTopComponent"]',
                'div[data-name="GuideComponent"]',
                'div[data-name="WhatYouNeedComponent"]',
                'div[data-props*="tools"]',
                'div[data-props*="parts"]',
                'div[data-props*="kits"]',
                'div[data-props*="materials"]'
            ]

            for selector in component_selectors:
                component = soup.select_one(selector)
                if component:
                    data_props = component.get('data-props')
                    if data_props:
                        try:
                            import json
                            import html

                            # HTMLè§£ç å¹¶è§£æJSON
                            decoded_props = html.unescape(data_props)
                            props_data = json.loads(decoded_props)

                            # æå–productData
                            product_data = props_data.get('productData', {})

                            # æå–Fix Kits/Parts
                            kits = product_data.get('kits', [])
                            if kits:
                                fix_kits = []
                                parts = []
                                materials = []
                                for kit in kits:
                                    name = kit.get('name', '').strip()
                                    if name:
                                        name = self.clean_product_name(name)
                                        if name:
                                            if 'kit' in name.lower() or 'fix kit' in name.lower():
                                                fix_kits.append(name)
                                            elif 'material' in name.lower():
                                                materials.append(name)
                                            else:
                                                parts.append(name)

                                if fix_kits:
                                    what_you_need['Fix Kits'] = fix_kits
                                if parts:
                                    what_you_need['Parts'] = parts
                                if materials:
                                    what_you_need['Materials'] = materials

                            # æå–Tools
                            tools = product_data.get('tools', [])
                            if tools:
                                tool_names = []
                                for tool in tools:
                                    name = tool.get('name', '').strip()
                                    if name:
                                        name = self.clean_product_name(name)
                                        if name:
                                            tool_names.append(name)

                                if tool_names:
                                    what_you_need['Tools'] = tool_names

                            # æå–å…¶ä»–å¯èƒ½çš„ç±»åˆ«
                            for category_key in ['supplies', 'components', 'accessories']:
                                category_items = product_data.get(category_key, [])
                                if category_items:
                                    category_names = []
                                    for item in category_items:
                                        name = item.get('name', '').strip()
                                        if name:
                                            name = self.clean_product_name(name)
                                            if name:
                                                category_names.append(name)

                                    if category_names:
                                        category_title = category_key.title()
                                        what_you_need[category_title] = category_names

                            if what_you_need:
                                break

                        except Exception as e:
                            continue

            return what_you_need

        except Exception as e:
            return what_you_need

    def _extract_from_what_you_need_section(self, soup):
        """ä»é¡µé¢çš„What You Needä¸“é—¨åŒºåŸŸæå–æ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾What You Needæ ‡é¢˜
            what_you_need_headers = soup.find_all(string=lambda text: text and
                'what you need' in text.lower())

            for header in what_you_need_headers:
                parent = header.parent
                if parent:
                    # æŸ¥æ‰¾çˆ¶çº§å®¹å™¨
                    container = parent.find_parent(['div', 'section'])
                    if container:
                        # æŸ¥æ‰¾å·¥å…·å’Œé›¶ä»¶åˆ—è¡¨
                        lists = container.find_all(['ul', 'ol'])
                        for list_elem in lists:
                            items = list_elem.find_all('li')
                            if items:
                                category_items = []
                                for item in items:
                                    text = item.get_text().strip()
                                    if text and len(text) > 2:
                                        cleaned_text = self.clean_product_name(text)
                                        if cleaned_text:
                                            category_items.append(cleaned_text)

                                if category_items:
                                    # æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ç±»åˆ«
                                    context = container.get_text().lower()
                                    if 'tool' in context:
                                        what_you_need['Tools'] = category_items
                                    elif 'part' in context:
                                        what_you_need['Parts'] = category_items
                                    elif 'kit' in context:
                                        what_you_need['Fix Kits'] = category_items
                                    else:
                                        what_you_need['Items'] = category_items
                                    break

                        if what_you_need:
                            break

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_product_links(self, soup):
        """ä»é¡µé¢çš„äº§å“é“¾æ¥ä¸­æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾äº§å“é“¾æ¥
            product_links = soup.find_all('a', href=lambda x: x and
                any(domain in x for domain in ['ifixit.com/products', 'ifixit.com/store']))

            tools = []
            parts = []
            fix_kits = []

            for link in product_links:
                text = link.get_text().strip()
                if text and len(text) > 2:
                    cleaned_text = self.clean_product_name(text)
                    if cleaned_text:
                        text_lower = cleaned_text.lower()
                        if any(tool_word in text_lower for tool_word in
                               ['screwdriver', 'spudger', 'pick', 'driver', 'tool']):
                            tools.append(cleaned_text)
                        elif any(kit_word in text_lower for kit_word in
                                ['kit', 'set']):
                            fix_kits.append(cleaned_text)
                        else:
                            parts.append(cleaned_text)

            if tools:
                what_you_need['Tools'] = list(dict.fromkeys(tools))  # å»é‡
            if parts:
                what_you_need['Parts'] = list(dict.fromkeys(parts))
            if fix_kits:
                what_you_need['Fix Kits'] = list(dict.fromkeys(fix_kits))

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_html_structure(self, soup):
        """ä»HTMLç»“æ„ä¸­ç›´æ¥æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾"What You Need"æ ‡é¢˜
            what_you_need_headers = soup.find_all(string=lambda text: text and
                any(phrase in text.lower() for phrase in ['what you need', 'tools', 'parts', 'materials']))

            for header in what_you_need_headers:
                parent = header.parent
                if parent:
                    # æŸ¥æ‰¾åç»­çš„åˆ—è¡¨æˆ–å†…å®¹
                    next_elements = parent.find_next_siblings(['ul', 'ol', 'div', 'section'])
                    for elem in next_elements:
                        # æå–åˆ—è¡¨é¡¹
                        items = elem.find_all(['li', 'a'])
                        if items:
                            category_items = []
                            for item in items:
                                text = item.get_text().strip()
                                if text and len(text) > 2:
                                    text = self.clean_product_name(text)
                                    if text:
                                        category_items.append(text)

                            if category_items:
                                # æ ¹æ®å†…å®¹åˆ¤æ–­ç±»åˆ«
                                header_text = header.lower()
                                if 'tool' in header_text:
                                    what_you_need['Tools'] = category_items
                                elif 'part' in header_text:
                                    what_you_need['Parts'] = category_items
                                elif 'kit' in header_text:
                                    what_you_need['Fix Kits'] = category_items
                                else:
                                    what_you_need['Items'] = category_items
                                break

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_from_page_text(self, soup):
        """ä»é¡µé¢æ–‡æœ¬ä¸­æå–What You Needæ•°æ®"""
        what_you_need = {}

        try:
            # æŸ¥æ‰¾åŒ…å«å·¥å…·å’Œé›¶ä»¶ä¿¡æ¯çš„æ–‡æœ¬
            text_content = soup.get_text()
            lines = text_content.split('\n')

            current_category = None
            items = []

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»åˆ«æ ‡é¢˜
                line_lower = line.lower()
                if any(keyword in line_lower for keyword in ['tools:', 'parts:', 'materials:', 'what you need:']):
                    if current_category and items:
                        what_you_need[current_category] = items

                    if 'tool' in line_lower:
                        current_category = 'Tools'
                    elif 'part' in line_lower:
                        current_category = 'Parts'
                    elif 'material' in line_lower:
                        current_category = 'Materials'
                    else:
                        current_category = 'Items'

                    items = []

                # æ£€æŸ¥æ˜¯å¦æ˜¯é¡¹ç›®
                elif current_category and len(line) > 2 and len(line) < 100:
                    cleaned_item = self.clean_product_name(line)
                    if cleaned_item:
                        items.append(cleaned_item)

            # æ·»åŠ æœ€åä¸€ä¸ªç±»åˆ«
            if current_category and items:
                what_you_need[current_category] = items

            return what_you_need

        except Exception:
            return what_you_need

    def _extract_troubleshooting_images_from_section(self, section):
        """ä»troubleshooting sectionä¸­æå–å›¾ç‰‡ï¼Œä½¿ç”¨enhanced_crawlerçš„è¿‡æ»¤é€»è¾‘å¹¶ä¸‹è½½åˆ°æœ¬åœ°"""
        images = []
        seen_urls = set()

        try:
            if not section:
                if self.verbose:
                    print("âŒ Sectionä¸ºç©º")
                return images

            # æŸ¥æ‰¾è¯¥sectionå†…çš„æ‰€æœ‰å›¾ç‰‡
            img_elements = section.find_all('img')
            if self.verbose:
                print(f"ğŸ“· æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")

            for img in img_elements:
                try:
                    # æ£€æŸ¥æ˜¯å¦åœ¨å•†ä¸šæ¨å¹¿åŒºåŸŸå†…
                    if self._is_element_in_promotional_area(img):
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ¨å¹¿åŒºåŸŸå›¾ç‰‡")
                        continue

                    # é¢å¤–æ£€æŸ¥ï¼šæ£€æŸ¥å›¾ç‰‡çš„altæ–‡æœ¬æ˜¯å¦åŒ…å«æ¨èå†…å®¹ç‰¹å¾
                    alt_text = img.get('alt', '').strip().lower()
                    if self._is_recommendation_image_alt(alt_text):
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ¨èå†…å®¹å›¾ç‰‡: {alt_text}")
                        continue

                    img_src = img.get('src') or img.get('data-src')
                    if img_src and isinstance(img_src, str):
                        # æ„å»ºå®Œæ•´URL
                        if img_src.startswith("//"):
                            img_src = "https:" + img_src
                        elif img_src.startswith("/"):
                            img_src = self.base_url + img_src

                        # ä½¿ç”¨enhanced_crawlerçš„è¿‡æ»¤é€»è¾‘
                        if self._is_valid_troubleshooting_image_enhanced(img_src, img):
                            # ç¡®ä¿ä½¿ç”¨mediumå°ºå¯¸
                            if 'guide-images.cdn.ifixit.com' in img_src and not img_src.endswith('.medium'):
                                # ç§»é™¤ç°æœ‰çš„å°ºå¯¸åç¼€å¹¶æ·»åŠ .medium
                                if '.standard' in img_src or '.large' in img_src or '.small' in img_src:
                                    img_src = img_src.replace('.standard', '.medium').replace('.large', '.medium').replace('.small', '.medium')
                                elif not any(suffix in img_src for suffix in ['.medium', '.jpg', '.png', '.gif']):
                                    img_src += '.medium'

                            # å»é‡å¹¶æ·»åŠ 
                            if img_src not in seen_urls:
                                seen_urls.add(img_src)

                                # è·å–å›¾ç‰‡æè¿°ï¼ˆä½¿ç”¨åŸå§‹altæ–‡æœ¬ï¼Œä¸æ˜¯å°å†™ç‰ˆæœ¬ï¼‰
                                alt_text = img.get('alt', '').strip()
                                title_text = img.get('title', '').strip()
                                description = alt_text or title_text or "Block Image"

                                # æ£€æŸ¥æè¿°æ˜¯å¦æ˜¯å•†ä¸šå†…å®¹
                                if not self._is_commercial_text(description):
                                    # åœ¨troubleshootingæå–é˜¶æ®µï¼Œå…ˆä¿å­˜åŸå§‹URL
                                    # å›¾ç‰‡ä¸‹è½½å°†åœ¨ä¿å­˜é˜¶æ®µè¿›è¡Œï¼Œç¡®ä¿ä¸‹è½½åˆ°æ­£ç¡®çš„ç›®å½•
                                    images.append({
                                        "url": img_src,  # ä¿å­˜åŸå§‹URLï¼Œç¨ååœ¨ä¿å­˜æ—¶ä¸‹è½½
                                        "description": description
                                    })

                                    if self.verbose:
                                        print(f"âœ… æ·»åŠ å›¾ç‰‡: {description}")
                                else:
                                    if self.verbose:
                                        print(f"â­ï¸ è·³è¿‡å•†ä¸šæè¿°: {description}")
                            else:
                                if self.verbose:
                                    print(f"â­ï¸ è·³è¿‡é‡å¤URL: {img_src}")
                        else:
                            if self.verbose:
                                print(f"â­ï¸ è·³è¿‡æ— æ•ˆURL: {img_src}")
                    else:
                        if self.verbose:
                            print(f"â­ï¸ è·³è¿‡æ— æ•ˆsrc")
                except Exception as e:
                    if self.verbose:
                        print(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue

            if self.verbose:
                print(f"ğŸ“· æœ€ç»ˆæå–åˆ° {len(images)} ä¸ªå›¾ç‰‡")
            return images

        except Exception as e:
            if self.verbose:
                print(f"âŒ æå–å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return images

    def _remove_promotional_content_from_section(self, section):
        """ç§»é™¤sectionä¸­çš„æ¨å¹¿å†…å®¹"""
        try:
            # ç§»é™¤æ¨å¹¿ç›¸å…³çš„å…ƒç´ 
            promotional_selectors = [
                '.promo', '.promotion', '.ad', '.advertisement',
                '.product-card', '.shop-card', '.buy-now',
                '[class*="promo"]', '[class*="shop"]', '[class*="buy"]'
            ]

            for selector in promotional_selectors:
                elements = section.select(selector)
                for elem in elements:
                    elem.decompose()

        except Exception:
            pass

    def _is_valid_troubleshooting_image_enhanced(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„troubleshootingå›¾ç‰‡ - ä½¿ç”¨enhanced_crawlerçš„é€»è¾‘"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # è¿‡æ»¤æ‰å°å›¾æ ‡ã€logoå’Œå•†ä¸šå›¾ç‰‡
            skip_keywords = [
                'icon', 'logo', 'flag', 'avatar', 'ad', 'banner',
                'promo', 'cart', 'buy', 'purchase', 'shop', 'header',
                'footer', 'nav', 'menu', 'sidebar'
            ]
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            # å¦‚æœæœ‰img_elemï¼Œæ£€æŸ¥altå±æ€§
            if img_elem:
                alt_text = img_elem.get('alt', '').lower()
                title_text = img_elem.get('title', '').lower()

                # æ’é™¤å•†ä¸šæè¿°
                commercial_keywords = [
                    'buy', 'purchase', 'cart', 'shop', 'price', 'review',
                    'advertisement', 'sponsor'
                ]
                if any(keyword in alt_text or keyword in title_text for keyword in commercial_keywords):
                    return False

                # è¿‡æ»¤è£…é¥°æ€§å›¾ç‰‡
                decorative_alt_keywords = [
                    'decoration', 'ornament', 'divider', 'spacer',
                    'bullet', 'arrow', 'pointer', 'separator'
                ]
                if any(keyword in alt_text for keyword in decorative_alt_keywords):
                    return False

            return True

        except Exception:
            return False



    def _is_valid_troubleshooting_image(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„troubleshootingå›¾ç‰‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # åªå…è®¸æ¥è‡ªguide-images.cdn.ifixit.comçš„å›¾ç‰‡
            if 'guide-images.cdn.ifixit.com' not in img_src_lower:
                return False

            # è¿‡æ»¤æ‰æ˜æ˜¾çš„è£…é¥°æ€§å›¾ç‰‡å’Œå›¾æ ‡
            skip_keywords = [
                'icon', 'logo', 'avatar', 'badge', 'star', 'rating',
                'promo', 'ad', 'banner', 'shop', 'buy', 'cart'
            ]
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            # å¦‚æœæœ‰img_elemï¼Œæ£€æŸ¥altå±æ€§
            if img_elem:
                alt_text = img_elem.get('alt', '').lower()
                if alt_text:
                    decorative_alt_keywords = [
                        'decoration', 'ornament', 'divider', 'spacer',
                        'bullet', 'arrow', 'pointer', 'separator'
                    ]
                    if any(keyword in alt_text for keyword in decorative_alt_keywords):
                        return False

            return True

        except Exception:
            return False

        except Exception:
            return False

    def _is_element_in_promotional_area(self, element):
        """æ£€æŸ¥å…ƒç´ æ˜¯å¦åœ¨æ¨å¹¿åŒºåŸŸå†… - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´ç²¾ç¡®åœ°è¯†åˆ«æ¨å¹¿å®¹å™¨"""
        try:
            # å‘ä¸Šéå†DOMæ ‘æ£€æŸ¥æ˜¯å¦åœ¨æ¨å¹¿å®¹å™¨ä¸­
            current = element
            level = 0
            while current and current.name and level < 5:  # é™åˆ¶éå†å±‚æ•°
                # ä¼˜å…ˆé€šè¿‡classåç§°è¯†åˆ«æ¨å¹¿å®¹å™¨
                classes = current.get('class', [])
                if isinstance(classes, list):
                    class_str = ' '.join(classes).lower()
                else:
                    class_str = str(classes).lower()

                # æ˜ç¡®çš„æ¨å¹¿å®¹å™¨classåç§°
                promo_class_keywords = [
                    'guide-recommendation', 'guide-promo', 'view-guide',
                    'product-box', 'product-purchase', 'buy-box', 'purchase-box',
                    'product-card', 'shop-card', 'buy-now',
                    'parts-finder', 'find-parts'
                ]

                # å¦‚æœæœ‰æ˜ç¡®çš„æ¨å¹¿classï¼Œç›´æ¥è¿”å›True
                if any(keyword in class_str for keyword in promo_class_keywords):
                    return True

                # å¯¹äºæ²¡æœ‰æ˜ç¡®classçš„å…ƒç´ ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ–‡æœ¬æ£€æµ‹
                # åªæœ‰å½“å…ƒç´ ç›¸å¯¹è¾ƒå°ä¸”æ˜ç¡®åŒ…å«æ¨å¹¿å†…å®¹æ—¶æ‰è®¤ä¸ºæ˜¯æ¨å¹¿å®¹å™¨
                if (level <= 2 and  # åªæ£€æŸ¥å‰3å±‚
                    self._is_specific_promotional_container(current)):
                    return True

                current = current.parent
                level += 1
            return False
        except Exception:
            return False

    def _is_specific_promotional_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šçš„æ¨å¹¿å®¹å™¨ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æµ‹ï¼‰"""
        try:
            text = element.get_text().lower()

            # æ–‡æœ¬å¤ªé•¿çš„ä¸å¤ªå¯èƒ½æ˜¯æ¨å¹¿å®¹å™¨
            if len(text) > 300:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"View Guide"æŒ‰é’®
            has_view_guide = 'view guide' in text

            # æ£€æŸ¥æ—¶é—´æ ¼å¼æ¨¡å¼
            import re
            time_pattern = r'\d+\s*(minute|hour)s?(\s*-\s*\d+\s*(minute|hour)s?)?'
            has_time_pattern = bool(re.search(time_pattern, text))

            # æ£€æŸ¥éš¾åº¦ç­‰çº§
            difficulty_levels = ['very easy', 'easy', 'moderate', 'difficult', 'very difficult']
            has_difficulty = any(level in text for level in difficulty_levels)

            # æ£€æŸ¥æŒ‡å—ç›¸å…³ç‰¹å¾
            guide_features = ['how to', 'guide', 'tutorial', 'install', 'replace', 'repair', 'fix']
            has_guide_features = any(feature in text for feature in guide_features)

            # æ£€æŸ¥è´­ä¹°ç›¸å…³ç‰¹å¾
            purchase_features = ['buy', 'purchase', '$', 'â‚¬', 'Â£', 'Â¥', 'add to cart', 'shop']
            has_purchase = any(feature in text for feature in purchase_features)

            # æ¨å¹¿å®¹å™¨çš„åˆ¤æ–­æ¡ä»¶ï¼š
            # 1. åŒ…å«è´­ä¹°ç›¸å…³å†…å®¹ OR
            # 2. åŒ…å«"View Guide"æŒ‰é’® OR
            # 3. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’Œéš¾åº¦ç­‰çº§ OR
            # 4. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’ŒæŒ‡å—ç‰¹å¾
            return (
                has_purchase or
                has_view_guide or
                (has_time_pattern and has_difficulty) or
                (has_time_pattern and has_guide_features)
            )
        except Exception:
            return False

    def _is_recommendation_image_alt(self, alt_text):
        """æ£€æŸ¥å›¾ç‰‡çš„altæ–‡æœ¬æ˜¯å¦åŒ…å«æ¨èå†…å®¹ç‰¹å¾"""
        try:
            if not alt_text:
                return False

            alt_text_lower = alt_text.lower()

            # ç²¾ç¡®çš„æ¨èæŒ‡å—æ ‡é¢˜æ¨¡å¼ - æ›´å…¨é¢çš„æ¨¡å¼åŒ¹é…
            guide_title_patterns = [
                # ç‰¹å®šæ¨¡å¼
                r'how to boot.*into safe mode',
                r'how to use internet recovery',
                r'how to recover data from',
                r'how to install.*to.*ssd',
                r'how to replace.*battery',
                r'how to repair.*screen',
                r'how to fix.*display',
                r'how to troubleshoot',
                r'how to run.*with disk utility',
                r'how to start up.*in.*recovery mode',
                r'how to create a bootable',
                # é€šç”¨æ¨¡å¼
                r'how to .*removal',
                r'how to .*replace',
                r'how to .*install',
                r'how to .*repair',
                r'how to .*upgrade',
                r'how to .*fix',
                r'replacement.*guide',
                r'repair.*guide',
                r'installation.*guide',
                r'removal.*guide',
                # ç‰¹å®šè®¾å¤‡æ¨¡å¼
                r'macbook pro.*unibody.*removal',
                r'macbook pro.*key.*removal',
                r'macbook air.*display',
                r'macbook.*retina.*replacement'
            ]

            # æ£€æŸ¥ç²¾ç¡®çš„æ¨èæŒ‡å—æ ‡é¢˜æ¨¡å¼
            import re
            for pattern in guide_title_patterns:
                if re.search(pattern, alt_text_lower):
                    return True

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å—æ¨èçš„å…³é”®ç‰¹å¾
            recommendation_keywords = [
                'how to use', 'how to install', 'how to replace', 'how to repair',
                'guide', 'tutorial', 'installation', 'replacement', 'repair guide'
            ]

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨èç›¸å…³çš„è¯æ±‡
            has_recommendation_keywords = any(keyword in alt_text_lower for keyword in recommendation_keywords)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è®¾å¤‡/äº§å“åç§°ï¼ˆé€šå¸¸æ¨èå†…å®¹ä¼šåŒ…å«å…·ä½“çš„è®¾å¤‡åï¼‰
            device_keywords = ['macbook', 'iphone', 'ipad', 'ssd', 'battery', 'screen', 'display']
            has_device_keywords = any(keyword in alt_text_lower for keyword in device_keywords)

            # å¦‚æœåŒæ—¶åŒ…å«æ¨èå…³é”®è¯å’Œè®¾å¤‡å…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯æ¨èå†…å®¹
            return has_recommendation_keywords and has_device_keywords

        except Exception:
            return False

    def _is_guide_recommendation_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡å—æ¨èå®¹å™¨ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´ç²¾ç¡®åœ°è¯†åˆ«æ¨èæ¡†"""
        try:
            # åªæ£€æŸ¥å½“å‰å…ƒç´ çš„ç›´æ¥æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…æ‹¬æ·±å±‚å­å…ƒç´ 
            # è¿™æ ·å¯ä»¥é¿å…æ•´ä¸ªæ–‡æ¡£è¢«è¯¯åˆ¤ä¸ºæ¨å¹¿å®¹å™¨
            if element.name in ['html', '[document]', 'body']:
                return False

            # æ£€æŸ¥classå±æ€§ï¼Œä¼˜å…ˆé€šè¿‡classè¯†åˆ«
            classes = element.get('class', [])
            if isinstance(classes, list):
                class_str = ' '.join(classes).lower()
            else:
                class_str = str(classes).lower()

            # é€šè¿‡classåç§°å¿«é€Ÿè¯†åˆ«
            guide_class_keywords = [
                'guide-recommendation', 'guide-promo', 'view-guide',
                'recommendation', 'promo-guide', 'guide-card', 'related-guide'
            ]
            if any(keyword in class_str for keyword in guide_class_keywords):
                return True

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯æ¨èæ¡†ï¼ˆé™¤éæœ‰æ˜ç¡®çš„classæ ‡è¯†ï¼‰
            if len(text) > 500:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«"View Guide"æŒ‰é’®æ–‡æœ¬
            has_view_guide = 'view guide' in text

            # æ£€æŸ¥æ—¶é—´æ ¼å¼æ¨¡å¼ (å¦‚ "30 minutes - 1 hour", "5 minutes", "2 hours")
            import re
            time_pattern = r'\d+\s*(minute|hour)s?(\s*-\s*\d+\s*(minute|hour)s?)?'
            has_time_pattern = bool(re.search(time_pattern, text))

            # æ£€æŸ¥éš¾åº¦ç­‰çº§
            difficulty_levels = ['very easy', 'easy', 'moderate', 'difficult', 'very difficult']
            has_difficulty = any(level in text for level in difficulty_levels)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å—ç›¸å…³çš„å…³é”®è¯
            guide_keywords = ['how to', 'guide', 'tutorial', 'install', 'replace', 'repair', 'fix']
            has_guide_keywords = any(keyword in text for keyword in guide_keywords)

            # æ¨èæ¡†çš„ç‰¹å¾ï¼š
            # 1. åŒ…å«"View Guide"æŒ‰é’® OR
            # 2. åŒæ—¶åŒ…å«æ—¶é—´ä¿¡æ¯å’Œéš¾åº¦ç­‰çº§ OR
            # 3. åŒæ—¶åŒ…å«æ—¶é—´æ¨¡å¼å’ŒæŒ‡å—å…³é”®è¯
            is_recommendation = (
                has_view_guide or
                (has_time_pattern and has_difficulty) or
                (has_time_pattern and has_guide_keywords)
            )

            # æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼šæ¨èæ¡†é€šå¸¸æ¯”è¾ƒç®€æ´
            return is_recommendation and len(text) < 400

        except Exception:
            return False

    def _is_product_purchase_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“è´­ä¹°å®¹å™¨ - ä»enhanced_crawlerç§»æ¤ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·³è¿‡æ–‡æ¡£çº§åˆ«çš„å…ƒç´ 
            if element.name in ['html', '[document]', 'body']:
                return False

            # æ£€æŸ¥classå±æ€§ï¼Œä¼˜å…ˆé€šè¿‡classè¯†åˆ«
            classes = element.get('class', [])
            if isinstance(classes, list):
                class_str = ' '.join(classes).lower()
            else:
                class_str = str(classes).lower()

            # é€šè¿‡classåç§°å¿«é€Ÿè¯†åˆ«
            product_class_keywords = [
                'product-box', 'product-purchase', 'buy-box', 'purchase-box',
                'product-card', 'shop-card', 'buy-now'
            ]
            if any(keyword in class_str for keyword in product_class_keywords):
                return True

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯è´­ä¹°æ¡†
            if len(text) > 400:
                return False

            import re
            has_price = bool(re.search(r'[\$â‚¬Â£Â¥]\s*\d+\.?\d*', text))
            has_buy = 'buy' in text
            has_reviews = bool(re.search(r'\d+\.?\d*\s*(reviews?|stars?)', text))
            # è´­ä¹°æ¡†é€šå¸¸åŒ…å«ä»·æ ¼ã€è´­ä¹°æŒ‰é’®æˆ–è¯„ä»·ä¿¡æ¯
            return (has_price or has_buy or has_reviews) and len(text) < 200
        except Exception:
            return False

    def _is_parts_finder_container(self, element):
        """æ£€æŸ¥æ˜¯å¦æ˜¯é…ä»¶æŸ¥æ‰¾å®¹å™¨ - ä»enhanced_crawlerç§»æ¤ï¼Œæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·³è¿‡æ–‡æ¡£çº§åˆ«çš„å…ƒç´ 
            if element.name in ['html', '[document]', 'body']:
                return False

            text = element.get_text().lower()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯é…ä»¶æŸ¥æ‰¾æ¡†
            if len(text) > 500:
                return False

            has_parts_text = any(keyword in text for keyword in [
                'find your parts', 'select my model', 'compatible replacement'
            ])
            return has_parts_text and len(text) < 300
        except Exception:
            return False

    def _is_commercial_text(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯å•†ä¸šå†…å®¹ - ä»enhanced_crawlerç§»æ¤"""
        try:
            text_lower = text.lower()
            # å•†ä¸šå†…å®¹å…³é”®è¯
            commercial_keywords = [
                'buy', 'purchase', 'view guide', 'find your parts', 'select my model',
                'add to cart', 'quality guarantee', 'compatible replacement'
            ]

            # æ£€æŸ¥ä»·æ ¼æ¨¡å¼
            import re
            if re.search(r'[\$â‚¬Â£Â¥]\s*\d+\.?\d*', text):
                return True

            # æ£€æŸ¥è¯„åˆ†æ¨¡å¼
            if re.search(r'\d+\.?\d*\s*(reviews?|stars?)', text_lower):
                return True

            # æ£€æŸ¥å•†ä¸šå…³é”®è¯
            keyword_count = sum(1 for keyword in commercial_keywords if keyword in text_lower)
            return keyword_count >= 1

        except Exception:
            return False

    def _is_image_in_commercial_area(self, img_elem):
        """æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åœ¨å•†ä¸šåŒºåŸŸå†…"""
        try:
            # å‘ä¸ŠæŸ¥æ‰¾çˆ¶å…ƒç´ ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å•†ä¸šç›¸å…³çš„å®¹å™¨ä¸­
            current = img_elem
            for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
                if not current or not hasattr(current, 'parent'):
                    break

                current = current.parent
                if not current:
                    break

                # æ£€æŸ¥classå±æ€§
                class_attr = current.get('class', [])
                if isinstance(class_attr, list):
                    class_str = ' '.join(class_attr).lower()
                else:
                    class_str = str(class_attr).lower()

                # å•†ä¸šåŒºåŸŸçš„classå…³é”®è¯
                commercial_keywords = [
                    'shop', 'store', 'buy', 'purchase', 'cart', 'checkout',
                    'product', 'price', 'sale', 'offer', 'deal', 'promo',
                    'advertisement', 'ad-', 'banner', 'sponsor'
                ]

                if any(keyword in class_str for keyword in commercial_keywords):
                    return True

                # æ£€æŸ¥idå±æ€§
                id_attr = current.get('id', '')
                if id_attr and any(keyword in id_attr.lower() for keyword in commercial_keywords):
                    return True

            return False
        except Exception:
            return False

    def _is_valid_guide_image(self, img_src, img_elem=None):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„guideå›¾ç‰‡ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡"""
        try:
            if not img_src:
                return False

            img_src_lower = img_src.lower()

            # åªå…è®¸æ¥è‡ªguide-images.cdn.ifixit.comçš„å›¾ç‰‡
            if 'guide-images.cdn.ifixit.com' not in img_src_lower:
                return False

            # è¿‡æ»¤æ‰æ˜æ˜¾çš„è£…é¥°æ€§å›¾ç‰‡
            skip_keywords = ['icon', 'logo', 'avatar', 'badge', 'star', 'rating']
            if any(keyword in img_src_lower for keyword in skip_keywords):
                return False

            return True

        except Exception:
            return False

    def extract_guide_content(self, guide_url):
        """é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨ç®€åŒ–çš„å›¾ç‰‡è¿‡æ»¤é€»è¾‘"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºæœ¬å†…å®¹
        guide_data = super().extract_guide_content(guide_url)

        if not guide_data:
            return None

        # ç®€åŒ–å›¾ç‰‡å¤„ç†ï¼šåªç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„URLæ ¼å¼ï¼Œä¸é‡æ–°æå–
        if 'steps' in guide_data and guide_data['steps']:
            for step in guide_data['steps']:
                if 'images' in step and step['images']:
                    filtered_images = []
                    seen_urls = set()

                    for img_src in step['images']:
                        if isinstance(img_src, str) and img_src:
                            # ç¡®ä¿ä½¿ç”¨å®Œæ•´URL
                            if img_src.startswith("//"):
                                img_src = "https:" + img_src
                            elif img_src.startswith("/"):
                                img_src = self.base_url + img_src

                            # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™guide-images.cdn.ifixit.comçš„å›¾ç‰‡
                            if self._is_valid_guide_image(img_src):
                                # ç¡®ä¿ä½¿ç”¨mediumå°ºå¯¸
                                if 'guide-images.cdn.ifixit.com' in img_src and not img_src.endswith('.medium'):
                                    # ç§»é™¤ç°æœ‰çš„å°ºå¯¸åç¼€å¹¶æ·»åŠ .medium
                                    if '.standard' in img_src or '.large' in img_src or '.small' in img_src:
                                        img_src = img_src.replace('.standard', '.medium').replace('.large', '.medium').replace('.small', '.medium')
                                    elif not any(suffix in img_src for suffix in ['.medium', '.jpg', '.png', '.gif']):
                                        img_src += '.medium'

                                # å»é‡
                                if img_src not in seen_urls:
                                    seen_urls.add(img_src)
                                    filtered_images.append(img_src)

                    step['images'] = filtered_images

        return guide_data

    def clean_product_name(self, text):
        """æ¸…ç†äº§å“åç§°ï¼Œç§»é™¤ä»·æ ¼ã€è¯„åˆ†ç­‰æ— å…³ä¿¡æ¯"""
        if not text:
            return ""

        # ç§»é™¤ä»·æ ¼ä¿¡æ¯
        text = re.sub(r'\$[\d.,]+.*$', '', text).strip()
        # ç§»é™¤è¯„åˆ†ä¿¡æ¯
        text = re.sub(r'[\d.]+\s*out of.*$', '', text).strip()
        text = re.sub(r'Sale price.*$', '', text).strip()
        text = re.sub(r'Rated [\d.]+.*$', '', text).strip()
        # ç§»é™¤å¸¸è§çš„æ— å…³è¯æ±‡
        unwanted_words = ['View', 'view', 'æŸ¥çœ‹', 'Add to cart', 'æ·»åŠ åˆ°è´­ç‰©è½¦', 'Buy', 'Sale', 'Available']
        for word in unwanted_words:
            if text.strip() == word:
                return ""

        # ç§»é™¤æè¿°æ€§æ–‡æœ¬
        text = re.sub(r'This kit contains.*$', '', text).strip()
        text = re.sub(r'Available for sale.*$', '', text).strip()

        return text.strip()

    def crawl_combined_tree(self, start_url, category_name=None):
        """
        æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"ğŸš€ å¼€å§‹æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # è®¾ç½®ç›®æ ‡URL
        self.target_url = start_url

        # æ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥
        if self.cache_manager and self.use_cache and not self.force_refresh:
            print("ğŸ” æ‰§è¡Œæ™ºèƒ½ç¼“å­˜é¢„æ£€æŸ¥...")
            self._perform_cache_precheck(start_url, category_name)
            self.cache_manager.display_cache_report()

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
        print("ğŸ“Š é˜¶æ®µ 1/3: æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
        print("   æ­£åœ¨åˆ†æé¡µé¢ç»“æ„å’Œåˆ†ç±»å±‚æ¬¡...")
        base_tree = self.tree_crawler.crawl_tree(start_url, category_name)

        if not base_tree:
            print("âŒ æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
            return None

        print(f"âœ… åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆ")
        print("=" * 60)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        print("ğŸ“ é˜¶æ®µ 2/3: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
        print("   æ­£åœ¨å¤„ç†èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯å’Œå…ƒæ•°æ®...")
        enriched_tree = self.enrich_tree_with_detailed_content(base_tree)

        # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
        print("ğŸ” é˜¶æ®µ 3/3: æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
        print("   æ­£åœ¨æå–æŒ‡å—å’Œæ•…éšœæ’é™¤è¯¦ç»†ä¿¡æ¯...")
        final_tree = self.deep_crawl_product_content(enriched_tree)

        return final_tree

    def _perform_cache_precheck(self, start_url, category_name=None):
        """æ‰§è¡Œç¼“å­˜é¢„æ£€æŸ¥ï¼Œåˆ†æå“ªäº›å†…å®¹éœ€è¦é‡æ–°å¤„ç†"""
        try:
            # æ¸…ç†æ— æ•ˆç¼“å­˜
            cleaned_count = self.cache_manager.clean_invalid_cache()
            if cleaned_count > 0:
                print(f"   å·²æ¸…ç† {cleaned_count} ä¸ªæ— æ•ˆç¼“å­˜æ¡ç›®")

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„é¢„æ£€æŸ¥é€»è¾‘
            # æ¯”å¦‚æ£€æŸ¥ç›®æ ‡URLçš„æ•´ä½“ç¼“å­˜çŠ¶æ€
            print("   ç¼“å­˜é¢„æ£€æŸ¥å®Œæˆ")

        except Exception as e:
            self.logger.error(f"ç¼“å­˜é¢„æ£€æŸ¥å¤±è´¥: {e}")

    def _get_node_cache_path(self, url, node_name):
        """è·å–èŠ‚ç‚¹çš„ç¼“å­˜è·¯å¾„ï¼Œç¡®ä¿ä¸ä¿å­˜æ—¶çš„è·¯å¾„ä¸€è‡´"""
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜ç´¢å¼•ä¸­æ˜¯å¦æœ‰è¿™ä¸ªURLçš„è®°å½•
        if self.cache_manager:
            url_hash = self.cache_manager.get_url_hash(url)
            if url_hash in self.cache_manager.cache_index:
                cache_entry = self.cache_manager.cache_index[url_hash]
                cached_path = cache_entry.get('local_path', '')
                if cached_path:
                    return Path(self.storage_root) / cached_path

        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰è®°å½•ï¼Œä½¿ç”¨æ ‡å‡†è·¯å¾„æ„å»ºé€»è¾‘
        if '/Device/' in url:
            # ä½¿ç”¨ä¸_get_target_root_dirç›¸åŒçš„é€»è¾‘
            device_path = url.split('/Device/')[-1]
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            device_path = device_path.rstrip('/')

            if device_path:
                import urllib.parse
                device_path = urllib.parse.unquote(device_path)
                return Path(self.storage_root) / "Device" / device_path
            else:
                return Path(self.storage_root) / "Device"
        else:
            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨èŠ‚ç‚¹åç§°
            safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")
            return Path(self.storage_root) / safe_name

    def _load_cached_node_data(self, local_path):
        """ä»ç¼“å­˜åŠ è½½èŠ‚ç‚¹æ•°æ®"""
        try:
            # åŠ è½½åŸºæœ¬ä¿¡æ¯
            info_file = local_path / "info.json"
            if not info_file.exists():
                return None

            with open(info_file, 'r', encoding='utf-8') as f:
                node_data = json.load(f)

            # åŠ è½½guidesæ•°æ®
            guides_dir = local_path / "guides"
            if guides_dir.exists():
                guides = []
                for guide_file in sorted(guides_dir.glob("guide_*.json")):
                    with open(guide_file, 'r', encoding='utf-8') as f:
                        guides.append(json.load(f))
                if guides:
                    node_data['guides'] = guides

            # åŠ è½½troubleshootingæ•°æ®
            ts_dir = local_path / "troubleshooting"
            if ts_dir.exists():
                troubleshooting = []
                for ts_file in sorted(ts_dir.glob("troubleshooting_*.json")):
                    with open(ts_file, 'r', encoding='utf-8') as f:
                        troubleshooting.append(json.load(f))
                if troubleshooting:
                    node_data['troubleshooting'] = troubleshooting

            return node_data

        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None

    def enrich_tree_with_detailed_content(self, node):
        """ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ­£ç¡®çš„å­—æ®µ"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            return node

        # æ£€æŸ¥ç¼“å­˜ - ä½¿ç”¨ä¸ä¿å­˜æ—¶ä¸€è‡´çš„è·¯å¾„æ„å»ºé€»è¾‘
        local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

        if self.verbose:
            print(f"ğŸ” æ£€æŸ¥ç¼“å­˜: {url}")
            print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

        if self._check_cache_validity(url, local_path):
            self.stats["cache_hits"] += 1
            if self.verbose:
                print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡å¤„ç†: {node.get('name', '')}")
            else:
                print(f"   âœ… è·³è¿‡å·²ç¼“å­˜: {node.get('name', '')}")
            return node

        self.processed_nodes.add(url)
        self.stats["cache_misses"] += 1

        time.sleep(random.uniform(0.5, 1.0))
        soup = self.get_soup(url)

        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½ç»è¿‡fix_node_dataå¤„ç†
        node = self.fix_node_data(node, soup)

        if self.verbose:
            self.logger.info(f"å¤„ç†èŠ‚ç‚¹: {node.get('name', '')} - {url}")
        else:
            print(f"   ğŸ“„ å¤„ç†èŠ‚ç‚¹: {node.get('name', '')}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            children_count = len(node['children'])
            if children_count > 0:
                print(f"   ğŸŒ³ å¤„ç† {children_count} ä¸ªå­èŠ‚ç‚¹...")
            enriched_children = []
            for i, child in enumerate(node['children'], 1):
                if children_count > 1:
                    print(f"      â””â”€ [{i}/{children_count}] {child.get('name', 'Unknown')}")
                enriched_child = self.enrich_tree_with_detailed_content(child)
                if enriched_child:
                    enriched_children.append(enriched_child)
            node['children'] = enriched_children

        return node

    def _process_content_concurrently(self, guide_links, device_url, skip_troubleshooting, soup):
        """å¹¶å‘å¤„ç†guideså’Œtroubleshootingå†…å®¹"""
        guides_data = []
        troubleshooting_data = []

        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []

        # æ·»åŠ guideä»»åŠ¡
        for guide_link in guide_links:
            tasks.append(('guide', guide_link))

        # æ·»åŠ troubleshootingä»»åŠ¡
        if not skip_troubleshooting:
            print(f"    ğŸ”§ æ­£åœ¨æœç´¢æ•…éšœæ’é™¤é¡µé¢...")
            troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, device_url)
            print(f"    ğŸ”§ æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢")

            for ts_link in troubleshooting_links:
                if isinstance(ts_link, dict):
                    ts_url = ts_link.get('url', '')
                else:
                    ts_url = ts_link
                if ts_url:
                    tasks.append(('troubleshooting', ts_url))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        if self.max_workers > 1 and len(tasks) > 1:
            print(f"    ğŸ”„ å¯åŠ¨å¹¶å‘å¤„ç† ({self.max_workers} çº¿ç¨‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡)...")
            with ThreadPoolExecutor(max_workers=min(self.max_workers, len(tasks))) as executor:
                future_to_task = {}
                completed_count = 0
                total_tasks = len(tasks)

                for task_type, url in tasks:
                    if task_type == 'guide':
                        future = executor.submit(self._process_guide_task, url)
                    else:  # troubleshooting
                        future = executor.submit(self._process_troubleshooting_task, url)
                    future_to_task[future] = (task_type, url)

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task_type, url = future_to_task[future]
                    completed_count += 1
                    try:
                        result = future.result()
                        if result:
                            if task_type == 'guide':
                                guides_data.append(result)
                                print(f"    âœ… [{completed_count}/{total_tasks}] æŒ‡å—: {result.get('title', '')}")
                            else:
                                troubleshooting_data.append(result)
                                print(f"    âœ… [{completed_count}/{total_tasks}] æ•…éšœæ’é™¤: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{completed_count}/{total_tasks}] {task_type} å¤„ç†å¤±è´¥")
                    except Exception as e:
                        print(f"    âŒ [{completed_count}/{total_tasks}] {task_type} ä»»åŠ¡å¤±è´¥: {str(e)[:50]}...")
                        self.logger.error(f"å¹¶å‘ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")
        else:
            # å•çº¿ç¨‹å¤„ç†
            print(f"    ğŸ”„ é¡ºåºå¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
            for i, (task_type, url) in enumerate(tasks, 1):
                try:
                    print(f"    â³ [{i}/{len(tasks)}] æ­£åœ¨å¤„ç† {task_type}...")
                    if task_type == 'guide':
                        result = self._process_guide_task(url)
                        if result:
                            guides_data.append(result)
                            print(f"    âœ… [{i}/{len(tasks)}] æŒ‡å—: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{i}/{len(tasks)}] æŒ‡å—å¤„ç†å¤±è´¥")
                    else:
                        result = self._process_troubleshooting_task(url)
                        if result:
                            troubleshooting_data.append(result)
                            print(f"    âœ… [{i}/{len(tasks)}] æ•…éšœæ’é™¤: {result.get('title', '')}")
                        else:
                            print(f"    âš ï¸  [{i}/{len(tasks)}] æ•…éšœæ’é™¤å¤„ç†å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"ä»»åŠ¡å¤±è´¥ {task_type} {url}: {e}")

        return guides_data, troubleshooting_data

    def _process_guide_task(self, guide_url):
        """å¤„ç†å•ä¸ªguideä»»åŠ¡"""
        try:
            guide_content = self.extract_guide_content(guide_url)
            if guide_content:
                guide_content['url'] = guide_url

                # ç¡®ä¿"What You Need"éƒ¨åˆ†è¢«æ­£ç¡®æå–
                if 'what_you_need' not in guide_content or not guide_content['what_you_need']:
                    print(f"  é‡æ–°å°è¯•æå–What You Needéƒ¨åˆ†: {guide_url}")
                    what_you_need = self.extract_what_you_need_enhanced(guide_url)
                    if what_you_need:
                        guide_content['what_you_need'] = what_you_need
                        print(f"  æˆåŠŸæå–What You Need: {list(what_you_need.keys())}")

                return guide_content
        except Exception as e:
            self.logger.error(f"å¤„ç†guideå¤±è´¥ {guide_url}: {e}")
            self._log_failed_url(guide_url, f"Guideå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _process_troubleshooting_task(self, ts_url):
        """å¤„ç†å•ä¸ªtroubleshootingä»»åŠ¡"""
        try:
            ts_content = self.extract_troubleshooting_content(ts_url)
            if ts_content:
                ts_content['url'] = ts_url
                return ts_content
        except Exception as e:
            self.logger.error(f"å¤„ç†troubleshootingå¤±è´¥ {ts_url}: {e}")
            self._log_failed_url(ts_url, f"Troubleshootingå¤„ç†å¤±è´¥: {str(e)}")
        return None

    def _print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)

        total_requests = self.stats["total_requests"]
        cache_hits = self.stats["cache_hits"]
        cache_misses = self.stats["cache_misses"]

        if total_requests > 0:
            cache_hit_rate = (cache_hits / (cache_hits + cache_misses)) * 100 if (cache_hits + cache_misses) > 0 else 0
            print(f"ğŸŒ æ€»è¯·æ±‚æ•°: {total_requests}")
            print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_hits} ({cache_hit_rate:.1f}%)")
            print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­: {cache_misses}")

        retry_success = self.stats["retry_success"]
        retry_failed = self.stats["retry_failed"]
        total_retries = retry_success + retry_failed

        if total_retries > 0:
            retry_success_rate = (retry_success / total_retries) * 100
            print(f"ğŸ” é‡è¯•æˆåŠŸ: {retry_success}/{total_retries} ({retry_success_rate:.1f}%)")
            print(f"âŒ é‡è¯•å¤±è´¥: {retry_failed}")

        media_downloaded = self.stats["media_downloaded"]
        media_failed = self.stats["media_failed"]
        total_media = media_downloaded + media_failed

        if total_media > 0:
            media_success_rate = (media_downloaded / total_media) * 100
            print(f"ğŸ“ åª’ä½“ä¸‹è½½æˆåŠŸ: {media_downloaded}/{total_media} ({media_success_rate:.1f}%)")
            print(f"ğŸ“ åª’ä½“ä¸‹è½½å¤±è´¥: {media_failed}")

        # è§†é¢‘å¤„ç†ç»Ÿè®¡
        videos_downloaded = self.stats.get("videos_downloaded", 0)
        videos_skipped = self.stats.get("videos_skipped", 0)
        total_videos = videos_downloaded + videos_skipped

        if total_videos > 0:
            print(f"ğŸ¥ è§†é¢‘æ–‡ä»¶å¤„ç†:")
            print(f"   âœ… å·²ä¸‹è½½: {videos_downloaded}")
            print(f"   â­ï¸ å·²è·³è¿‡: {videos_skipped}")
            if not self.download_videos:
                print(f"   ğŸ’¡ æç¤º: è§†é¢‘ä¸‹è½½å·²ç¦ç”¨ï¼Œå¦‚éœ€ä¸‹è½½è¯·è®¾ç½® download_videos=True")
            else:
                print(f"   ğŸ“ å¤§å°é™åˆ¶: {self.max_video_size_mb}MB")

        if self.use_proxy and hasattr(self, 'proxy_switch_count'):
            print(f"ğŸ”„ ä»£ç†åˆ‡æ¢æ¬¡æ•°: {self.proxy_switch_count}")

        print("=" * 60)
        


    def count_detailed_nodes(self, node):
        """
        ç»Ÿè®¡æ ‘ä¸­åŒ…å«è¯¦ç»†å†…å®¹çš„èŠ‚ç‚¹æ•°é‡
        """
        count = {'guides': 0, 'troubleshooting': 0, 'products': 0, 'categories': 0}

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„guideså’Œtroubleshootingæ•°ç»„
        if 'guides' in node and isinstance(node['guides'], list):
            count['guides'] += len(node['guides'])

        if 'troubleshooting' in node and isinstance(node['troubleshooting'], list):
            count['troubleshooting'] += len(node['troubleshooting'])

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹
        if 'title' in node and 'steps' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹ï¼ˆä¸åœ¨guidesæ•°ç»„ä¸­ï¼‰
            count['guides'] += 1
        elif 'causes' in node:
            # è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹ï¼ˆä¸åœ¨troubleshootingæ•°ç»„ä¸­ï¼‰
            count['troubleshooting'] += 1
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            count['products'] += 1
        else:
            # åˆ†ç±»èŠ‚ç‚¹
            count['categories'] += 1

        # é€’å½’ç»Ÿè®¡å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for child in node['children']:
                child_count = self.count_detailed_nodes(child)
                for key in count:
                    count[key] += child_count[key]

        return count

    def extract_guides_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æŒ‡å—é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§é¡µé¢å¸ƒå±€
        """
        guides = []
        seen_guide_urls = set()

        if not soup:
            return guides

        # æŸ¥æ‰¾æ‰€æœ‰æŒ‡å—é“¾æ¥ï¼ˆåŒ…æ‹¬Guideå’ŒTeardownï¼‰
        guide_links = soup.select("a[href*='/Guide/'], a[href*='/Teardown/']")

        for link in guide_links:
            href = link.get("href")
            text = link.get_text().strip()

            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„é“¾æ¥
            skip_patterns = [
                "Create Guide", "åˆ›å»ºæŒ‡å—", "Guide/new", "Guide/edit", "/edit/",
                "Guide/history", "/history/", "Edit Guide", "ç¼–è¾‘æŒ‡å—",
                "Version History", "ç‰ˆæœ¬å†å²", "Teardown/edit", "#comment", "permalink=comment"
            ]

            if href and text and not any(pattern in href or pattern in text for pattern in skip_patterns):
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_guide_urls:
                    seen_guide_urls.add(href)
                    guides.append({
                        "title": text,
                        "url": href
                    })

        return guides

    def extract_troubleshooting_from_device_page(self, soup, device_url):
        """
        ä»è®¾å¤‡é¡µé¢æå–æ•…éšœæ’é™¤é“¾æ¥ï¼Œæ”¹è¿›ç‰ˆæœ¬
        """
        troubleshooting_links = []
        seen_ts_urls = set()

        if not soup:
            return troubleshooting_links

        # æŸ¥æ‰¾æ‰€æœ‰æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
        # 1. ç›´æ¥çš„Troubleshootingé“¾æ¥
        ts_links = soup.select("a[href*='/Troubleshooting/']")

        # 2. æŸ¥æ‰¾åŒ…å«æ•…éšœæ’é™¤æ–‡æœ¬çš„é“¾æ¥
        all_links = soup.select("a[href]")
        for link in all_links:
            text = link.get_text().strip().lower()
            href = link.get("href", "")

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•…éšœæ’é™¤ç›¸å…³çš„é“¾æ¥
            troubleshooting_keywords = [
                'troubleshooting', 'æ•…éšœæ’é™¤', 'æ•…éšœ', 'problems', 'issues',
                'won\'t turn on', 'not working', 'broken'
            ]

            if any(keyword in text for keyword in troubleshooting_keywords) or '/Troubleshooting/' in href:
                ts_links.append(link)

        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        for link in ts_links:
            href = link.get("href")
            text = link.get_text().strip()

            if href and text:
                # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬çš„URL
                if href.startswith("/"):
                    href = self.base_url + href
                if 'zh.ifixit.com' in href:
                    href = href.replace('zh.ifixit.com', 'www.ifixit.com')

                # æ·»åŠ lang=enå‚æ•°
                if '?' in href:
                    if 'lang=' not in href:
                        href += '&lang=en'
                else:
                    href += '?lang=en'

                if href not in seen_ts_urls and '/Troubleshooting/' in href:
                    seen_ts_urls.add(href)
                    troubleshooting_links.append({
                        "title": text,
                        "url": href
                    })

        return troubleshooting_links

    def extract_troubleshooting_content(self, troubleshooting_url):
        """
        æå–æ•…éšœæ’é™¤é¡µé¢çš„è¯¦ç»†å†…å®¹ - å®Œå…¨æŒ‰ç…§combined_crawler.pyçš„é€»è¾‘
        """
        if not troubleshooting_url:
            return None

        # ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
        if 'zh.ifixit.com' in troubleshooting_url:
            troubleshooting_url = troubleshooting_url.replace('zh.ifixit.com', 'www.ifixit.com')

        # æ·»åŠ lang=enå‚æ•°
        if '?' in troubleshooting_url:
            if 'lang=' not in troubleshooting_url:
                troubleshooting_url += '&lang=en'
        else:
            troubleshooting_url += '?lang=en'

        print(f"Crawling troubleshooting content: {troubleshooting_url}")

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(random.uniform(1, 2))

        soup = self.get_soup(troubleshooting_url)
        if not soup:
            return None

        troubleshooting_data = {
            "url": troubleshooting_url,
            "title": "",
            "causes": []
        }

        try:
            # æå–æ ‡é¢˜
            title_elem = soup.select_one("h1")
            if title_elem:
                title_text = title_elem.get_text().strip()
                title_text = re.sub(r'\s+', ' ', title_text)
                troubleshooting_data["title"] = title_text
                print(f"æå–æ ‡é¢˜: {title_text}")

            # åŠ¨æ€æå–é¡µé¢ä¸Šçš„çœŸå®å­—æ®µå†…å®¹ï¼ˆå¦‚first_stepsç­‰ï¼‰
            dynamic_sections = self.extract_dynamic_sections(soup)
            troubleshooting_data.update(dynamic_sections)

            # æå–causeså¹¶ä¸ºæ¯ä¸ªcauseæå–å¯¹åº”çš„å›¾ç‰‡å’Œè§†é¢‘
            troubleshooting_data["causes"] = self.extract_causes_sections_with_media(soup)

            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•…éšœæ’é™¤é¡µé¢
            if not self._is_valid_troubleshooting_page(troubleshooting_data):
                print(f"è·³è¿‡é€šç”¨æˆ–æ— æ•ˆçš„æ•…éšœæ’é™¤é¡µé¢: {troubleshooting_data.get('title', '')}")
                return None

            # æå–ç»Ÿè®¡æ•°æ®
            statistics = self.extract_page_statistics(soup)

            # é‡æ„ç»Ÿè®¡æ•°æ®ç»“æ„ - å°†view_statisticsç§»åŠ¨åˆ°titleä¸‹é¢
            # é‡æ–°æ„å»ºtroubleshooting_dataï¼Œç¡®ä¿å­—æ®µé¡ºåºæ­£ç¡®
            ordered_ts_data = {}
            ordered_ts_data["url"] = troubleshooting_data["url"]
            ordered_ts_data["title"] = troubleshooting_data["title"]

            # å°†ç»Ÿè®¡æ•°æ®ç´§è·Ÿåœ¨titleåé¢ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if statistics:
                # åˆ›å»ºview_statisticså¯¹è±¡ï¼ŒåªåŒ…å«æ—¶é—´ç›¸å…³çš„ç»Ÿè®¡
                view_stats = {}
                if 'past_24_hours' in statistics:
                    view_stats['past_24_hours'] = statistics['past_24_hours']
                if 'past_7_days' in statistics:
                    view_stats['past_7_days'] = statistics['past_7_days']
                if 'past_30_days' in statistics:
                    view_stats['past_30_days'] = statistics['past_30_days']
                if 'all_time' in statistics:
                    view_stats['all_time'] = statistics['all_time']

                if view_stats:
                    ordered_ts_data["view_statistics"] = view_stats
                if 'completed' in statistics:
                    ordered_ts_data["completed"] = statistics['completed']
                if 'favorites' in statistics:
                    ordered_ts_data["favorites"] = statistics['favorites']

            # æ·»åŠ åŠ¨æ€æå–çš„å­—æ®µï¼ˆæŒ‰é¡µé¢å®é™…å­—æ®µåç§°ï¼‰
            for key, value in troubleshooting_data.items():
                if key not in ["url", "title", "causes", "images", "videos"] and value:
                    ordered_ts_data[key] = value

            # æ·»åŠ å…¶ä»–å­—æ®µ
            if "causes" in troubleshooting_data and troubleshooting_data["causes"]:
                ordered_ts_data["causes"] = troubleshooting_data["causes"]

            troubleshooting_data = ordered_ts_data

            # æ‰“å°æå–ç»“æœç»Ÿè®¡
            dynamic_fields = [k for k in troubleshooting_data.keys()
                            if k not in ['url', 'title', 'causes', 'view_statistics', 'completed', 'favorites']]

            print(f"æå–å®Œæˆ:")
            for field in dynamic_fields:
                content = troubleshooting_data.get(field, '')
                if content:
                    print(f"  {field}: {len(content)} å­—ç¬¦")

            causes = troubleshooting_data.get('causes', [])
            print(f"  Causes: {len(causes)} ä¸ª")

            # ç»Ÿè®¡æ¯ä¸ªcauseä¸­çš„å›¾ç‰‡å’Œè§†é¢‘æ•°é‡
            total_images = sum(len(cause.get('images', [])) for cause in causes)
            total_videos = sum(len(cause.get('videos', [])) for cause in causes)
            print(f"  Images (in causes): {total_images} ä¸ª")
            print(f"  Videos (in causes): {total_videos} ä¸ª")

            if statistics:
                print(f"  Statistics: {len(statistics)} é¡¹")

            return troubleshooting_data

        except Exception as e:
            print(f"æå–æ•…éšœæ’é™¤å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _is_valid_troubleshooting_page(self, troubleshooting_data):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å…·ä½“æ•…éšœæ’é™¤é¡µé¢ï¼Œè€Œä¸æ˜¯é€šç”¨é¡µé¢"""
        if not troubleshooting_data:
            return False

        title = troubleshooting_data.get('title', '').lower()
        causes = troubleshooting_data.get('causes', [])

        # æ£€æŸ¥æ˜¯å¦ä¸ºé€šç”¨é¡µé¢çš„æ ‡é¢˜
        generic_keywords = [
            'common problems', 'fix common', 'general troubleshooting',
            'troubleshooting guide', 'basic troubleshooting', 'general issues',
            'common issues', 'general problems'
        ]

        for keyword in generic_keywords:
            if keyword in title:
                print(f"æ£€æµ‹åˆ°é€šç”¨é¡µé¢æ ‡é¢˜å…³é”®è¯: {keyword}")
                return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ•…éšœåŸå› 
        if not causes or len(causes) == 0:
            print("æ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„æ•…éšœåŸå› ")
            return False

        # æ£€æŸ¥causeså†…å®¹æ˜¯å¦è¶³å¤Ÿå…·ä½“
        if len(causes) < 2:  # è‡³å°‘è¦æœ‰2ä¸ªå…·ä½“çš„æ•…éšœåŸå› 
            print(f"æ•…éšœåŸå› å¤ªå°‘: {len(causes)} ä¸ª")
            return False

        # æ£€æŸ¥causesæ˜¯å¦æœ‰å®é™…å†…å®¹
        valid_causes = 0
        for cause in causes:
            content = cause.get('content', '')
            if content and len(content.strip()) > 50:  # å†…å®¹è¦è¶³å¤Ÿè¯¦ç»†
                valid_causes += 1

        if valid_causes < 2:
            print(f"æœ‰æ•ˆçš„æ•…éšœåŸå› å¤ªå°‘: {valid_causes} ä¸ª")
            return False

        print(f"éªŒè¯é€šè¿‡: {len(causes)} ä¸ªæ•…éšœåŸå› , {valid_causes} ä¸ªæœ‰æ•ˆ")
        return True

    def deep_crawl_product_content(self, node, skip_troubleshooting=False):
        """æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶æ­£ç¡®æ„å»ºæ•°æ®ç»“æ„"""
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        is_specified_target = self._is_specified_target_url(url)

        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (is_specified_target or not node.get('children') or len(node.get('children', [])) == 0)
        )

        if is_target_page:
            # æ£€æŸ¥æ·±åº¦å†…å®¹çš„ç¼“å­˜
            local_path = self._get_node_cache_path(url, node.get('name', 'unknown'))

            if self.verbose:
                print(f"ğŸ” æ£€æŸ¥æ·±åº¦å†…å®¹ç¼“å­˜: {url}")
                print(f"   ç¼“å­˜è·¯å¾„: {local_path}")

            if self._check_cache_validity(url, local_path):
                if self.verbose:
                    print(f"âœ… æ·±åº¦å†…å®¹ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡å¤„ç†: {node.get('name', '')}")
                else:
                    print(f"   âœ… è·³è¿‡å·²ç¼“å­˜çš„æ·±åº¦å†…å®¹: {node.get('name', '')}")

                # ä»ç¼“å­˜åŠ è½½æ•°æ®
                try:
                    cached_node = self._load_cached_node_data(local_path)
                    if cached_node:
                        return cached_node
                except Exception as e:
                    if self.verbose:
                        print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡æ–°å¤„ç†: {e}")

            if self.verbose:
                print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹æ·±åº¦å¤„ç†: {node.get('name', '')}")
            print(f"ğŸ¯ æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")
            print(f"   ğŸ“ URL: {url}")

            try:
                print(f"   ğŸŒ æ­£åœ¨è·å–é¡µé¢å†…å®¹...")
                soup = self.get_soup(url)
                if soup:
                    # åŸºæœ¬æ•°æ®ä¿®å¤
                    node = self.fix_node_data(node, soup)

                    # åªåœ¨ç›®æ ‡äº§å“é¡µé¢æ·»åŠ è¿™äº›å­—æ®µ
                    is_root_device = url == f"{self.base_url}/Device"
                    if not is_root_device:
                        # æ·»åŠ titleå­—æ®µ
                        real_title = self.extract_real_title_from_page(soup)
                        if real_title:
                            node['title'] = real_title

                        # æ·»åŠ instruction_urlå­—æ®µ
                        node['instruction_url'] = ""

                        # æ·»åŠ view_statisticså­—æ®µ
                        real_stats = self.extract_real_view_statistics(soup, url)
                        if real_stats:
                            node['view_statistics'] = real_stats

                    print(f"   ğŸ” æ­£åœ¨æœç´¢æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹...")
                    guides = self.extract_guides_from_device_page(soup, url)
                    guide_links = [guide["url"] for guide in guides]
                    print(f"   ğŸ“– æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

                    # ä½¿ç”¨å¹¶å‘å¤„ç†guideså’Œtroubleshooting
                    print(f"   âš™ï¸  å¼€å§‹å¤„ç†è¯¦ç»†å†…å®¹...")
                    guides_data, troubleshooting_data = self._process_content_concurrently(
                        guide_links, url, skip_troubleshooting, soup
                    )

                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                    # å¤„ç†å­ç±»åˆ« - æ— è®ºæ˜¯å¦ä¸ºæŒ‡å®šç›®æ ‡éƒ½è¦ä¿æŒå±‚çº§ç»“æ„
                    if is_specified_target:
                        # å¦‚æœæ˜¯æŒ‡å®šç›®æ ‡ï¼Œå°è¯•å‘ç°æ–°çš„å­ç±»åˆ«
                        subcategories = self.discover_subcategories_from_page(soup, url)
                        if subcategories:
                            print(f"  å‘ç° {len(subcategories)} ä¸ªå­ç±»åˆ«ï¼Œå¼€å§‹å¤„ç†...")

                            subcategory_children = []
                            for subcat in subcategories:
                                print(f"    å¤„ç†å­ç±»åˆ«: {subcat['name']}")

                                subcat_node = {
                                    'name': subcat['name'],
                                    'url': subcat['url'],
                                    'title': subcat['title']
                                }

                                processed_subcat = self.deep_crawl_product_content(subcat_node, skip_troubleshooting=True)
                                if processed_subcat:
                                    subcategory_children.append(processed_subcat)

                            if subcategory_children:
                                node['children'] = subcategory_children
                                print(f"  æˆåŠŸå¤„ç† {len(subcategory_children)} ä¸ªå­ç±»åˆ«")

                    # ä¿æŒç°æœ‰çš„childrenç»“æ„ï¼Œä¸è¦åˆ é™¤å±‚çº§å…³ç³»

                    print(f"  æ€»è®¡: {len(guides_data)} ä¸ªæŒ‡å—, {len(troubleshooting_data)} ä¸ªæ•…éšœæ’é™¤")

            except Exception as e:
                print(f"  âœ— æ·±å…¥çˆ¬å–æ—¶å‡ºé”™: {str(e)}")

        elif 'children' in node and node['children']:
            children_count = len(node['children'])
            if children_count > 0:
                print(f"ğŸ”„ é€’å½’å¤„ç† {children_count} ä¸ªå­èŠ‚ç‚¹...")
            for i, child in enumerate(node['children']):
                if children_count > 1:
                    print(f"   â””â”€ [{i+1}/{children_count}] å¤„ç†å­èŠ‚ç‚¹: {child.get('name', 'Unknown')}")
                node['children'][i] = self.deep_crawl_product_content(child, skip_troubleshooting)

        return node
        
    def _check_target_completeness(self, target_name):
        """æ£€æŸ¥ç›®æ ‡äº§å“ç›®å½•æ˜¯å¦å·²å­˜åœ¨å®Œæ•´çš„æ–‡ä»¶ç»“æ„"""
        if not target_name:
            return False, None

        safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
        target_dir = Path(self.storage_root) / f"auto_{safe_name}"

        if not target_dir.exists():
            return False, target_dir

        # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶ç»“æ„
        info_file = target_dir / "info.json"
        if not info_file.exists():
            return False, target_dir

        # æ£€æŸ¥æ˜¯å¦æœ‰guidesæˆ–troubleshootingç›®å½•
        guides_dir = target_dir / "guides"
        troubleshooting_dir = target_dir / "troubleshooting"

        has_content = False
        if guides_dir.exists() and list(guides_dir.glob("*.json")):
            has_content = True
        if troubleshooting_dir.exists() and list(troubleshooting_dir.glob("*.json")):
            has_content = True

        return has_content, target_dir

    def _get_target_root_dir(self, target_url):
        """ä»ç›®æ ‡URLè·å–çœŸå®çš„è®¾å¤‡è·¯å¾„"""
        if not target_url:
            return None

        # ä»URLä¸­æå–Deviceåçš„è·¯å¾„
        if '/Device/' in target_url:
            # æå–Deviceåé¢çš„è·¯å¾„éƒ¨åˆ†
            device_path = target_url.split('/Device/')[-1]
            # ç§»é™¤æŸ¥è¯¢å‚æ•°
            if '?' in device_path:
                device_path = device_path.split('?')[0]
            # ç§»é™¤æœ«å°¾çš„æ–œæ 
            device_path = device_path.rstrip('/')

            if device_path:
                # URLè§£ç 
                import urllib.parse
                device_path = urllib.parse.unquote(device_path)
                # æ„å»ºå®Œæ•´è·¯å¾„ï¼šifixit_data/Device/...
                return Path(self.storage_root) / "Device" / device_path
            else:
                # å¦‚æœæ˜¯æ ¹Deviceé¡µé¢
                return Path(self.storage_root) / "Device"

        # å¦‚æœä¸æ˜¯Device URLï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ä½œä¸ºåå¤‡
        safe_name = target_url.replace("/", "_").replace("\\", "_").replace(":", "_")
        return Path(self.storage_root) / "Device" / safe_name

    def save_combined_result(self, tree_data, target_name=None):
        """ä¿å­˜æ•´åˆç»“æœåˆ°çœŸå®çš„è®¾å¤‡è·¯å¾„ç»“æ„"""
        print("   ğŸ“ åˆ›å»ºç›®å½•ç»“æ„...")

        # ä½¿ç”¨ç›®æ ‡URLæ¥æ„å»ºçœŸå®è·¯å¾„
        target_url = getattr(self, 'target_url', None)
        if target_url:
            root_dir = self._get_target_root_dir(target_url)
        elif target_name:
            # å¦‚æœæ²¡æœ‰URLï¼Œå°è¯•ä»target_nameæ„å»º
            if target_name.startswith('http'):
                root_dir = self._get_target_root_dir(target_name)
            else:
                # å‡è®¾æ˜¯è®¾å¤‡åç§°ï¼Œæ„å»ºDeviceè·¯å¾„
                safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                root_dir = Path(self.storage_root) / "Device" / safe_name
        else:
            # ä»æ ‘æ•°æ®ä¸­æå–è·¯å¾„ä¿¡æ¯
            root_name = tree_data.get("name", "Unknown")
            if " > " in root_name:
                root_name = root_name.split(" > ")[-1]
            safe_name = root_name.replace("/", "_").replace("\\", "_").replace(":", "_")
            root_dir = Path(self.storage_root) / "Device" / safe_name

        if root_dir:
            root_dir.mkdir(parents=True, exist_ok=True)
            print(f"   ğŸ“‚ ç›®æ ‡è·¯å¾„: {root_dir}")

            # ä¿å­˜æ•´ä¸ªæ ‘ç»“æ„ï¼Œè€Œä¸æ˜¯åªä¿å­˜æ ¹èŠ‚ç‚¹
            print("   ğŸ’¾ ä¿å­˜å†…å®¹å’Œä¸‹è½½åª’ä½“æ–‡ä»¶...")
            self._save_tree_structure(tree_data, root_dir)

        print(f"\nğŸ“ æ•´åˆç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {root_dir}")
        return str(root_dir)

    def _save_tree_structure(self, tree_data, base_dir):
        """ä¿å­˜æ•´ä¸ªæ ‘ç»“æ„åˆ°çœŸå®çš„è®¾å¤‡è·¯å¾„"""
        if not tree_data or not isinstance(tree_data, dict):
            return

        # å¦‚æœæ˜¯ç›®æ ‡èŠ‚ç‚¹ï¼ˆæœ‰guidesæˆ–troubleshootingï¼‰ï¼Œç›´æ¥ä¿å­˜
        if tree_data.get('guides') or tree_data.get('troubleshooting'):
            self._save_node_content(tree_data, base_dir)

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in tree_data and tree_data['children']:
            for child in tree_data['children']:
                # ä¸ºå­èŠ‚ç‚¹åˆ›å»ºç›®å½•
                child_name = child.get('name', 'Unknown')
                safe_name = child_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                child_dir = base_dir / safe_name

                # é€’å½’ä¿å­˜å­èŠ‚ç‚¹
                self._save_tree_structure(child, child_dir)

    def _save_node_content(self, node_data, node_dir):
        """ä¿å­˜å•ä¸ªèŠ‚ç‚¹çš„å†…å®¹ï¼ˆguideså’Œtroubleshootingï¼‰"""
        if not node_data or not isinstance(node_data, dict):
            return

        # åˆ›å»ºèŠ‚ç‚¹ç›®å½•
        node_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
        info_data = {k: v for k, v in node_data.items()
                    if k not in ['children', 'guides', 'troubleshooting']}

        if info_data:
            info_file = node_dir / "info.json"
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(info_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = node_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            ts_media_dir = ts_dir / "media"
            ts_media_dir.mkdir(exist_ok=True)

            for i, ts in enumerate(node_data['troubleshooting']):
                ts_data = ts.copy()
                self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                with open(ts_file, 'w', encoding='utf-8') as f:
                    json.dump(ts_data, f, ensure_ascii=False, indent=2)

        # æ›´æ–°ç¼“å­˜ç´¢å¼•
        self._update_cache_for_node(node_data, node_dir)

    def _update_cache_for_node(self, node_data, node_dir):
        """ä¸ºèŠ‚ç‚¹æ›´æ–°ç¼“å­˜ç´¢å¼•"""
        if not self.cache_manager or not node_data:
            return

        url = node_data.get('url', '')
        if not url:
            return

        try:
            # ç»Ÿè®¡å†…å®¹æ•°é‡
            guides_count = len(node_data.get('guides', []))
            troubleshooting_count = len(node_data.get('troubleshooting', []))

            # ç»Ÿè®¡åª’ä½“æ–‡ä»¶æ•°é‡
            media_count = 0
            media_dir = node_dir / "media"
            if media_dir.exists():
                media_count = len(list(media_dir.glob("*")))

            # æ·»åŠ åˆ°ç¼“å­˜
            self.cache_manager.add_to_cache(
                url, node_dir,
                guides_count=guides_count,
                troubleshooting_count=troubleshooting_count,
                media_count=media_count
            )

        except Exception as e:
            self.logger.error(f"æ›´æ–°ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def _save_target_content_to_root(self, tree_data, root_dir):
        """å°†ç›®æ ‡äº§å“å†…å®¹ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•ï¼Œä¼˜åŒ–ç›®å½•ç»“æ„"""
        if not tree_data or not isinstance(tree_data, dict):
            return

        # å¤åˆ¶æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = tree_data.copy()
        self._process_media_urls(node_data, root_dir)

        # ä¿å­˜ä¸»è¦ä¿¡æ¯åˆ°æ ¹ç›®å½•çš„info.json
        info_data = {k: v for k, v in node_data.items()
                    if k not in ['children', 'guides', 'troubleshooting']}

        info_file = root_dir / "info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides - ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•çš„guidesæ–‡ä»¶å¤¹
        if 'guides' in node_data and node_data['guides']:
            guides_dir = root_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting - ç›´æ¥ä¿å­˜åˆ°æ ¹ç›®å½•çš„troubleshootingæ–‡ä»¶å¤¹
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = root_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            ts_media_dir = ts_dir / "media"
            ts_media_dir.mkdir(exist_ok=True)

            for i, ts in enumerate(node_data['troubleshooting']):
                ts_data = ts.copy()
                self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                with open(ts_file, 'w', encoding='utf-8') as f:
                    json.dump(ts_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†å­ç±»åˆ« - ä¿å­˜åˆ°subcategoriesæ–‡ä»¶å¤¹
        if 'children' in node_data and node_data['children']:
            subcategories_dir = root_dir / "subcategories"
            subcategories_dir.mkdir(exist_ok=True)
            for child in node_data['children']:
                self._save_subcategory_to_filesystem(child, subcategories_dir)

    def _save_subcategory_to_filesystem(self, node, subcategories_dir):
        """ä¿å­˜å­ç±»åˆ«åˆ°subcategoriesç›®å½•"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")

        # åˆ›å»ºå­ç±»åˆ«ç›®å½•
        subcat_dir = subcategories_dir / safe_name
        subcat_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, subcat_dir)

        # ä¿å­˜å­ç±»åˆ«ä¿¡æ¯
        info_file = subcat_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†å­ç±»åˆ«çš„guideså’Œtroubleshooting
        if 'guides' in node_data and node_data['guides']:
            guides_dir = subcat_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)  # ä½¿ç”¨guidesç›®å½•çš„mediaæ–‡ä»¶å¤¹
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = subcat_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            ts_media_dir = ts_dir / "media"
            ts_media_dir.mkdir(exist_ok=True)

            for i, ts in enumerate(node_data['troubleshooting']):
                ts_data = ts.copy()
                self._process_media_urls(ts_data, ts_dir)  # ä½¿ç”¨troubleshootingç›®å½•çš„mediaæ–‡ä»¶å¤¹
                ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                with open(ts_file, 'w', encoding='utf-8') as f:
                    json.dump(ts_data, f, ensure_ascii=False, indent=2)

        # é€’å½’å¤„ç†æ›´æ·±å±‚çš„å­ç±»åˆ«
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_subcategory_to_filesystem(child, subcat_dir)

    def _save_node_to_filesystem(self, node, base_dir, path_prefix):
        """é€’å½’ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        if not node or not isinstance(node, dict):
            return

        node_name = node.get('name', 'unknown')
        safe_name = node_name.replace("/", "_").replace("\\", "_").replace(":", "_")

        # åˆ›å»ºå½“å‰èŠ‚ç‚¹ç›®å½•
        current_path = path_prefix + "/" + safe_name if path_prefix else safe_name
        node_dir = base_dir / current_path
        node_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶èŠ‚ç‚¹æ•°æ®å¹¶å¤„ç†åª’ä½“URL
        node_data = node.copy()
        self._process_media_urls(node_data, node_dir)

        # ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°JSONæ–‡ä»¶
        info_file = node_dir / "info.json"
        node_info = {k: v for k, v in node_data.items() if k not in ['children', 'guides', 'troubleshooting']}
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(node_info, f, ensure_ascii=False, indent=2)

        # å¤„ç†guides
        if 'guides' in node_data and node_data['guides']:
            guides_dir = node_dir / "guides"
            guides_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            guides_media_dir = guides_dir / "media"
            guides_media_dir.mkdir(exist_ok=True)

            for i, guide in enumerate(node_data['guides']):
                guide_data = guide.copy()
                self._process_media_urls(guide_data, guides_dir)
                guide_file = guides_dir / f"guide_{i+1}.json"
                with open(guide_file, 'w', encoding='utf-8') as f:
                    json.dump(guide_data, f, ensure_ascii=False, indent=2)

        # å¤„ç†troubleshooting
        if 'troubleshooting' in node_data and node_data['troubleshooting']:
            ts_dir = node_dir / "troubleshooting"
            ts_dir.mkdir(exist_ok=True)
            # ç¡®ä¿mediaæ–‡ä»¶å¤¹å­˜åœ¨
            ts_media_dir = ts_dir / "media"
            ts_media_dir.mkdir(exist_ok=True)

            for i, ts in enumerate(node_data['troubleshooting']):
                ts_data = ts.copy()
                self._process_media_urls(ts_data, ts_dir)
                ts_file = ts_dir / f"troubleshooting_{i+1}.json"
                with open(ts_file, 'w', encoding='utf-8') as f:
                    json.dump(ts_data, f, ensure_ascii=False, indent=2)

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node_data and node_data['children']:
            for child in node_data['children']:
                self._save_node_to_filesystem(child, base_dir, current_path)
        
    def print_combined_tree_structure(self, node, level=0):
        """
        æ‰“å°æ•´åˆåçš„æ ‘å½¢ç»“æ„ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦
        """
        if not node:
            return

        indent = "  " * level
        name = node.get('name', 'Unknown')

        # ç»Ÿè®¡å½“å‰èŠ‚ç‚¹çš„å†…å®¹
        guides_count = len(node.get('guides', []))
        troubleshooting_count = len(node.get('troubleshooting', []))
        children_count = len(node.get('children', []))

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if 'title' in node and 'steps' in node:
            # ç‹¬ç«‹çš„æŒ‡å—èŠ‚ç‚¹
            steps_count = len(node.get('steps', []))
            print(f"{indent}ğŸ“– {name} [æŒ‡å— - {steps_count}æ­¥éª¤]")
        elif 'causes' in node:
            # ç‹¬ç«‹çš„æ•…éšœæ’é™¤èŠ‚ç‚¹
            causes_count = len(node.get('causes', []))
            print(f"{indent}ğŸ”§ {name} [æ•…éšœæ’é™¤ - {causes_count}åŸå› ]")
        elif 'product_name' in node or ('title' in node and 'instruction_url' in node):
            # äº§å“èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“± {name} [äº§å“{content_str}]")
        elif children_count > 0 or guides_count > 0 or troubleshooting_count > 0:
            # åˆ†ç±»èŠ‚ç‚¹
            content_info = []
            if guides_count > 0:
                content_info.append(f"{guides_count}æŒ‡å—")
            if troubleshooting_count > 0:
                content_info.append(f"{troubleshooting_count}æ•…éšœæ’é™¤")
            if children_count > 0:
                content_info.append(f"{children_count}å­é¡¹")

            content_str = " - " + ", ".join(content_info) if content_info else ""
            print(f"{indent}ğŸ“ {name} [åˆ†ç±»{content_str}]")
        else:
            # å…¶ä»–èŠ‚ç‚¹
            print(f"{indent}â“ {name} [æœªçŸ¥ç±»å‹]")

        # æ˜¾ç¤ºguideså†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if guides_count > 0:
            for guide in node.get('guides', []):
                guide_name = guide.get('title', 'Unknown Guide')
                steps_count = len(guide.get('steps', []))
                print(f"{indent}  ğŸ“– {guide_name} [{steps_count}æ­¥éª¤]")

        # æ˜¾ç¤ºtroubleshootingå†…å®¹ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        if troubleshooting_count > 0:
            for ts in node.get('troubleshooting', []):
                ts_name = ts.get('title', 'Unknown Troubleshooting')
                causes_count = len(ts.get('causes', []))
                print(f"{indent}  ğŸ”§ {ts_name} [{causes_count}åŸå› ]")

        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if children_count > 0:
            for child in node['children']:
                self.print_combined_tree_structure(child, level + 1)

    def extract_dynamic_sections(self, soup):
        """åŠ¨æ€æå–é¡µé¢ä¸Šçš„çœŸå®å­—æ®µåç§°å’Œå†…å®¹ï¼ŒåŸºäºå®é™…é¡µé¢ç»“æ„ï¼Œç¡®ä¿å­—æ®µåˆ†ç¦»å’Œæ— é‡å¤"""
        sections = {}
        seen_content = set()  # ç”¨äºå»é‡
        processed_elements = set()  # è®°å½•å·²å¤„ç†çš„å…ƒç´ ï¼Œé¿å…é‡å¤å¤„ç†

        try:
            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸï¼Œé¿å…å¯¼èˆªå’Œé¡µè„š
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda x: x and 'content' in str(x).lower())
            if not main_content:
                main_content = soup

            # é¦–å…ˆé€šè¿‡IDå’Œæ ‡é¢˜æ–‡æœ¬ç²¾ç¡®å®šä½å„ä¸ªç‹¬ç«‹éƒ¨åˆ†
            section_mappings = [
                ('introduction', ['#introduction']),
                ('first_steps', ['#Section_First_Steps']),
                ('triage', ['#Section_Triage'])
            ]

            for field_name, selectors in section_mappings:
                found = False
                for selector in selectors:
                    section_element = main_content.select_one(selector)
                    if section_element and id(section_element) not in processed_elements:
                        content = self.extract_section_content_precisely(section_element, main_content, field_name)
                        if content and content.strip():
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸å·²æœ‰å†…å®¹é‡å¤
                            if not self.is_content_duplicate(content, seen_content):
                                sections[field_name] = content.strip()
                                seen_content.add(content.strip())
                                processed_elements.add(id(section_element))
                                print(f"ç²¾ç¡®æå–å­—æ®µ: {field_name} ({len(content)} å­—ç¬¦)")
                                found = True
                                break

                # å¦‚æœé€šè¿‡IDæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡æ ‡é¢˜æ–‡æœ¬æŸ¥æ‰¾
                if not found:
                    target_texts = {
                        'introduction': ['introduction'],
                        'first_steps': ['first steps', 'before undertaking'],
                        'triage': ['triage']
                    }

                    if field_name in target_texts:
                        for heading in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                            heading_text = heading.get_text().strip().lower()
                            if any(target in heading_text for target in target_texts[field_name]):
                                if id(heading) not in processed_elements:
                                    content = self.extract_section_content_precisely(heading, main_content, field_name)
                                    if content and content.strip():
                                        if not self.is_content_duplicate(content, seen_content):
                                            sections[field_name] = content.strip()
                                            seen_content.add(content.strip())
                                            processed_elements.add(id(heading))
                                            print(f"é€šè¿‡æ ‡é¢˜æå–å­—æ®µ: {field_name} ({len(content)} å­—ç¬¦)")
                                            found = True
                                            break

        except Exception as e:
            print(f"åŠ¨æ€æå–sectionsæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

        return sections

    def extract_section_content_precisely(self, element, main_content, field_name):
        """ç²¾ç¡®æå–sectionå†…å®¹"""
        try:
            content_parts = []

            # ä»å…ƒç´ åé¢å¼€å§‹æå–å†…å®¹
            next_elem = element.find_next_sibling()
            while next_elem and next_elem.name not in ['h1', 'h2', 'h3']:
                if next_elem.name in ['p', 'div', 'ul', 'ol']:
                    text = next_elem.get_text().strip()
                    if text and len(text) > 10 and not self.is_commercial_text(text):
                        content_parts.append(text)
                next_elem = next_elem.find_next_sibling()

            return '\n\n'.join(content_parts) if content_parts else ""
        except Exception:
            return ""

    def is_content_duplicate(self, content, seen_content):
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        content_clean = content.strip()
        return content_clean in seen_content

    def extract_causes_sections_with_media(self, soup):
        """æå–Causeséƒ¨åˆ†å†…å®¹ï¼Œå¹¶ä¸ºæ¯ä¸ªcauseæå–å¯¹åº”çš„å›¾ç‰‡å’Œè§†é¢‘"""
        causes = []
        seen_numbers = set()  # é˜²æ­¢é‡å¤

        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¸¦ç¼–å·çš„é“¾æ¥ï¼Œè¿™äº›é€šå¸¸æ˜¯causes
            cause_links = soup.find_all('a', href=lambda x: x and '#Section_' in x)

            for link in cause_links:
                href = link.get('href', '')
                if '#Section_' in href:
                    # æå–ç¼–å·å’Œæ ‡é¢˜
                    link_text = link.get_text().strip()

                    # è·³è¿‡éç¼–å·çš„é“¾æ¥
                    if not any(char.isdigit() for char in link_text):
                        continue

                    # æå–ç¼–å·
                    number_match = re.search(r'^(\d+)', link_text)
                    if number_match:
                        number = number_match.group(1)

                        # é˜²æ­¢é‡å¤
                        if number in seen_numbers:
                            continue
                        seen_numbers.add(number)

                        title = re.sub(r'^\d+\s*', '', link_text).strip()

                        # æŸ¥æ‰¾å¯¹åº”çš„å†…å®¹éƒ¨åˆ†
                        section_id = href.split('#')[-1]
                        content_section = soup.find(id=section_id)

                        content = ""
                        images = []
                        videos = []

                        if content_section:
                            content = self.extract_section_content(content_section)
                            # ä»è¯¥sectionä¸­æå–å›¾ç‰‡å’Œè§†é¢‘
                            images = self._extract_troubleshooting_images_from_section(content_section)
                            videos = self.extract_videos_from_section(content_section)

                        cause_data = {
                            "number": number,
                            "title": title,
                            "content": content
                        }

                        # åªæœ‰å½“æœ‰å›¾ç‰‡æ—¶æ‰æ·»åŠ imageså­—æ®µ
                        if images:
                            cause_data["images"] = images

                        # åªæœ‰å½“æœ‰è§†é¢‘æ—¶æ‰æ·»åŠ videoså­—æ®µ
                        if videos:
                            cause_data["videos"] = videos

                        causes.append(cause_data)

                        if self.verbose:
                            print(f"æå–Cause {number}: {title} ({len(content)} å­—ç¬¦, {len(images)} å›¾ç‰‡, {len(videos)} è§†é¢‘)")

            return causes
        except Exception as e:
            print(f"æå–Causesæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def extract_section_content(self, section):
        """ä»sectionä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            content_parts = []
            p_elements = section.find_all('p')
            for p in p_elements:
                p_text = p.get_text().strip()
                if self.is_commercial_text(p_text):
                    continue
                if p_text and len(p_text) > 10:
                    content_parts.append(p_text)
            return '\n\n'.join(content_parts)
        except Exception:
            return ""

    def extract_videos_from_section(self, section):
        """ä»sectionä¸­æå–è§†é¢‘"""
        videos = []
        try:
            video_elements = section.find_all(['video', 'iframe'])
            for video in video_elements:
                try:
                    from bs4 import Tag
                    if isinstance(video, Tag):
                        video_src = video.get('src') or video.get('data-src')
                        if video_src and isinstance(video_src, str):
                            if 'youtube.com' in video_src or 'youtu.be' in video_src:
                                video_data = {
                                    "url": video_src,
                                    "title": video.get('title', 'Video')
                                }
                                videos.append(video_data)
                except Exception:
                    continue
        except Exception:
            pass
        return videos

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("=" * 60)
    print("iFixitæ•´åˆçˆ¬è™«å·¥å…· - ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python auto_crawler.py [URLæˆ–è®¾å¤‡å] [é€‰é¡¹...]")
    print("\nåŸºæœ¬ç”¨æ³•:")
    print("  python auto_crawler.py iMac_M_Series")
    print("  python auto_crawler.py https://www.ifixit.com/Device/Television")
    print("  python auto_crawler.py Television")
    print("\nå¸¸ç”¨é€‰é¡¹:")
    print("  --verbose              å¯ç”¨è¯¦ç»†è¾“å‡º")
    print("  --no-proxy             å…³é—­ä»£ç†æ± ")
    print("  --no-cache             ç¦ç”¨ç¼“å­˜æ£€æŸ¥")
    print("  --force-refresh        å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
    print("  --workers N            è®¾ç½®å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰")
    print("  --max-retries N        è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤5ï¼‰")
    print("\nè§†é¢‘å¤„ç†é€‰é¡¹:")
    print("  --download-videos      å¯ç”¨è§†é¢‘æ–‡ä»¶ä¸‹è½½ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰")
    print("  --max-video-size N     è®¾ç½®è§†é¢‘æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼Œé»˜è®¤50ï¼‰")
    print("\nç¤ºä¾‹:")
    print("  # åŸºæœ¬çˆ¬å–ï¼ˆä¸ä¸‹è½½è§†é¢‘ï¼‰")
    print("  python auto_crawler.py iMac_M_Series")
    print("")
    print("  # å¯ç”¨è§†é¢‘ä¸‹è½½ï¼Œé™åˆ¶10MB")
    print("  python auto_crawler.py iMac_M_Series --download-videos --max-video-size 10")
    print("")
    print("  # è¯¦ç»†è¾“å‡ºï¼Œç¦ç”¨ä»£ç†")
    print("  python auto_crawler.py Television --verbose --no-proxy")
    print("\nåŠŸèƒ½è¯´æ˜:")
    print("- æ™ºèƒ½ç¼“å­˜ï¼šè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„å†…å®¹")
    print("- ä»£ç†æ± ï¼šå¤©å¯IPè‡ªåŠ¨åˆ‡æ¢ï¼Œé¿å…å°ç¦")
    print("- å¹¶å‘çˆ¬å–ï¼šå¤šçº¿ç¨‹æå‡çˆ¬å–é€Ÿåº¦")
    print("- é”™è¯¯é‡è¯•ï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•")
    print("- åª’ä½“ä¸‹è½½ï¼šè‡ªåŠ¨ä¸‹è½½å¹¶æœ¬åœ°åŒ–æ‰€æœ‰åª’ä½“æ–‡ä»¶")
    print("- è§†é¢‘å¤„ç†ï¼šå¯é€‰æ‹©æ€§ä¸‹è½½è§†é¢‘æ–‡ä»¶ï¼Œæ”¯æŒå¤§å°é™åˆ¶")
    print("=" * 60)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = sys.argv[1:]

    # æ£€æŸ¥å¸®åŠ©å‚æ•°
    if not args or args[0].lower() in ['--help', '-h', 'help']:
        print_usage()
        return

    # è·å–ç›®æ ‡URL/åç§°
    input_text = args[0]

    # è§£æé€‰é¡¹
    verbose = '--verbose' in args
    use_proxy = '--no-proxy' not in args  # é»˜è®¤å¯ç”¨ä»£ç†
    use_cache = '--no-cache' not in args  # é»˜è®¤å¯ç”¨ç¼“å­˜
    force_refresh = '--force-refresh' in args

    # è§£æworkerså‚æ•°
    max_workers = 4
    if '--workers' in args:
        try:
            workers_idx = args.index('--workers')
            if workers_idx + 1 < len(args):
                max_workers = int(args[workers_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: workerså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼4")

    # è§£æmax-retrieså‚æ•°
    max_retries = 3  # å‡å°‘é»˜è®¤é‡è¯•æ¬¡æ•°
    if '--max-retries' in args:
        try:
            retries_idx = args.index('--max-retries')
            if retries_idx + 1 < len(args):
                max_retries = int(args[retries_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-retrieså‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼3")

    # è§£æè§†é¢‘ä¸‹è½½å‚æ•°
    download_videos = '--download-videos' in args
    max_video_size_mb = 50
    if '--max-video-size' in args:
        try:
            size_idx = args.index('--max-video-size')
            if size_idx + 1 < len(args):
                max_video_size_mb = int(args[size_idx + 1])
        except (ValueError, IndexError):
            print("è­¦å‘Š: max-video-sizeå‚æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼50MB")

    if not input_text:
        print_usage()
        return

    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)

    if url:
        # åˆ›å»ºæ•´åˆçˆ¬è™«å®ä¾‹ï¼ˆç”¨äºæ£€æµ‹ï¼‰
        temp_crawler = CombinedIFixitCrawler(verbose=False, use_proxy=False)

        # æ£€æŸ¥ç›®æ ‡æ˜¯å¦å·²ç»å®Œæ•´çˆ¬å–
        is_complete, target_dir = temp_crawler._check_target_completeness(input_text)

        if is_complete and not force_refresh and target_dir:
            print("\n" + "=" * 60)
            print(f"ğŸ¯ æ£€æµ‹åˆ°ç›®æ ‡å·²å®Œæˆçˆ¬å–: {name}")
            print(f"ğŸ“ ç›®å½•ä½ç½®: {target_dir}")
            print("=" * 60)

            # æ£€æŸ¥ç›®å½•å†…å®¹
            info_file = target_dir / "info.json"
            guides_dir = target_dir / "guides"
            troubleshooting_dir = target_dir / "troubleshooting"
            media_dir = target_dir / "media"

            guides_count = len(list(guides_dir.glob("*.json"))) if guides_dir.exists() else 0
            ts_count = len(list(troubleshooting_dir.glob("*.json"))) if troubleshooting_dir.exists() else 0
            media_count = len(list(media_dir.glob("*"))) if media_dir.exists() else 0

            print(f"ğŸ“Š å·²æœ‰å†…å®¹ç»Ÿè®¡:")
            print(f"   åŸºæœ¬ä¿¡æ¯: {'âœ…' if info_file.exists() else 'âŒ'}")
            print(f"   æŒ‡å—æ•°é‡: {guides_count}")
            print(f"   æ•…éšœæ’é™¤æ•°é‡: {ts_count}")
            print(f"   åª’ä½“æ–‡ä»¶æ•°é‡: {media_count}")

            print(f"\nğŸ’¡ é€‰é¡¹:")
            print(f"   1. è·³è¿‡çˆ¬å–ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
            print(f"   2. é‡æ–°çˆ¬å–ï¼ˆè¦†ç›–ç°æœ‰æ•°æ®ï¼‰")
            print(f"   3. é€€å‡ºç¨‹åº")

            while True:
                choice = input("\nè¯·é€‰æ‹© (1/2/3): ").strip()
                if choice == "1":
                    print(f"âœ… è·³è¿‡çˆ¬å–ï¼Œç°æœ‰æ•°æ®ä½ç½®: {target_dir}")
                    return
                elif choice == "2":
                    print("ğŸ”„ å°†é‡æ–°çˆ¬å–å¹¶è¦†ç›–ç°æœ‰æ•°æ®...")
                    force_refresh = True
                    break
                elif choice == "3":
                    print("ğŸ‘‹ ç¨‹åºé€€å‡º")
                    return
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")

        print("\n" + "=" * 60)
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {name}")
        print("é˜¶æ®µ1: æ„å»ºå®Œæ•´æ ‘å½¢ç»“æ„")
        print("é˜¶æ®µ2: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹")
        print("=" * 60)

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"ğŸ“‹ çˆ¬å–é…ç½®:")
        print(f"   ç›®æ ‡: {name}")
        print(f"   è¯¦ç»†è¾“å‡º: {'æ˜¯' if verbose else 'å¦'}")
        print(f"   ä½¿ç”¨ä»£ç†: {'æ˜¯' if use_proxy else 'å¦'}")
        print(f"   ä½¿ç”¨ç¼“å­˜: {'æ˜¯' if use_cache else 'å¦'}")
        print(f"   å¼ºåˆ¶åˆ·æ–°: {'æ˜¯' if force_refresh else 'å¦'}")
        print(f"   å¹¶å‘æ•°: {max_workers}")
        print(f"   æœ€å¤§é‡è¯•: {max_retries}")
        print(f"   ä¸‹è½½è§†é¢‘: {'æ˜¯' if download_videos else 'å¦'}")
        if download_videos:
            print(f"   è§†é¢‘å¤§å°é™åˆ¶: {max_video_size_mb}MB")

        # åˆ›å»ºæ•´åˆçˆ¬è™«
        crawler = CombinedIFixitCrawler(
            verbose=verbose,
            use_proxy=use_proxy,
            use_cache=use_cache,
            force_refresh=force_refresh,
            max_workers=max_workers,
            max_retries=max_retries,
            download_videos=download_videos,
            max_video_size_mb=max_video_size_mb
        )

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œæ•´åˆçˆ¬å–
        try:
            combined_data = crawler.crawl_combined_tree(url, name)

            if combined_data:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = crawler.count_detailed_nodes(combined_data)
                elapsed_time = time.time() - start_time

                # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
                print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
                print("=" * 60)
                crawler.print_combined_tree_structure(combined_data)
                print("=" * 60)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\næ•´åˆçˆ¬å–ç»Ÿè®¡:")
                print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
                print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
                print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
                print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
                print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
                print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

                # ä¿å­˜ç»“æœ
                print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶å¤¹...")
                filename = crawler.save_combined_result(combined_data, target_name=input_text)

                print(f"\nâœ… æ•´åˆçˆ¬å–å®Œæˆ!")
                print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {filename}")

                # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
                crawler._print_performance_stats()

                # ä¿å­˜ç¼“å­˜ç´¢å¼•
                if crawler.cache_manager:
                    crawler.cache_manager.save_cache_index()
                    cache_stats = crawler.cache_manager.get_cache_stats()
                    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
                    print(f"- æ€»å¤„ç†URL: {cache_stats['total_urls']}")
                    print(f"- ç¼“å­˜å‘½ä¸­: {cache_stats['cached_urls']}")
                    print(f"- æ–°å¢å¤„ç†: {cache_stats['new_urls']}")
                    if cache_stats['total_urls'] > 0:
                        hit_rate = (cache_stats['cached_urls'] / cache_stats['total_urls']) * 100
                        print(f"- ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")

                # æä¾›ä½¿ç”¨å»ºè®®
                print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
                print(f"- æŸ¥çœ‹æ–‡ä»¶å¤¹ç»“æ„äº†è§£å®Œæ•´çš„æ•°æ®å±‚çº§")
                print(f"- æ¯ä¸ªç›®å½•åŒ…å«info.jsonã€guides/ã€troubleshooting/å­ç›®å½•")
                print(f"- æ‰€æœ‰åª’ä½“æ–‡ä»¶å·²ä¸‹è½½åˆ°media/ç›®å½•ï¼ŒURLå·²æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„")
                print(f"- ä¸‹æ¬¡çˆ¬å–ç›¸åŒå†…å®¹å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œå¤§å¹…æå‡é€Ÿåº¦")

            else:
                print("\nâœ— æ•´åˆçˆ¬å–å¤±è´¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
            # å³ä½¿ä¸­æ–­ä¹Ÿä¿å­˜ç¼“å­˜ç´¢å¼•
            if crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
        except Exception as e:
            print(f"\nâœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ç¼“å­˜ç´¢å¼•
            if crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
            if verbose:
                import traceback
                traceback.print_exc()
        finally:
            # ç¡®ä¿ç¼“å­˜ç´¢å¼•è¢«ä¿å­˜
            if 'crawler' in locals() and crawler.cache_manager:
                crawler.cache_manager.save_cache_index()
    else:
        print_usage()

def test_tianqi_proxy():
    """æµ‹è¯•å¤©å¯IPä»£ç†æ± åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•å¤©å¯IPä»£ç†æ± åŠŸèƒ½")
    print("=" * 60)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œå¯ç”¨ä»£ç†
    crawler = CombinedIFixitCrawler(
        verbose=True,
        use_proxy=True,
        use_cache=False,
        max_workers=1
    )

    # æµ‹è¯•ä»£ç†è·å–
    print("\n1. æµ‹è¯•ä»£ç†è·å–...")
    if crawler.proxy_manager:
        proxy = crawler.proxy_manager.get_proxy()
        if proxy:
            print("âœ… ä»£ç†è·å–æˆåŠŸ")
            print(f"å½“å‰ä»£ç†: {proxy}")
            stats = crawler.proxy_manager.get_stats()
            print(f"ä»£ç†æ± ç»Ÿè®¡: {stats}")
        else:
            print("âŒ ä»£ç†è·å–å¤±è´¥")
            return
    else:
        print("âŒ ä»£ç†ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return

    # æµ‹è¯•ä»£ç†åˆ‡æ¢
    print("\n2. æµ‹è¯•ä»£ç†åˆ‡æ¢...")
    old_proxy = crawler.current_proxy.copy() if crawler.current_proxy else None
    if crawler._switch_proxy("æµ‹è¯•åˆ‡æ¢"):
        new_proxy = crawler.current_proxy
        if new_proxy != old_proxy:
            print("âœ… ä»£ç†åˆ‡æ¢æˆåŠŸ")
            print(f"æ–°ä»£ç†: {new_proxy}")
        else:
            print("âš ï¸  ä»£ç†æœªå˜åŒ–ï¼ˆå¯èƒ½æ˜¯ä»£ç†æ± ä¸ºç©ºï¼‰")
    else:
        print("âŒ ä»£ç†åˆ‡æ¢å¤±è´¥")

    # æµ‹è¯•å®é™…ç½‘ç»œè¯·æ±‚
    print("\n3. æµ‹è¯•å®é™…ç½‘ç»œè¯·æ±‚...")
    test_url = "https://www.ifixit.com/Device"
    try:
        soup = crawler.get_soup(test_url)
        if soup:
            title = soup.find('title')
            print(f"âœ… ç½‘ç»œè¯·æ±‚æˆåŠŸ")
            print(f"é¡µé¢æ ‡é¢˜: {title.get_text() if title else 'No title'}")
        else:
            print("âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œæœªè·å–åˆ°é¡µé¢å†…å®¹")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")

    print("\n" + "=" * 60)
    print("ğŸ ä»£ç†æ± æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "--test-proxy":
        test_tianqi_proxy()
        sys.exit(0)

    main()
