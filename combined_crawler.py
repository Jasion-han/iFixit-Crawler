#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•´åˆçˆ¬è™«å·¥å…· - ç»“åˆæ ‘å½¢ç»“æ„å’Œè¯¦ç»†å†…å®¹æå–
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
ç”¨æ³•: python combined_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python combined_crawler.py https://www.ifixit.com/Device/Television
      python combined_crawler.py Television
"""

import os
import sys
import json
import time
import random

# å¯¼å…¥ä¸¤ä¸ªåŸºç¡€çˆ¬è™«
from enhanced_crawler import EnhancedIFixitCrawler
from tree_crawler import TreeCrawler

class CombinedIFixitCrawler(EnhancedIFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", verbose=False):
        super().__init__(base_url, verbose)
        # åˆå§‹åŒ–æ ‘å½¢çˆ¬è™«çš„åŠŸèƒ½
        self.tree_crawler = TreeCrawler(base_url)
        self.processed_nodes = set()  # è®°å½•å·²å¤„ç†çš„èŠ‚ç‚¹ï¼Œé¿å…é‡å¤

        # å¤åˆ¶æ ‘å½¢çˆ¬è™«çš„é‡è¦æ–¹æ³•åˆ°å½“å‰ç±»
        self.find_exact_path = self.tree_crawler.find_exact_path
        self.extract_breadcrumbs_from_page = self.tree_crawler.extract_breadcrumbs_from_page
        self._build_tree_from_path = self.tree_crawler._build_tree_from_path
        self._find_node_by_url = self.tree_crawler._find_node_by_url
        self.ensure_instruction_url_in_leaf_nodes = self.tree_crawler.ensure_instruction_url_in_leaf_nodes

    def extract_real_name_from_url(self, url):
        """
        ä»URLä¸­æå–çœŸå®çš„nameå­—æ®µï¼ˆURLè·¯å¾„çš„æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦ï¼‰
        """
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_url = url.split('?')[0].split('#')[0]

        # ç§»é™¤æœ«å°¾çš„æ–œæ 
        clean_url = clean_url.rstrip('/')

        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µ
        path_parts = clean_url.split('/')
        if path_parts:
            name = path_parts[-1]
            # URLè§£ç 
            import urllib.parse
            name = urllib.parse.unquote(name)
            # ä¿®å¤å¼•å·ç¼–ç é—®é¢˜ï¼šå°† \" è½¬æ¢ä¸º "
            name = name.replace('\\"', '"')
            return name

        return ""

    def extract_real_title_from_page(self, soup):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„titleå­—æ®µ
        """
        if not soup:
            return ""

        # ä¼˜å…ˆä»h1æ ‡ç­¾æå–
        h1_elem = soup.select_one("h1")
        if h1_elem:
            title = h1_elem.get_text().strip()
            if title:
                return title

        # å…¶æ¬¡ä»titleæ ‡ç­¾æå–
        title_elem = soup.select_one("title")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = title.replace(" - iFixit", "").replace(" | iFixit", "").strip()
            if title:
                return title

        return ""

    def _is_specified_target_url(self, url):
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        """
        # è¿™é‡Œå¯ä»¥æ·»åŠ é€»è¾‘æ¥æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        # ç›®å‰ç®€å•è¿”å›Falseï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨åˆ¤æ–­å¶å­èŠ‚ç‚¹
        return False

    def extract_real_view_statistics(self, soup, url):
        """
        ä»é¡µé¢HTMLä¸­æå–çœŸå®çš„æµè§ˆç»Ÿè®¡æ•°æ®
        """
        if not soup:
            return {}

        stats = {}

        # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®çš„å„ç§å¯èƒ½ä½ç½®
        stat_selectors = [
            '.view-count',
            '.stats-item',
            '.view-stats',
            '[data-stats]',
            '.page-stats'
        ]

        for selector in stat_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # è§£æç»Ÿè®¡æ•°æ®æ ¼å¼
                if 'past 24 hours' in text.lower() or '24h' in text.lower():
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_24_hours'] = numbers[0]
                elif 'past 7 days' in text.lower() or '7d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_7_days'] = numbers[0]
                elif 'past 30 days' in text.lower() or '30d' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['past_30_days'] = numbers[0]
                elif 'all time' in text.lower() or 'total' in text.lower():
                    numbers = re.findall(r'[\d,]+', text)
                    if numbers:
                        stats['all_time'] = numbers[0]

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»Ÿè®¡æ•°æ®ï¼Œå°è¯•ä»APIæˆ–å…¶ä»–ä½ç½®è·å–
        if not stats:
            # å°è¯•ä»é¡µé¢çš„JavaScriptæ•°æ®ä¸­æå–
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    script_content = script.string
                    if 'viewCount' in script_content or 'statistics' in script_content:
                        import re
                        # æŸ¥æ‰¾å¯èƒ½çš„ç»Ÿè®¡æ•°æ®
                        view_matches = re.findall(r'"viewCount[^"]*":\s*"?(\d+[,\d]*)"?', script_content)
                        if view_matches:
                            stats['all_time'] = view_matches[0]

        return stats

    def build_correct_url_from_path(self, base_url, path_segments):
        """
        æ ¹æ®è·¯å¾„æ®µæ­£ç¡®æ„å»ºURLï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥
        """
        if not path_segments:
            return base_url

        # ä»base_urlå¼€å§‹ï¼Œé€æ­¥æ›¿æ¢è·¯å¾„æ®µ
        current_url = base_url

        for segment in path_segments:
            # è·å–å½“å‰URLçš„è·¯å¾„éƒ¨åˆ†
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(current_url)
            path_parts = [p for p in parsed.path.split('/') if p]

            # æ·»åŠ æ–°çš„è·¯å¾„æ®µ
            path_parts.append(segment)

            # é‡æ–°æ„å»ºURL
            new_path = '/' + '/'.join(path_parts)
            new_parsed = parsed._replace(path=new_path)
            current_url = urlunparse(new_parsed)

        return current_url

    def restructure_target_content(self, node, is_target_page=False):
        """
        é‡æ„ç›®æ ‡æ–‡ä»¶çš„å†…å®¹ç»“æ„ï¼Œä½¿ç”¨guides[]å’Œtroubleshooting[]è€Œéchildren[]
        """
        if not node or not isinstance(node, dict):
            return node

        # å¦‚æœè¿™æ˜¯ç›®æ ‡é¡µé¢ä¸”æœ‰childrenåŒ…å«guideså’Œtroubleshooting
        if is_target_page and 'children' in node and node['children']:
            guides = []
            troubleshooting = []
            real_children = []

            for child in node['children']:
                if 'steps' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæŒ‡å—
                    guide_data = {k: v for k, v in child.items() if k not in ['children']}
                    guides.append(guide_data)
                elif 'causes' in child and 'title' in child:
                    # è¿™æ˜¯ä¸€ä¸ªæ•…éšœæ’é™¤
                    ts_data = {k: v for k, v in child.items() if k not in ['children']}
                    troubleshooting.append(ts_data)
                else:
                    # è¿™æ˜¯çœŸæ­£çš„å­ç±»åˆ«
                    real_children.append(child)

            # é‡æ„èŠ‚ç‚¹
            if guides:
                node['guides'] = guides
            if troubleshooting:
                node['troubleshooting'] = troubleshooting

            # åªæœ‰çœŸæ­£çš„å­ç±»åˆ«æ‰ä¿ç•™åœ¨childrenä¸­
            if real_children:
                node['children'] = real_children
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„å­ç±»åˆ«ï¼Œç§»é™¤childrenå­—æ®µ
                if 'children' in node:
                    del node['children']

        return node

    def fix_node_data(self, node, soup=None):
        """
        ä¿®å¤èŠ‚ç‚¹çš„nameã€titleå’Œview_statisticsæ•°æ®
        """
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼ˆDeviceé¡µé¢ï¼‰
        is_root_device = url == f"{self.base_url}/Device"

        # ä¿®å¤nameå­—æ®µ - ä»URLæå–çœŸå®æ ‡è¯†ç¬¦
        real_name = self.extract_real_name_from_url(url)
        if real_name:
            node['name'] = real_name

        # è·å–é¡µé¢å†…å®¹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        if not soup and url:
            soup = self.get_soup(url)

        # åªå¯¹éæ ¹ç›®å½•é¡µé¢æ·»åŠ titleå­—æ®µ
        if soup and not is_root_device:
            real_title = self.extract_real_title_from_page(soup)
            if real_title:
                node['title'] = real_title

            # ä¿®å¤view_statistics - ä»é¡µé¢æå–çœŸå®ç»Ÿè®¡æ•°æ®
            real_stats = self.extract_real_view_statistics(soup, url)
            if real_stats:
                node['view_statistics'] = real_stats

        return node
        
    def crawl_combined_tree(self, start_url, category_name=None):
        """
        æ•´åˆçˆ¬å–ï¼šæ„å»ºæ ‘å½¢ç»“æ„å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {start_url}")
        print("=" * 60)

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ tree_crawler é€»è¾‘æ„å»ºåŸºç¡€æ ‘ç»“æ„
        print("1. æ„å»ºåŸºç¡€æ ‘å½¢ç»“æ„...")
        base_tree = self.tree_crawler.crawl_tree(start_url, category_name)

        if not base_tree:
            print("æ— æ³•æ„å»ºåŸºç¡€æ ‘ç»“æ„")
            return None

        print(f"åŸºç¡€æ ‘ç»“æ„æ„å»ºå®Œæˆï¼Œå¼€å§‹æå–è¯¦ç»†å†…å®¹...")
        print("=" * 60)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        print("2. ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹...")
        enriched_tree = self.enrich_tree_with_detailed_content(base_tree)

        # ç¬¬ä¸‰æ­¥ï¼šå¯¹äºäº§å“é¡µé¢ï¼Œæ·±å…¥çˆ¬å–å…¶æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
        print("3. æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„å­å†…å®¹...")
        final_tree = self.deep_crawl_product_content(enriched_tree)

        return final_tree
        
    def enrich_tree_with_detailed_content(self, node):
        """
        é€’å½’åœ°ä¸ºæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹ï¼Œå¹¶ä¿®å¤æ•°æ®é—®é¢˜
        """
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')
        if not url or url in self.processed_nodes:
            return node

        self.processed_nodes.add(url)

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼ˆDeviceé¡µé¢ï¼‰
        is_root_device = url == f"{self.base_url}/Device"

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        time.sleep(random.uniform(0.5, 1.0))

        # è·å–é¡µé¢å†…å®¹
        soup = self.get_soup(url)

        # ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®ï¼ˆnameã€titleã€view_statisticsï¼‰
        node = self.fix_node_data(node, soup)

        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æå–ç›¸åº”çš„è¯¦ç»†å†…å®¹
        node_type = self.get_node_type(url)

        if self.verbose:
            print(f"å¤„ç†èŠ‚ç‚¹: {node.get('name', '')} ({node_type}) - {url}")
        else:
            print(f"å¤„ç†: {node.get('name', '')} ({node_type})")

        # æ ¹æ®èŠ‚ç‚¹ç±»å‹æå–è¯¦ç»†å†…å®¹
        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æå–è¯¦ç»†å†…å®¹
            if self.should_extract_detailed_content(url, node_type):

                if node_type == 'guide':
                    detailed_content = self.extract_guide_content(url)
                    if detailed_content:
                        node = self.merge_detailed_content_to_node(node, detailed_content, 'guide')
                        steps_count = len(detailed_content.get('steps', []))
                        videos_count = len(detailed_content.get('videos', []))
                        print(f"  âœ“ æŒ‡å—å†…å®¹: {steps_count} æ­¥éª¤, {videos_count} è§†é¢‘")

                elif node_type == 'troubleshooting':
                    detailed_content = self.extract_troubleshooting_content(url)
                    if detailed_content:
                        node = self.merge_detailed_content_to_node(node, detailed_content, 'troubleshooting')
                        causes_count = len(detailed_content.get('causes', []))
                        print(f"  âœ“ æ•…éšœæ’é™¤å†…å®¹: {causes_count} åŸå› ")

                elif node_type == 'product':
                    # å¯¹äºäº§å“èŠ‚ç‚¹ï¼Œæå–åŸºæœ¬äº§å“ä¿¡æ¯ï¼Œä½†æ’é™¤æ ¹ç›®å½•
                    if soup and not is_root_device:
                        product_info = self.extract_product_info(soup, url, [])
                        if product_info and product_info.get('product_name'):
                            node['product_name'] = product_info['product_name']
                            if product_info.get('instruction_url'):
                                node['instruction_url'] = product_info['instruction_url']
                            print(f"  âœ“ äº§å“ä¿¡æ¯: {product_info['product_name']}")
                    elif is_root_device:
                        print(f"  â†’ æ ¹ç›®å½•èŠ‚ç‚¹ï¼Œè·³è¿‡äº§å“ä¿¡æ¯æå–")

            else:
                if node_type == 'category':
                    print(f"  â†’ åˆ†ç±»èŠ‚ç‚¹ï¼Œä¿®å¤åŸºæœ¬æ•°æ®")
                else:
                    print(f"  â†’ è·³è¿‡è¯¦ç»†å†…å®¹æå–")

        except Exception as e:
            print(f"  âœ— æå–å†…å®¹æ—¶å‡ºé”™: {str(e)}")
            if self.verbose:
                import traceback
                traceback.print_exc()

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            enriched_children = []
            for child in node['children']:
                enriched_child = self.enrich_tree_with_detailed_content(child)
                if enriched_child:
                    enriched_children.append(enriched_child)
            node['children'] = enriched_children

        return node
        
    def get_node_type(self, url):
        """
        åˆ¤æ–­èŠ‚ç‚¹ç±»å‹ï¼šguide, troubleshooting, product, category
        """
        if '/Guide/' in url:
            return 'guide'
        elif '/Troubleshooting/' in url:
            return 'troubleshooting'
        else:
            # éœ€è¦è·å–é¡µé¢å†…å®¹æ¥è¿›ä¸€æ­¥åˆ¤æ–­
            soup = self.get_soup(url)
            if soup:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•…éšœæ’é™¤é¡µé¢
                if self.is_troubleshooting_page(soup):
                    return 'troubleshooting'
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆäº§å“é¡µé¢
                elif self.is_final_product_page(soup, url):
                    return 'product'
            
        return 'category'
        
    def is_troubleshooting_page(self, soup):
        """
        æ£€æŸ¥é¡µé¢æ˜¯å¦æ˜¯æ•…éšœæ’é™¤é¡µé¢
        """
        if not soup:
            return False

        # æ£€æŸ¥é¡µé¢æ ‡é¢˜
        title_elem = soup.select_one("h1")
        if title_elem:
            title_text = title_elem.get_text().lower()
            if any(keyword in title_text for keyword in ['troubleshooting', 'æ•…éšœæ’é™¤', 'problems', 'issues']):
                return True

        # æ£€æŸ¥é¡µé¢å†…å®¹ä¸­æ˜¯å¦åŒ…å«æ•…éšœæ’é™¤ç›¸å…³çš„ç»“æ„
        troubleshooting_indicators = [
            '.troubleshooting-section',
            '.problem-section',
            '.issue-section',
            '[href*="#Section_"]'  # æ•…éšœæ’é™¤é¡µé¢é€šå¸¸æœ‰è¿™ç§é”šç‚¹é“¾æ¥
        ]

        for indicator in troubleshooting_indicators:
            if soup.select(indicator):
                return True

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–å·çš„åŸå› é“¾æ¥ï¼ˆæ•…éšœæ’é™¤é¡µé¢çš„ç‰¹å¾ï¼‰
        cause_links = soup.find_all('a', href=lambda x: x and '#Section_' in x)
        if len(cause_links) >= 2:  # è‡³å°‘æœ‰2ä¸ªåŸå› é“¾æ¥
            return True

        return False

    def should_extract_detailed_content(self, url, node_type):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºè¯¥èŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹
        """
        # è·³è¿‡æŸäº›ä¸éœ€è¦è¯¦ç»†å†…å®¹çš„é¡µé¢
        skip_patterns = [
            '/Edit/', '/History/', '?revision', '/Answers/',
            'action=edit', 'action=create', '/new'
        ]

        for pattern in skip_patterns:
            if pattern in url:
                return False

        # å¯¹äºæŒ‡å—å’Œæ•…éšœæ’é™¤é¡µé¢ï¼Œæ€»æ˜¯æå–è¯¦ç»†å†…å®¹
        if node_type in ['guide', 'troubleshooting']:
            return True

        # å¯¹äºäº§å“é¡µé¢ï¼Œå¦‚æœæ˜¯å¶å­èŠ‚ç‚¹åˆ™æå–
        if node_type == 'product':
            return True

        # åˆ†ç±»é¡µé¢ä¸éœ€è¦è¯¦ç»†å†…å®¹
        return False
        
    def merge_detailed_content_to_node(self, node, detailed_content, content_type):
        """
        å°†è¯¦ç»†å†…å®¹åˆå¹¶åˆ°æ ‘èŠ‚ç‚¹ä¸­ï¼Œä¿æŒæ ‘å½¢ç»“æ„
        """
        if not detailed_content:
            return node

        # ä¿ç•™åŸæœ‰çš„æ ‘å½¢ç»“æ„å­—æ®µ
        original_name = node.get('name', '')
        original_url = node.get('url', '')
        original_children = node.get('children', [])

        # åˆå¹¶è¯¦ç»†å†…å®¹ï¼ŒæŒ‰ç…§ enhanced_crawler çš„å­—æ®µé¡ºåº
        if content_type == 'guide':
            # å¯¹äºæŒ‡å—ï¼ŒæŒ‰ç…§ enhanced_crawler çš„å­—æ®µé¡ºåºåˆå¹¶
            ordered_fields = ['title', 'time_required', 'difficulty', 'introduction',
                            'what_you_need', 'view_statistics', 'completed', 'favorites',
                            'videos', 'steps']

            for field in ordered_fields:
                if field in detailed_content and detailed_content[field]:
                    node[field] = detailed_content[field]

            # æ·»åŠ å…¶ä»–å¯èƒ½å­˜åœ¨çš„å­—æ®µ
            for key, value in detailed_content.items():
                if key not in ['url'] + ordered_fields and value:
                    node[key] = value

        elif content_type == 'troubleshooting':
            # å¯¹äºæ•…éšœæ’é™¤ï¼ŒæŒ‰ç…§ enhanced_crawler çš„å­—æ®µé¡ºåºåˆå¹¶
            ordered_fields = ['title', 'view_statistics', 'completed', 'favorites', 'causes']

            for field in ordered_fields:
                if field in detailed_content and detailed_content[field]:
                    node[field] = detailed_content[field]

            # æ·»åŠ åŠ¨æ€æå–çš„å­—æ®µ
            for key, value in detailed_content.items():
                if key not in ['url'] + ordered_fields and value:
                    node[key] = value

        # ç¡®ä¿æ ‘å½¢ç»“æ„å­—æ®µå­˜åœ¨ä¸”æ­£ç¡®ï¼Œå¹¶ä¿æŒæ­£ç¡®çš„å­—æ®µé¡ºåº
        # é‡æ–°æ„å»ºèŠ‚ç‚¹ï¼Œç¡®ä¿å­—æ®µé¡ºåºï¼šname -> url -> title -> å…¶ä»–è¯¦ç»†å†…å®¹å­—æ®µ -> children
        new_node = {}
        new_node['name'] = original_name
        new_node['url'] = original_url

        # ä¼˜å…ˆæ·»åŠ titleå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'title' in node:
            new_node['title'] = node['title']

        # æ·»åŠ å…¶ä»–è¯¦ç»†å†…å®¹å­—æ®µï¼ˆæ’é™¤å·²å¤„ç†çš„å­—æ®µï¼‰
        for key, value in node.items():
            if key not in ['name', 'url', 'title', 'children']:
                new_node[key] = value

        new_node['children'] = original_children

        return new_node

    def count_detailed_nodes(self, node):
        """
        ç»Ÿè®¡æ ‘ä¸­åŒ…å«è¯¦ç»†å†…å®¹çš„èŠ‚ç‚¹æ•°é‡
        """
        count = {'guides': 0, 'troubleshooting': 0, 'products': 0, 'categories': 0}

        if 'title' in node and 'steps' in node:
            count['guides'] += 1
        elif 'causes' in node:
            count['troubleshooting'] += 1
        elif 'product_name' in node:
            count['products'] += 1
        else:
            count['categories'] += 1

        if 'children' in node and node['children']:
            for child in node['children']:
                child_count = self.count_detailed_nodes(child)
                for key in count:
                    count[key] += child_count[key]

        return count

    def deep_crawl_product_content(self, node):
        """
        æ·±å…¥çˆ¬å–äº§å“é¡µé¢çš„æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹ï¼Œå¹¶æ­£ç¡®æ„å»ºæ•°æ®ç»“æ„
        """
        if not node or not isinstance(node, dict):
            return node

        url = node.get('url', '')

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡äº§å“é¡µé¢ï¼ˆå¶å­èŠ‚ç‚¹é¡µé¢ï¼ŒåŒ…å«å®é™…çš„guideså’Œtroubleshootingå†…å®¹ï¼‰
        # ç›®æ ‡é¡µé¢ç‰¹å¾ï¼š
        # 1. æ˜¯Deviceé¡µé¢
        # 2. ä¸æ˜¯ç¼–è¾‘ã€å†å²ç­‰åŠŸèƒ½é¡µé¢
        # 3. æ²¡æœ‰å­ç±»åˆ«ï¼ˆæ˜¯å¶å­èŠ‚ç‚¹ï¼‰æˆ–è€…æ˜¯ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡URL
        is_target_page = (
            '/Device/' in url and
            not any(skip in url for skip in ['/Guide/', '/Troubleshooting/', '/Edit/', '/History/', '/Answers/']) and
            (not node.get('children') or len(node.get('children', [])) == 0 or self._is_specified_target_url(url))
        )

        if is_target_page:
            print(f"æ·±å…¥çˆ¬å–ç›®æ ‡äº§å“é¡µé¢: {node.get('name', '')}")

            # ä½¿ç”¨ enhanced_crawler çš„é€»è¾‘æå–è¯¥é¡µé¢çš„æ‰€æœ‰æŒ‡å—å’Œæ•…éšœæ’é™¤å†…å®¹
            try:
                # è·å–é¡µé¢å†…å®¹
                soup = self.get_soup(url)
                if soup:
                    # ä¿®å¤èŠ‚ç‚¹çš„åŸºæœ¬æ•°æ®
                    node = self.fix_node_data(node, soup)

                    # æå–æŒ‡å—é“¾æ¥
                    guides = self.extract_guides_from_device_page(soup, url)
                    guide_links = [guide["url"] for guide in guides]
                    print(f"  æ‰¾åˆ° {len(guide_links)} ä¸ªæŒ‡å—")

                    # æå–æ•…éšœæ’é™¤é“¾æ¥
                    troubleshooting_links = self.extract_troubleshooting_from_device_page(soup, url)
                    print(f"  æ‰¾åˆ° {len(troubleshooting_links)} ä¸ªæ•…éšœæ’é™¤é¡µé¢")

                    # åˆ›å»ºguideså’Œtroubleshootingæ•°ç»„è€Œä¸æ˜¯children
                    guides_data = []
                    troubleshooting_data = []

                    # æ·»åŠ æŒ‡å—æ•°æ® - çˆ¬å–æ‰€æœ‰æŒ‡å—ï¼Œä¸é™åˆ¶æ•°é‡
                    for guide_link in guide_links:
                        guide_content = self.extract_guide_content(guide_link)
                        if guide_content:
                            # ä¸æ·»åŠ nameå­—æ®µï¼Œåªè®¾ç½®url
                            guide_content['url'] = guide_link
                            guides_data.append(guide_content)
                            print(f"    âœ“ æŒ‡å—: {guide_content.get('title', '')}")

                    # æ·»åŠ æ•…éšœæ’é™¤æ•°æ® - çˆ¬å–æ‰€æœ‰æ•…éšœæ’é™¤ï¼Œä¸é™åˆ¶æ•°é‡
                    for ts_link in troubleshooting_links:
                        # ç¡®ä¿ ts_link æ˜¯å­—ç¬¦ä¸²
                        if isinstance(ts_link, dict):
                            ts_url = ts_link.get('url', '')
                        else:
                            ts_url = ts_link

                        if ts_url:
                            ts_content = self.extract_troubleshooting_content(ts_url)
                            if ts_content:
                                # ä¸æ·»åŠ nameå­—æ®µï¼Œåªè®¾ç½®url
                                ts_content['url'] = ts_url
                                troubleshooting_data.append(ts_content)
                                print(f"    âœ“ æ•…éšœæ’é™¤: {ts_content.get('title', '')}")

                    # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç»“æ„
                    if guides_data:
                        node['guides'] = guides_data
                    if troubleshooting_data:
                        node['troubleshooting'] = troubleshooting_data

                    # ç§»é™¤childrenå­—æ®µï¼Œå› ä¸ºè¿™æ˜¯ç›®æ ‡é¡µé¢
                    if 'children' in node:
                        del node['children']

                    print(f"  æ€»è®¡: {len(guides_data)} ä¸ªæŒ‡å—, {len(troubleshooting_data)} ä¸ªæ•…éšœæ’é™¤")

            except Exception as e:
                print(f"  âœ— æ·±å…¥çˆ¬å–æ—¶å‡ºé”™: {str(e)}")

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹ï¼ˆå¯¹äºéç›®æ ‡é¡µé¢ï¼‰
        elif 'children' in node and node['children']:
            for i, child in enumerate(node['children']):
                node['children'][i] = self.deep_crawl_product_content(child)

        return node
        
    def save_combined_result(self, tree_data, filename=None, target_name=None):
        """
        ä¿å­˜æ•´åˆç»“æœåˆ°JSONæ–‡ä»¶
        """
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)
        
        if not filename:
            if target_name:
                safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                filename = f"results/combined_{safe_name}.json"
            else:
                root_name = tree_data.get("name", "combined")
                if " > " in root_name:
                    root_name = root_name.split(" > ")[-1]
                root_name = root_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                filename = f"results/combined_{root_name}.json"
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(tree_data, f, ensure_ascii=False, indent=2)
            
        print(f"\næ•´åˆç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename
        
    def print_combined_tree_structure(self, node, level=0):
        """
        æ‰“å°æ•´åˆåçš„æ ‘å½¢ç»“æ„ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦
        """
        indent = "  " * level
        name = node.get('name', 'Unknown')
        
        # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹å¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
        if 'title' in node and 'steps' in node:
            # æŒ‡å—èŠ‚ç‚¹
            steps_count = len(node.get('steps', []))
            print(f"{indent}ğŸ“– {name} [æŒ‡å— - {steps_count}æ­¥éª¤]")
        elif 'causes' in node:
            # æ•…éšœæ’é™¤èŠ‚ç‚¹  
            causes_count = len(node.get('causes', []))
            print(f"{indent}ğŸ”§ {name} [æ•…éšœæ’é™¤ - {causes_count}åŸå› ]")
        elif 'product_name' in node:
            # äº§å“èŠ‚ç‚¹
            print(f"{indent}ğŸ“± {name} [äº§å“]")
        elif 'children' in node and node['children']:
            # åˆ†ç±»èŠ‚ç‚¹
            children_count = len(node['children'])
            print(f"{indent}ğŸ“ {name} [åˆ†ç±» - {children_count}å­é¡¹]")
        else:
            # å…¶ä»–èŠ‚ç‚¹
            print(f"{indent}â“ {name} [æœªçŸ¥ç±»å‹]")
            
        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if 'children' in node and node['children']:
            for child in node['children']:
                self.print_combined_tree_structure(child, level + 1)

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python combined_crawler.py [URLæˆ–è®¾å¤‡å]")
    print("ä¾‹å¦‚: python combined_crawler.py https://www.ifixit.com/Device/Television")
    print("      python combined_crawler.py Television")

def main():
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†å‚æ•°
    if len(sys.argv) > 1:
        input_text = sys.argv[1]

        # æ£€æŸ¥å¸®åŠ©å‚æ•°
        if input_text.lower() in ['--help', '-h', 'help']:
            print_usage()
            return

        verbose = len(sys.argv) > 2 and sys.argv[2].lower() in ('verbose', 'debug', 'true', '1')
    else:
        print("=" * 60)
        print("iFixitæ•´åˆçˆ¬è™«å·¥å…· - æ ‘å½¢ç»“æ„ + è¯¦ç»†å†…å®¹")
        print("=" * 60)
        print("\nè¯·è¾“å…¥è¦çˆ¬å–çš„iFixitäº§å“åç§°æˆ–URL:")
        print("ä¾‹å¦‚: 70UK6570PUB            - æŒ‡å®šäº§å“å‹å·")
        print("      https://www.ifixit.com/Device/70UK6570PUB - æŒ‡å®šäº§å“URL")
        print("      Television             - ç”µè§†ç±»åˆ«")
        print("      LG_Television          - å“ç‰Œç”µè§†ç±»åˆ«")

        input_text = input("\n> ").strip()
        verbose = input("\næ˜¯å¦å¼€å¯è¯¦ç»†è¾“å‡º? (y/n): ").strip().lower() == 'y'

    if not input_text:
        print_usage()
        return

    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)

    if url:
        print("\n" + "=" * 60)
        print(f"å¼€å§‹æ•´åˆçˆ¬å–: {name}")
        print("é˜¶æ®µ1: æ„å»ºå®Œæ•´æ ‘å½¢ç»“æ„")
        print("é˜¶æ®µ2: ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–è¯¦ç»†å†…å®¹")
        print("=" * 60)

        # åˆ›å»ºæ•´åˆçˆ¬è™«
        crawler = CombinedIFixitCrawler(verbose=verbose)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œæ•´åˆçˆ¬å–
        try:
            combined_data = crawler.crawl_combined_tree(url, name)

            if combined_data:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = crawler.count_detailed_nodes(combined_data)
                elapsed_time = time.time() - start_time

                # æ˜¾ç¤ºæ•´åˆåçš„æ ‘å½¢ç»“æ„
                print("\næ•´åˆåçš„æ ‘å½¢ç»“æ„:")
                print("=" * 60)
                crawler.print_combined_tree_structure(combined_data)
                print("=" * 60)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\næ•´åˆçˆ¬å–ç»Ÿè®¡:")
                print(f"- æŒ‡å—èŠ‚ç‚¹: {stats['guides']} ä¸ª")
                print(f"- æ•…éšœæ’é™¤èŠ‚ç‚¹: {stats['troubleshooting']} ä¸ª")
                print(f"- äº§å“èŠ‚ç‚¹: {stats['products']} ä¸ª")
                print(f"- åˆ†ç±»èŠ‚ç‚¹: {stats['categories']} ä¸ª")
                print(f"- æ€»è®¡: {sum(stats.values())} ä¸ªèŠ‚ç‚¹")
                print(f"- è€—æ—¶: {elapsed_time:.1f} ç§’")

                # ä¿å­˜ç»“æœ
                filename = crawler.save_combined_result(combined_data, target_name=input_text)

                print(f"\nâœ“ æ•´åˆçˆ¬å–å®Œæˆ!")
                print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

                # æä¾›ä½¿ç”¨å»ºè®®
                print(f"\nä½¿ç”¨å»ºè®®:")
                print(f"- æŸ¥çœ‹JSONæ–‡ä»¶äº†è§£å®Œæ•´çš„æ•°æ®ç»“æ„")
                print(f"- æ ‘å½¢ç»“æ„ä¿æŒäº†ä»æ ¹ç›®å½•åˆ°ç›®æ ‡çš„å®Œæ•´è·¯å¾„")
                print(f"- æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«äº†ç›¸åº”çš„è¯¦ç»†å†…å®¹æ•°æ®")

            else:
                print("\nâœ— æ•´åˆçˆ¬å–å¤±è´¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            print(f"\nâœ— çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            if verbose:
                import traceback
                traceback.print_exc()
    else:
        print_usage()

if __name__ == "__main__":
    main()
