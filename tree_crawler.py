#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ ‘å½¢ç»“æ„çˆ¬è™«å·¥å…·
å°†iFixitç½‘ç«™çš„è®¾å¤‡åˆ†ç±»ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤º
ç”¨æ³•: python tree_crawler.py [URLæˆ–è®¾å¤‡å]
ç¤ºä¾‹: python tree_crawler.py https://www.ifixit.com/Device/Television
      python tree_crawler.py Television
"""

import os
import sys
import json
import time
import random
import re
import logging
from crawler import IFixitCrawler
from tree_building_progress import TreeBuildingProgressManager, TreeBuildingResumeHelper

class TreeCrawler(IFixitCrawler):
    def __init__(self, base_url="https://www.ifixit.com", enable_resume=True, logger=None, verbose=False):
        super().__init__(base_url)
        self.tree_data = {}  # å­˜å‚¨æ ‘å½¢ç»“æ„æ•°æ®
        self.enable_resume = enable_resume
        self.logger = logger or logging.getLogger(__name__)
        self.progress_manager = None
        self.resume_helper = None
        self.verbose = verbose  # æ·»åŠ verboseå±æ€§

    def _extract_command_arg_from_url(self, url):
        """ä»URLä¸­æå–å‘½ä»¤å‚æ•°ç”¨äºç”Ÿæˆå‹å¥½çš„æ–‡ä»¶å"""
        try:
            if '/Device/' in url:
                # æå–Deviceåé¢çš„éƒ¨åˆ†
                device_path = url.split('/Device/')[-1]
                if '?' in device_path:
                    device_path = device_path.split('?')[0]
                device_path = device_path.rstrip('/')

                if device_path:
                    from urllib.parse import unquote
                    device_path = unquote(device_path)
                    # å–æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºå‘½ä»¤å‚æ•°
                    return device_path.split('/')[-1]
            elif '/Guide/' in url:
                # æå–Guideåé¢çš„éƒ¨åˆ†
                guide_path = url.split('/Guide/')[-1]
                if '?' in guide_path:
                    guide_path = guide_path.split('?')[0]
                guide_path = guide_path.rstrip('/')

                if guide_path:
                    from urllib.parse import unquote
                    guide_path = unquote(guide_path)
                    return guide_path.split('/')[-1]

            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨URLçš„æœ€åä¸€ä¸ªè·¯å¾„æ®µ
            from urllib.parse import urlparse, unquote
            parsed = urlparse(url)
            path_parts = parsed.path.strip('/').split('/')
            if path_parts and path_parts[-1]:
                return unquote(path_parts[-1])

            return "default"
        except Exception:
            return "default"
        
    def find_exact_path(self, target_url):
        """
        ä»æ ¹ç›®å½•å¼€å§‹ï¼Œæ‰¾åˆ°åˆ°è¾¾ç›®æ ‡URLçš„ç¡®åˆ‡è·¯å¾„
        ä¼˜å…ˆä½¿ç”¨ç½‘é¡µå†…é¢åŒ…å±‘å¯¼èˆªæå–å®Œæ•´è·¯å¾„
        """
        # ç¡®ä¿ç›®æ ‡URLè¢«æ­£ç¡®ç¼–ç 
        import urllib.parse
        # å…ˆè§£ç å†é‡æ–°ç¼–ç ï¼Œç¡®ä¿ä¸€è‡´æ€§
        decoded_url = urllib.parse.unquote(target_url)
        # é‡æ–°ç¼–ç ï¼Œç¡®ä¿å¼•å·ç­‰ç‰¹æ®Šå­—ç¬¦è¢«æ­£ç¡®å¤„ç†
        target_url = urllib.parse.quote(decoded_url, safe=':/?#[]@!$&\'()*+,;=')

        # å§‹ç»ˆä»è®¾å¤‡æ ¹ç›®å½•å¼€å§‹
        device_url = self.base_url + "/Device"

        # å¦‚æœç›®æ ‡å°±æ˜¯æ ¹ç›®å½•ï¼Œåˆ™ç›´æ¥è¿”å›
        if target_url == device_url:
            return [{"name": "Device", "url": device_url}]
        
        # ä»ç›®æ ‡é¡µé¢æå–é¢åŒ…å±‘å¯¼èˆª
        target_soup = self.get_soup(target_url)
        if not target_soup:
            print(f"æ— æ³•è·å–é¡µé¢å†…å®¹: {target_url}")
            return [{"name": "Device", "url": device_url}]
        
        # å°è¯•ä»é¡µé¢æå–é¢åŒ…å±‘å¯¼èˆª
        breadcrumbs = self.extract_breadcrumbs_from_page(target_soup)
        
        # å¦‚æœæˆåŠŸæå–åˆ°é¢åŒ…å±‘å¯¼èˆª
        if breadcrumbs and len(breadcrumbs) > 1:
            if self.verbose:
                print(f"ä»é¡µé¢æå–åˆ°é¢åŒ…å±‘å¯¼èˆª: {' > '.join(breadcrumbs)}")
            
            # æ„å»ºå®Œæ•´çš„è·¯å¾„
            path = []
            
            # æ˜ å°„å¸¸è§çš„åˆ†ç±»åç§°åˆ°URLè·¯å¾„éƒ¨åˆ†
            category_map = {
                "Device": "Device",
                "Electronics": "Electronics",
                "Television": "Television",
                "Headphone": "Headphone",
                "Phone": "Phone",
                "Tablet": "Tablet",
                "Laptop": "Laptop",
                "Game_Console": "Game_Console",
                "Camera": "Camera"
            }
            
            # é‡æ–°è®¾è®¡URLæ„å»ºé€»è¾‘ï¼šæ¯ä¸ªå±‚çº§éƒ½æ˜¯çœŸå®çš„é¡µé¢URLï¼Œä¸æ˜¯æ‹¼æ¥è·¯å¾„
            # ä»/Deviceé¡µé¢å¼€å§‹ï¼Œé€å±‚æŸ¥æ‰¾çœŸå®çš„URL

            path = []
            current_page_url = f"{self.base_url}/Device"

            # ç¬¬ä¸€çº§ï¼šDeviceé¡µé¢
            path.append({
                "name": breadcrumbs[0],
                "url": current_page_url
            })

            # é€å±‚æŸ¥æ‰¾çœŸå®URL
            for i in range(1, len(breadcrumbs)):
                crumb_name = breadcrumbs[i]

                # å¦‚æœæ˜¯æœ€åä¸€çº§ï¼Œç¡®ä¿ç›®æ ‡URLè¢«æ­£ç¡®ç¼–ç 
                if i == len(breadcrumbs) - 1:
                    # å†æ¬¡ç¡®ä¿URLè¢«æ­£ç¡®ç¼–ç 
                    import urllib.parse
                    decoded_target = urllib.parse.unquote(target_url)
                    encoded_target = urllib.parse.quote(decoded_target, safe=':/?#[]@!$&\'()*+,;=')
                    path.append({
                        "name": crumb_name,
                        "url": encoded_target
                    })
                    break

                # åœ¨å½“å‰é¡µé¢ä¸­æŸ¥æ‰¾åŒ¹é…çš„å­ç±»åˆ«é“¾æ¥
                real_url = self._find_real_url_for_category(current_page_url, crumb_name)

                if real_url:
                    current_page_url = real_url
                    path.append({
                        "name": crumb_name,
                        "url": real_url
                    })
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°çœŸå®URLï¼Œä½¿ç”¨fallbacké€»è¾‘
                    if self.verbose:
                        print(f"è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ° '{crumb_name}' çš„çœŸå®URLï¼Œä½¿ç”¨fallback")
                    fallback_url = self._generate_fallback_url(crumb_name)
                    current_page_url = fallback_url
                    path.append({
                        "name": crumb_name,
                        "url": fallback_url
                    })
            
            # ä¿®å¤URLè·¯å¾„
            fixed_path = []
            for item in path:
                url = item["url"]
                name = item["name"]
                
                # å¤„ç†ä¸€äº›å¸¸è§çš„URLæ˜ å°„é—®é¢˜
                if "ç”µå­äº§å“" in name and not "/Electronics" in url:
                    url = f"{self.base_url}/Device/Electronics"
                elif "è€³æœº" in name and not "/Headphone" in url:
                    url = f"{self.base_url}/Device/Headphone"
                elif "ç”µè§†" in name and not "/Television" in url:
                    url = f"{self.base_url}/Device/Television"
                elif "Apple Headphone" in name:
                    url = f"{self.base_url}/Device/Apple_Headphone"
                
                fixed_path.append({"name": name, "url": url})
            
            return fixed_path
        
        # å¦‚æœé¢åŒ…å±‘æå–å¤±è´¥ï¼Œå°è¯•æ ¹æ®URLç»“æ„æ¨æ–­è·¯å¾„
        print("é¢åŒ…å±‘å¯¼èˆªæå–å¤±è´¥ï¼Œå°è¯•æ ¹æ®URLæ¨æ–­è·¯å¾„")
        
        # ä»ç›®æ ‡URLæ„å»ºè·¯å¾„
        parts = target_url.split("/")
        if len(parts) >= 5 and "Device" in parts:
            device_index = parts.index("Device")
            if device_index < len(parts) - 1:
                # æå–Deviceåé¢çš„æ‰€æœ‰éƒ¨åˆ†
                category_parts = parts[device_index+1:]
                
                # æ„å»ºæ ‡å‡†è·¯å¾„
                path = [{"name": "è®¾å¤‡", "url": device_url}]
                
                # æ„å»ºè·¯å¾„
                current_path = device_url
                for part in category_parts:
                    # æ¨æ–­åˆ†ç±»åç§°
                    if part == "Electronics":
                        name = "ç”µå­äº§å“"
                    elif part == "Television":
                        name = "ç”µè§†"
                    elif part == "Headphone":
                        name = "è€³æœº"
                    else:
                        name = part.replace("_", " ")
                    
                    current_path = f"{current_path}/{part}"
                    path.append({"name": name, "url": current_path})
                
                return path
        
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿçš„çˆ¬å–æ–¹æ³•
        print("URLæ¨æ–­å¤±è´¥ï¼Œä½¿ç”¨çˆ¬å–æ–¹æ³•æŸ¥æ‰¾è·¯å¾„")
        return self._find_path_to_target(device_url, target_url, [{"name": "è®¾å¤‡", "url": device_url}])

    def _find_real_url_for_category(self, current_page_url, category_name):
        """
        åœ¨å½“å‰é¡µé¢ä¸­æŸ¥æ‰¾æŒ‡å®šç±»åˆ«çš„çœŸå®URL
        """
        try:
            soup = self.get_soup(current_page_url)
            if not soup:
                return None

            # æå–å½“å‰é¡µé¢çš„æ‰€æœ‰å­ç±»åˆ«
            categories = self.extract_categories(soup, current_page_url)

            # æ™ºèƒ½åŒ¹é…ç±»åˆ«åç§°
            for category in categories:
                if self._is_category_match(category["name"], category_name):
                    if self.verbose:
                        print(f"æ‰¾åˆ°çœŸå®URL: {category_name} -> {category['url']}")
                    return category["url"]

            if self.verbose:
                print(f"åœ¨é¡µé¢ {current_page_url} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„ç±»åˆ«: {category_name}")
            return None

        except Exception as e:
            print(f"æŸ¥æ‰¾çœŸå®URLæ—¶å‡ºé”™: {str(e)}")
            return None

    def _is_category_match(self, page_link_name, breadcrumb_name):
        """
        æ™ºèƒ½åŒ¹é…é¡µé¢é“¾æ¥åç§°å’Œé¢åŒ…å±‘åç§°
        """
        # æ¸…ç†å’Œæ ‡å‡†åŒ–åç§°
        def normalize_name(name):
            return name.lower().strip().replace(" repair", "").replace(" devices", "")

        page_name = normalize_name(page_link_name)
        crumb_name = normalize_name(breadcrumb_name)

        # ç²¾ç¡®åŒ¹é…
        if page_name == crumb_name:
            return True

        # åŒ…å«åŒ¹é…
        if crumb_name in page_name or page_name in crumb_name:
            return True

        # ç‰¹æ®Šæ˜ å°„
        mappings = {
            "mac": ["mac", "macintosh"],
            "laptop": ["laptop", "notebook"],
            "macbook pro": ["macbook pro", "macbook_pro"],
            "17\"": ["17\"", "17 inch", "17-inch"]
        }

        for key, values in mappings.items():
            if crumb_name in values and any(v in page_name for v in values):
                return True

        return False

    def _generate_fallback_url(self, category_name):
        """
        ç”Ÿæˆfallback URLï¼ˆå½“æ— æ³•æ‰¾åˆ°çœŸå®URLæ—¶ä½¿ç”¨ï¼‰
        """
        # ç®€å•çš„fallbackï¼šä½¿ç”¨Device + ç±»åˆ«åç§°
        clean_name = category_name.replace(" ", "_").replace("\"", "%22")
        return f"{self.base_url}/Device/{clean_name}"

    def extract_breadcrumbs_from_page(self, soup):
        """ä»é¡µé¢æå–é¢åŒ…å±‘å¯¼èˆª"""
        breadcrumbs = []

        try:
            # æ–¹æ³•1: ä»æ–°ç‰ˆChakra UIé¢åŒ…å±‘å¯¼èˆªä¸­æå– (æœ€æ–°çš„æ–¹æ³•)
            chakra_breadcrumb = soup.select_one("nav[aria-label='breadcrumb'].chakra-breadcrumb")
            if chakra_breadcrumb:
                # æå–æ‰€æœ‰é¢åŒ…å±‘é¡¹ç›®
                breadcrumb_items = chakra_breadcrumb.select("li[itemtype='http://schema.org/ListItem']")
                if breadcrumb_items:
                    chakra_breadcrumbs = []
                    for item in breadcrumb_items:
                        # ä»data-nameå±æ€§è·å–åç§°
                        name = item.get("data-name")
                        if name:
                            chakra_breadcrumbs.append(name)
                        else:
                            # å¤‡é€‰ï¼šä»é“¾æ¥æ–‡æœ¬è·å–
                            link = item.select_one("a")
                            if link:
                                text = link.get_text().strip()
                                if text:
                                    # å¼ºåˆ¶è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
                                    text = self._force_english_content(text)
                                    chakra_breadcrumbs.append(text)

                    if chakra_breadcrumbs:
                        # éœ€è¦åè½¬é¡ºåºï¼Œå› ä¸ºé¡µé¢æ˜¾ç¤ºçš„æ˜¯åå‘çš„
                        chakra_breadcrumbs.reverse()
                        # æ·»åŠ "Home"ä½œä¸ºæ ¹èŠ‚ç‚¹
                        if chakra_breadcrumbs and chakra_breadcrumbs[0] != "Home":
                            chakra_breadcrumbs.insert(0, "Home")
                        print(f"ä»Chakra UIé¢åŒ…å±‘æå–åˆ°: {' > '.join(chakra_breadcrumbs)}")
                        return chakra_breadcrumbs

            # æ–¹æ³•2: ä»Reactç»„ä»¶æ•°æ®ä¸­æå–å®Œæ•´é¢åŒ…å±‘ (å¤‡é€‰æ–¹æ³•)
            breadcrumb_component = soup.select_one(".component-NavBreadcrumbs")
            if breadcrumb_component:
                data_props = breadcrumb_component.get("data-props")
                if data_props:
                    # å°è¯•ä»JSONæ•°æ®ä¸­æå–é¢åŒ…å±‘
                    import json
                    import html

                    # è§£ç HTMLå®ä½“
                    decoded_props = html.unescape(data_props)

                    # è§£æJSON
                    try:
                        props_data = json.loads(decoded_props)
                        if "breadcrumbs" in props_data and isinstance(props_data["breadcrumbs"], list):
                            react_breadcrumbs = []
                            for crumb in props_data["breadcrumbs"]:
                                if "name" in crumb and crumb["name"]:
                                    # è¿™é‡Œçš„nameå¯èƒ½æ˜¯ä¸­æ–‡çš„"è®¾å¤‡"ã€"ç”µå­äº§å“"ç­‰ï¼Œéœ€è¦å¼ºåˆ¶è½¬æ¢
                                    name = self._force_english_content(crumb["name"])
                                    react_breadcrumbs.append(name)

                            if react_breadcrumbs:
                                if self.verbose:
                                    print(f"ä»Reactç»„ä»¶æå–åˆ°é¢åŒ…å±‘: {' > '.join(react_breadcrumbs)}")
                                return react_breadcrumbs
                    except json.JSONDecodeError:
                        print("æ— æ³•è§£æé¢åŒ…å±‘JSONæ•°æ®")

            # æ–¹æ³•3: ä»é¡µé¢çš„metaæ ‡ç­¾ä¸­æå–(å¤‡é€‰æ–¹æ³•)
            breadcrumb_items = soup.select("[itemtype='http://schema.org/BreadcrumbList'] [itemtype='http://schema.org/ListItem']")
            if breadcrumb_items:
                schema_breadcrumbs = []
                for item in breadcrumb_items:
                    name_meta = item.select_one("[itemprop='name']")
                    if name_meta and name_meta.get("content"):
                        content = self._force_english_content(name_meta.get("content"))
                        schema_breadcrumbs.append(content)

                if schema_breadcrumbs:
                    print(f"ä»Schema.orgæ ‡è®°æå–åˆ°é¢åŒ…å±‘: {' > '.join(schema_breadcrumbs)}")
                    return schema_breadcrumbs
                
            # æ–¹æ³•3: ç›´æ¥ä»é¢åŒ…å±‘å®¹å™¨ä¸­æå–é“¾æ¥
            breadcrumbs_container = soup.select_one(".breadcrumbs-container")
            if breadcrumbs_container:
                links = breadcrumbs_container.select("a")
                if links:
                    container_breadcrumbs = []
                    for link in links:
                        text = link.get_text().strip()
                        if text:
                            text = self._force_english_content(text)
                            container_breadcrumbs.append(text)
                    
                    if container_breadcrumbs:
                        print(f"ä»é¢åŒ…å±‘å®¹å™¨æå–åˆ°: {' > '.join(container_breadcrumbs)}")
                        return container_breadcrumbs
            
            # æ–¹æ³•4: ä»é¡µé¢å¤´éƒ¨çš„å¯¼èˆªé“¾æ¥ä¸­æå–
            page_header = soup.select_one("#page-header-container")
            if page_header:
                nav_links = page_header.select("a")
                standard_breadcrumbs = []
                for link in nav_links:
                    href = link.get("href", "")
                    text = link.get_text().strip()
                    # åªä¿ç•™æœ‰æ•ˆçš„è®¾å¤‡ç›¸å…³é“¾æ¥ï¼Œè¿‡æ»¤æ‰"åˆ›å»ºæŒ‡å—"ç­‰
                    if ("/Device/" in href and
                        text and
                        not any(invalid in text.lower() for invalid in ["åˆ›å»º", "ç¿»è¯‘", "è´¡çŒ®è€…", "è®ºå›", "guide", "create", "translate", "contributor", "forum"])):
                        text = self._force_english_content(text)
                        standard_breadcrumbs.append(text)
                
                if standard_breadcrumbs:
                    print(f"ä»é¡µé¢é¡¶éƒ¨æå–åˆ°é¢åŒ…å±‘: {' > '.join(standard_breadcrumbs)}")
                    return standard_breadcrumbs
            
            # æ–¹æ³•5: ä»URLç»“æ„æ¨æ–­åŸºæœ¬è·¯å¾„
            # è¿™æ˜¯æœ€åçš„å¤‡é€‰æ–¹æ³•ï¼Œå¦‚æœå‰é¢çš„æ–¹æ³•éƒ½å¤±è´¥äº†
            url_path = soup.select_one("link[rel='canonical']")
            if url_path:
                url = url_path.get("href", "")
                if "Headphone" in url:
                    # è€³æœºç±»åˆ«çš„æ ‡å‡†è·¯å¾„
                    return ["Device", "Electronics", "Headphone", url.split("/")[-1].replace("_", " ")]
                elif "Television" in url:
                    # ç”µè§†ç±»åˆ«çš„æ ‡å‡†è·¯å¾„
                    if "_" in url:
                        brand = url.split("/")[-1].split("_")[0]
                        return ["Device", "Electronics", "Television", f"{brand} Television"]
                    else:
                        return ["Device", "Electronics", "Television"]
                elif "Electronics" in url:
                    return ["Device", "Electronics"]
                
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            print("æ— æ³•æå–é¢åŒ…å±‘å¯¼èˆª")
            return []
                
        except Exception as e:
            print(f"æå–é¢åŒ…å±‘æ—¶å‡ºé”™: {e}")
            
        return breadcrumbs
    
    def _find_path_to_target(self, current_url, target_url, current_path):
        """é€’å½’å‘ä¸‹æŸ¥æ‰¾ç›®æ ‡URLçš„è·¯å¾„"""
        if self.verbose:
            print(f"æ£€æŸ¥è·¯å¾„: {current_url}")
        
        # å…ˆæ£€æŸ¥å½“å‰URLæ˜¯å¦å°±æ˜¯ç›®æ ‡URL
        if current_url == target_url:
            return current_path
            
        # è·å–å½“å‰é¡µé¢å†…å®¹
        soup = self.get_soup(current_url)
        if not soup:
            return None
        
        # æå–å½“å‰é¡µé¢ä¸Šçš„æ‰€æœ‰å­ç±»åˆ«
        categories = self.extract_categories(soup, current_url)
        
        # è¿‡æ»¤æ‰"åˆ›å»ºæŒ‡å—"ç­‰éå®é™…ç±»åˆ«
        real_categories = [c for c in categories if not any(x in c["name"] or x in c["url"] for x in ["åˆ›å»ºæŒ‡å—", "Guide/new"])]
        
        # æ£€æŸ¥ç›®æ ‡URLæ˜¯å¦åœ¨å½“å‰é¡µé¢çš„ç›´æ¥å­ç±»åˆ«ä¸­
        for category in real_categories:
            if category["url"] == target_url:
                # æ‰¾åˆ°äº†ç›®æ ‡ï¼Œæ·»åŠ åˆ°è·¯å¾„å¹¶è¿”å›
                clean_name = category["name"].replace(" Repair", "")
                current_path.append({"name": clean_name, "url": target_url})
                return current_path
        
        # ç‰¹æ®Šå¤„ç†LG_Televisionçš„æƒ…å†µ
        target_url_lower = target_url.lower()
        if "lg_television" in target_url_lower:
            # æ£€æŸ¥æ˜¯å¦åœ¨ç”µè§†ç±»åˆ«ä¸‹
            if "television" in current_url.lower():
                for category in real_categories:
                    if "lg" in category["name"].lower() and "television" in category["name"].lower():
                        # æ‰¾åˆ°äº†LG Televisionç±»åˆ«
                        clean_name = category["name"].replace(" Repair", "")
                        current_path.append({"name": clean_name, "url": target_url})
                        return current_path
        
        # å¦‚æœç›®æ ‡ä¸æ˜¯å½“å‰é¡µé¢çš„ç›´æ¥å­ç±»åˆ«ï¼Œæ£€æŸ¥æ¯ä¸ªå­ç±»åˆ«ä¸‹æ˜¯å¦å­˜åœ¨ç›®æ ‡
        # ä¼˜å…ˆæ£€æŸ¥åç§°æˆ–URLä¸­åŒ…å«ç›®æ ‡å…³é”®å­—çš„ç±»åˆ«
        prioritized_categories = []
        other_categories = []
        
        # ä»ç›®æ ‡URLä¸­æå–å…³é”®å­—
        target_keywords = target_url.split("/")[-1].replace("_", " ").lower().split()
        
        for category in real_categories:
            # é¿å…é‡å¤è®¿é—®å·²åœ¨è·¯å¾„ä¸­çš„URL
            if any(item["url"] == category["url"] for item in current_path):
                continue
                
            # æ£€æŸ¥ç±»åˆ«åç§°æˆ–URLæ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®å­—
            category_name_lower = category["name"].lower()
            category_url_lower = category["url"].lower()
            
            has_keyword = False
            for keyword in target_keywords:
                if len(keyword) > 2 and (keyword in category_name_lower or keyword in category_url_lower):
                    has_keyword = True
                    break
                    
            # æ ¹æ®æ˜¯å¦åŒ…å«å…³é”®å­—åˆ†ç»„
            if has_keyword:
                prioritized_categories.append(category)
            else:
                other_categories.append(category)
        
        # å…ˆæ£€æŸ¥ä¼˜å…ˆçº§é«˜çš„ç±»åˆ«
        for category in prioritized_categories:
            # æ·»åŠ å½“å‰ç±»åˆ«åˆ°ä¸´æ—¶è·¯å¾„
            clean_name = category["name"].replace(" Repair", "")
            temp_path = current_path.copy()
            temp_path.append({"name": clean_name, "url": category["url"]})
            
            # é€’å½’æ£€æŸ¥è¯¥ç±»åˆ«
            result = self._find_path_to_target(category["url"], target_url, temp_path)
            if result:
                return result
        
        # å¦‚æœåœ¨ä¼˜å…ˆçº§é«˜çš„ç±»åˆ«ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œæ£€æŸ¥å…¶ä»–ç±»åˆ«
        for category in other_categories:
            # æ·»åŠ å½“å‰ç±»åˆ«åˆ°ä¸´æ—¶è·¯å¾„
            clean_name = category["name"].replace(" Repair", "")
            temp_path = current_path.copy()
            temp_path.append({"name": clean_name, "url": category["url"]})
            
            # é€’å½’æ£€æŸ¥è¯¥ç±»åˆ«
            result = self._find_path_to_target(category["url"], target_url, temp_path)
            if result:
                return result
        
        # å¦‚æœåœ¨æ‰€æœ‰å­ç±»åˆ«ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡ï¼Œè¿”å›None
        return None
        
    def find_parent_categories(self, url, current_name):
        """æŸ¥æ‰¾ç»™å®šURLçš„æ‰€æœ‰çˆ¶ç±»åˆ«ï¼Œæ„å»ºå®Œæ•´çš„åˆ†ç±»è·¯å¾„"""
        # ä½¿ç”¨æ–°çš„ç²¾ç¡®è·¯å¾„æŸ¥æ‰¾æ–¹æ³•
        exact_path = self.find_exact_path(url)
        if exact_path:
            print(f"æ‰¾åˆ°ç²¾ç¡®è·¯å¾„: {' > '.join([c['name'] for c in exact_path])}")
            return exact_path
        
        # å¦‚æœç²¾ç¡®è·¯å¾„æŸ¥æ‰¾å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        print("ç²¾ç¡®è·¯å¾„æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æŸ¥æ‰¾æ–¹æ³•...")
        
        parent_categories = []
        
        # å…ˆæ·»åŠ å½“å‰ç±»åˆ«
        parent_categories.append({"name": current_name, "url": url})
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è®¾å¤‡æ ¹ç›®å½•
        if url == self.base_url + "/Device":
            return parent_categories
        
        # æ·»åŠ è®¾å¤‡æ ¹ç›®å½•ä½œä¸ºæœ€é¡¶å±‚
        if not any(c["url"] == self.base_url + "/Device" for c in parent_categories):
            parent_categories.append({"name": "è®¾å¤‡", "url": self.base_url + "/Device"})
            
        # è¿”å›æ‰¾åˆ°çš„åˆ†ç±»è·¯å¾„ï¼ˆåè½¬é¡ºåºï¼Œä»æ ¹åˆ°å¶ï¼‰
        return list(reversed(parent_categories))
        
    def _force_english_content(self, text):
        """
        å¼ºåˆ¶å°†ä¸­æ–‡å†…å®¹è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
        """
        if not text:
            return text

        # ä¸­æ–‡åˆ°è‹±æ–‡çš„æ˜ å°„è¡¨
        chinese_to_english = {
            'è‹±å¯¸': '"',
            'å¯¸': '"',
            'ä¿®å¤': 'Repair',
            'ç»´ä¿®': 'Repair',
            'æŒ‡å—': 'Guide',
            'æ•…éšœæ’é™¤': 'Troubleshooting',
            'é—®é¢˜': 'Issues',
            'è§£å†³æ–¹æ¡ˆ': 'Solutions',
            'æ•™ç¨‹': 'Tutorial',
            'æ‹†è§£': 'Teardown',
            'å®‰è£…': 'Installation',
            'æ›´æ¢': 'Replacement',
            'å‹å·': 'Models',
            'ç³»åˆ—': 'Series'
        }

        # æ›¿æ¢ä¸­æ–‡å†…å®¹
        result = text
        for chinese, english in chinese_to_english.items():
            result = result.replace(chinese, english)

        return result

    def _get_page_title(self, soup):
        """ä»é¡µé¢æå–æ›´å‡†ç¡®çš„æ ‡é¢˜ï¼Œå¹¶å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡å†…å®¹"""
        # å°è¯•ä»æ ‡é¢˜æ ‡ç­¾è·å–
        title_elem = soup.select_one("div.device-title h1, h1.device-title, h1.title, h1")
        if title_elem:
            title = title_elem.get_text().strip()
            # ç§»é™¤"Repair"ç­‰åç¼€
            title = title.replace(" Repair", "").replace(" repair", "")
            # å¼ºåˆ¶è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
            title = self._force_english_content(title)
            return title

        # å°è¯•ä»é¢åŒ…å±‘å¯¼èˆªè·å–
        breadcrumbs = self.extract_breadcrumbs(soup)
        if breadcrumbs and len(breadcrumbs) > 0:
            title = breadcrumbs[-1]
            # å¼ºåˆ¶è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
            title = self._force_english_content(title)
            return title

        # å°è¯•ä»é¡µé¢æ ‡é¢˜è·å–
        if soup.title:
            title = soup.title.string
            # ç§»é™¤ç½‘ç«™åç§°å’Œ"Repair"ç­‰åç¼€
            title = title.replace(" - iFixit", "").replace(" - iFixit.cn", "")
            title = title.replace(" Repair", "").replace(" repair", "")
            # å¼ºåˆ¶è½¬æ¢ä¸ºè‹±æ–‡å†…å®¹
            title = self._force_english_content(title)
            return title

        return None

    def crawl_tree(self, start_url=None, category_name=None):
        """ä»¥æ ‘å½¢ç»“æ„çˆ¬å–è®¾å¤‡åˆ†ç±» - æ”¯æŒæ–­ç‚¹ç»­çˆ¬"""
        if not start_url:
            # ä»è®¾å¤‡é¡µé¢å¼€å§‹
            start_url = self.base_url + "/Device"
            category_name = "è®¾å¤‡"

        # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        if self.enable_resume:
            # ä»start_urlæå–å‘½ä»¤å‚æ•°ç”¨äºç”Ÿæˆå‹å¥½çš„æ–‡ä»¶å
            command_arg = self._extract_command_arg_from_url(start_url)
            self.progress_manager = TreeBuildingProgressManager(start_url, logger=self.logger, command_arg=command_arg)
            self.resume_helper = TreeBuildingResumeHelper(self.progress_manager, self.logger)

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
            if self.resume_helper.can_resume():
                print(f"ğŸ”„ æ£€æµ‹åˆ°æœªå®Œæˆçš„æ ‘æ„å»ºä»»åŠ¡ï¼Œå‡†å¤‡ä»æ–­ç‚¹æ¢å¤...")
                self.progress_manager.display_progress_report()

                # è·å–æ¢å¤æ•°æ®
                resume_data = self.resume_helper.prepare_resume_data()
                if resume_data:
                    # æ¢å¤å·²è®¿é—®çš„URL
                    self.visited_urls.update(resume_data.get('visited_urls', set()))

                    # æ¢å¤æ ‘å½¢ç»“æ„
                    existing_tree = resume_data.get('tree_structure')
                    if existing_tree:
                        print(f"ğŸ“‹ æ¢å¤å·²æ„å»ºçš„æ ‘å½¢ç»“æ„...")
                        # ä»ç°æœ‰æ ‘å½¢ç»“æ„ç»§ç»­æ„å»º
                        return self._resume_tree_building(existing_tree, start_url, resume_data)

            # å¼€å§‹æ–°çš„æ„å»ºä¼šè¯
            self.progress_manager.start_session()

        print(f"ğŸŒ³ å¼€å§‹çˆ¬å–: {start_url}")

        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å“ç‰Œç”µè§†é¡µé¢(å¦‚TCL_Television, LG_Televisionç­‰)
            brand_match = re.search(r'([A-Za-z]+)_Television', start_url)
            if brand_match:
                brand_name = brand_match.group(1)
                print(f"æ£€æµ‹åˆ° {brand_name} Television é¡µé¢ï¼Œæ„å»ºæ ‡å‡†è·¯å¾„")

                # ç›´æ¥æ„å»ºæ ‡å‡†å››çº§è·¯å¾„: è®¾å¤‡ > ç”µå­äº§å“ > ç”µè§† > å“ç‰Œç”µè§†
                device_url = self.base_url + "/Device"
                electronics_url = self.base_url + "/Device/Electronics"
                tv_url = self.base_url + "/Device/Television"

                full_path = [
                    {"name": "è®¾å¤‡", "url": device_url},
                    {"name": "ç”µå­äº§å“", "url": electronics_url},
                    {"name": "ç”µè§†", "url": tv_url},
                    {"name": f"{brand_name} Television", "url": start_url}
                ]

                print(f"æ ‡å‡†è·¯å¾„æ„å»ºå®Œæˆ: {' > '.join([c['name'] for c in full_path])}")

                # æ„å»ºæ ‘å½¢ç»“æ„
                tree = self._build_tree_from_path(full_path)

                # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹
                target_node = self._find_node_by_url(tree, start_url)
                if not target_node:
                    target_node = tree

                # ä»ç›®æ ‡èŠ‚ç‚¹å¼€å§‹çˆ¬å–å­ç±»åˆ«
                print(f"å¼€å§‹ä» {target_node['url']} çˆ¬å–å­ç±»åˆ«")
                self._crawl_recursive_tree(target_node["url"], target_node)

                # å®Œæˆæ„å»ºä¼šè¯
                if self.enable_resume and self.progress_manager:
                    self.progress_manager.save_tree_structure(tree)
                    self.progress_manager.complete_session()
                    print(f"âœ… æ ‘æ„å»ºå®Œæˆï¼Œå·²å¤„ç† {len(self.visited_urls)} ä¸ªURL")

                return tree

            # å¯¹äºå…¶ä»–URLï¼Œå°è¯•æ„å»ºä»æ ¹ç›®å½•åˆ°ç›®æ ‡URLçš„ç²¾ç¡®è·¯å¾„
            full_path = self.find_exact_path(start_url)
            if not full_path or len(full_path) < 2:
                print(f"æ— æ³•æ‰¾åˆ°ä»æ ¹ç›®å½•åˆ° {start_url} çš„è·¯å¾„ï¼Œå°è¯•ä»é¡µé¢å…ƒç´ ä¸­æå–æ›´å¤šä¿¡æ¯")

                # å°è¯•è·å–é¡µé¢å†…å®¹ä»¥æ£€æŸ¥é¡µé¢ç»“æ„
                target_soup = self.get_soup(start_url)
                if target_soup:
                    # ä»é¡µé¢æ ‡é¢˜æˆ–é¢åŒ…å±‘ä¸­æ¨æ–­åç§°
                    page_title = self._get_page_title(target_soup)

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”µè§†ç›¸å…³å†…å®¹
                    if "Television" in start_url or "TV" in start_url or "ç”µè§†" in page_title:
                        print("æ£€æµ‹åˆ°ç”µè§†ç›¸å…³é¡µé¢ï¼ŒæŸ¥æ‰¾ç”µè§†ç±»åˆ«è·¯å¾„")
                        # å°è¯•æ„å»ºç”µè§†ç›¸å…³è·¯å¾„
                        tv_path = self.find_tv_path(start_url, category_name)
                        if tv_path:
                            full_path = tv_path

                # å¦‚æœä»ç„¶æ— æ³•æ‰¾åˆ°è·¯å¾„ï¼Œä½¿ç”¨åŸºæœ¬è·¯å¾„
                if not full_path or len(full_path) < 2:
                    print(f"æ— æ³•æ‰¾åˆ°å®Œæ•´è·¯å¾„ï¼Œä½¿ç”¨åŸºæœ¬è·¯å¾„")
                    full_path = [{"name": "è®¾å¤‡", "url": self.base_url + "/Device"}]
                    if start_url != self.base_url + "/Device":
                        full_path.append({"name": category_name or start_url.split("/")[-1].replace("_", " "), "url": start_url})

            print(f"å®Œæ•´è·¯å¾„: {' > '.join([c['name'] for c in full_path])}")

            # 2. æ„å»ºæ ‘å½¢ç»“æ„
            tree = self._build_tree_from_path(full_path)

            # 3. æ ¹æ®å®Œæ•´è·¯å¾„æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹
            target_node = self._find_node_by_url(tree, start_url)
            if not target_node:
                print("æ— æ³•åœ¨æ ‘ä¸­æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹ï¼Œä½¿ç”¨æ ¹èŠ‚ç‚¹")
                target_node = tree

            # 4. åªå¯¹ç›®æ ‡èŠ‚ç‚¹è¿›è¡Œå†…å®¹çˆ¬å–ï¼Œä¿ç•™å®Œæ•´è·¯å¾„ç»“æ„
            print(f"å¼€å§‹ä» {target_node['url']} çˆ¬å–å­ç±»åˆ«")

            # æ£€æŸ¥ç›®æ ‡èŠ‚ç‚¹æ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹ï¼ˆæœ€ç»ˆäº§å“é¡µé¢ï¼‰
            soup = self.get_soup(target_node["url"])
            if soup:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ç»ˆäº§å“é¡µé¢
                is_final_page = self.is_final_product_page(soup, target_node["url"])

                if is_final_page:
                    # å¦‚æœæ˜¯æœ€ç»ˆäº§å“é¡µé¢ï¼Œæå–äº§å“ä¿¡æ¯ä½†ä¿ç•™è·¯å¾„ç»“æ„
                    product_info = self.extract_product_info(soup, target_node["url"], [])
                    if product_info["product_name"]:
                        target_node["name"] = product_info["product_name"]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼Œä¸ç»™æ ¹ç›®å½•æ·»åŠ instruction_url
                        target_url = target_node.get("url", "")
                        is_root_device = target_url == f"{self.base_url}/Device"
                        if not is_root_device:
                            target_node["instruction_url"] = product_info["instruction_url"]
                        print(f"å·²æ‰¾åˆ°äº§å“: {product_info['product_name']}")
                else:
                    # å¦‚æœä¸æ˜¯æœ€ç»ˆäº§å“é¡µé¢ï¼Œè¿›è¡Œæ­£å¸¸çš„å­ç±»åˆ«çˆ¬å–
                    self._crawl_recursive_tree(target_node["url"], target_node)

        finally:
            # æ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ï¼Œéƒ½è¦ä¿å­˜è¿›åº¦
            if self.enable_resume and self.progress_manager:
                if 'tree' in locals():
                    self.progress_manager.save_tree_structure(tree)
                self.progress_manager.complete_session()
                print(f"âœ… æ ‘æ„å»ºå®Œæˆï¼Œå·²å¤„ç† {len(self.visited_urls)} ä¸ªURL")

        return tree

    def _resume_tree_building(self, existing_tree, start_url, resume_data):
        """ä»æ–­ç‚¹æ¢å¤æ ‘æ„å»º"""
        try:
            print(f"ğŸ”„ ä»æ–­ç‚¹æ¢å¤æ ‘æ„å»º...")

            # è·å–æ¢å¤ç­–ç•¥
            strategy = resume_data.get('strategy', 'continue_normal')
            print(f"ğŸ“‹ æ¢å¤ç­–ç•¥: {strategy}")

            if strategy == "retry_failed_first":
                # ä¼˜å…ˆé‡è¯•å¤±è´¥çš„URL
                failed_urls = self.progress_manager.get_failed_urls_for_retry()
                if failed_urls:
                    print(f"ğŸ”„ é‡è¯• {len(failed_urls)} ä¸ªå¤±è´¥çš„URL...")
                    for failed_url in failed_urls:
                        self.progress_manager.clear_failed_url(failed_url)
                        self.visited_urls.discard(failed_url)

            # ä»ç°æœ‰æ ‘å½¢ç»“æ„ç»§ç»­æ„å»º
            tree = existing_tree

            # æ‰¾åˆ°éœ€è¦ç»§ç»­å¤„ç†çš„èŠ‚ç‚¹
            current_processing = resume_data.get('current_processing')
            if current_processing:
                current_url = current_processing['url']
                print(f"ğŸ¯ ç»§ç»­å¤„ç†ä¸­æ–­çš„URL: {current_url}")

                # æ‰¾åˆ°å¯¹åº”çš„æ ‘èŠ‚ç‚¹
                target_node = self._find_node_by_url(tree, current_url)
                if target_node:
                    # ç»§ç»­ä»è¯¥èŠ‚ç‚¹çˆ¬å–
                    self._crawl_recursive_tree_with_resume(current_url, target_node)
                else:
                    print(f"âš ï¸ æ— æ³•æ‰¾åˆ°ä¸­æ–­çš„èŠ‚ç‚¹ï¼Œä»æ ¹èŠ‚ç‚¹ç»§ç»­")
                    target_node = self._find_node_by_url(tree, start_url)
                    if target_node:
                        self._crawl_recursive_tree_with_resume(start_url, target_node)
            else:
                # æ²¡æœ‰ä¸­æ–­çš„å¤„ç†ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„èŠ‚ç‚¹
                self._continue_incomplete_nodes(tree)

            # å®Œæˆæ„å»ºä¼šè¯
            if self.progress_manager:
                self.progress_manager.save_tree_structure(tree)
                self.progress_manager.complete_session()
                print(f"âœ… æ¢å¤çš„æ ‘æ„å»ºå®Œæˆ")

            return tree

        except Exception as e:
            self.logger.error(f"æ¢å¤æ ‘æ„å»ºå¤±è´¥: {e}")
            if self.progress_manager:
                self.progress_manager.fail_session(str(e))
            raise

    def _continue_incomplete_nodes(self, tree):
        """ç»§ç»­å¤„ç†æœªå®Œæˆçš„èŠ‚ç‚¹"""
        def check_node(node):
            url = node.get('url', '')

            # å¦‚æœèŠ‚ç‚¹æœªè¢«å¤„ç†è¿‡ï¼Œç»§ç»­å¤„ç†
            if url and not self.progress_manager.is_url_processed(url):
                print(f"ğŸ”„ ç»§ç»­å¤„ç†æœªå®Œæˆçš„èŠ‚ç‚¹: {url}")
                self._crawl_recursive_tree_with_resume(url, node)

            # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
            if 'children' in node:
                for child in node['children']:
                    check_node(child)

        check_node(tree)

    def _continue_from_saved_tree_node(self, url, parent_node):
        """ä»å·²ä¿å­˜çš„æ ‘ç»“æ„ä¸­æ¢å¤å­èŠ‚ç‚¹å¹¶ç»§ç»­éå†æœªå¤„ç†çš„éƒ¨åˆ†"""
        try:
            # è·å–å·²ä¿å­˜çš„æ ‘ç»“æ„
            saved_tree = self.progress_manager.get_tree_structure()
            if not saved_tree:
                print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ ‘ç»“æ„ï¼Œè·³è¿‡: {url}")
                return

            # åœ¨å·²ä¿å­˜çš„æ ‘ä¸­æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
            saved_node = self._find_node_by_url(saved_tree, url)
            if not saved_node:
                print(f"âš ï¸ åœ¨å·²ä¿å­˜çš„æ ‘ä¸­æœªæ‰¾åˆ°èŠ‚ç‚¹: {url}")
                return

            # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„èŠ‚ç‚¹ï¼Œæ¢å¤å…¶å­èŠ‚ç‚¹åˆ°å½“å‰èŠ‚ç‚¹
            if 'children' in saved_node and saved_node['children']:
                print(f"ğŸ“‹ ä»å·²ä¿å­˜çš„æ ‘ä¸­æ¢å¤ {len(saved_node['children'])} ä¸ªå­èŠ‚ç‚¹: {url}")

                # å°†å·²ä¿å­˜çš„å­èŠ‚ç‚¹å¤åˆ¶åˆ°å½“å‰èŠ‚ç‚¹
                if 'children' not in parent_node:
                    parent_node['children'] = []

                # åˆå¹¶å­èŠ‚ç‚¹ï¼ˆé¿å…é‡å¤ï¼‰
                existing_urls = {child.get('url', '') for child in parent_node['children']}
                for saved_child in saved_node['children']:
                    child_url = saved_child.get('url', '')
                    if child_url and child_url not in existing_urls:
                        parent_node['children'].append(saved_child.copy())
                        existing_urls.add(child_url)

                # ç»§ç»­éå†æœªå¤„ç†çš„å­èŠ‚ç‚¹
                for child in parent_node['children']:
                    child_url = child.get('url', '')
                    if child_url and not self.progress_manager.is_url_processed(child_url):
                        print(f"ğŸ”„ ç»§ç»­å¤„ç†æœªå®Œæˆçš„å­èŠ‚ç‚¹: {child_url}")
                        self._crawl_recursive_tree_with_resume(child_url, child)
                    elif child_url and self.progress_manager.is_url_processed(child_url):
                        # å³ä½¿å­èŠ‚ç‚¹å·²å¤„ç†ï¼Œä¹Ÿè¦æ£€æŸ¥å…¶å­å­èŠ‚ç‚¹
                        self._continue_from_saved_tree_node(child_url, child)
            else:
                print(f"ğŸ“‹ èŠ‚ç‚¹ {url} æ²¡æœ‰å­èŠ‚ç‚¹éœ€è¦æ¢å¤")

        except Exception as e:
            self.logger.error(f"ä»å·²ä¿å­˜æ ‘ç»“æ„æ¢å¤èŠ‚ç‚¹å¤±è´¥ {url}: {e}")
            print(f"âŒ æ¢å¤èŠ‚ç‚¹å¤±è´¥: {url} - {e}")

    def find_tv_path(self, target_url, category_name):
        """
        å°è¯•æ„å»ºç”µè§†ç±»åˆ«çš„è·¯å¾„
        é¦–å…ˆæŸ¥æ‰¾Electronicsé¡µé¢ï¼Œç„¶åæŸ¥æ‰¾Televisioné¡µé¢
        """
        path = []
        
        # è®¾å¤‡æ ¹ç›®å½•
        device_url = self.base_url + "/Device"
        path.append({"name": "è®¾å¤‡", "url": device_url})
        
        # ç”µå­äº§å“ç±»åˆ«
        electronics_url = self.base_url + "/Device/Electronics"
        electronics_soup = self.get_soup(electronics_url)
        if electronics_soup:
            path.append({"name": "ç”µå­äº§å“", "url": electronics_url})
            
            # æŸ¥æ‰¾ç”µè§†ç±»åˆ«
            categories = self.extract_categories(electronics_soup, electronics_url)
            for category in categories:
                if "Television" in category["url"] or "ç”µè§†" in category["name"]:
                    # æ·»åŠ ç”µè§†ç±»åˆ«
                    tv_url = category["url"]
                    path.append({"name": "ç”µè§†", "url": tv_url})
                    
                    # åœ¨ç”µè§†ç±»åˆ«é¡µé¢ä¸­æŸ¥æ‰¾ç›®æ ‡ç±»åˆ«
                    tv_soup = self.get_soup(tv_url)
                    if tv_soup:
                        tv_categories = self.extract_categories(tv_soup, tv_url)
                        for tv_cat in tv_categories:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦æ‰¾çš„ç›®æ ‡ç±»åˆ«
                            if tv_cat["url"] == target_url:
                                path.append({"name": tv_cat["name"], "url": tv_cat["url"]})
                                return path
                                
                            # å¦‚æœç›®æ ‡ç±»åˆ«åœ¨ä¸‹ä¸€çº§ï¼Œç»§ç»­æŸ¥æ‰¾
                            if category_name in tv_cat["name"] or category_name in tv_cat["url"]:
                                path.append({"name": tv_cat["name"], "url": tv_cat["url"]})
                                
                                # æŸ¥æ‰¾ä¸‹ä¸€çº§
                                sub_soup = self.get_soup(tv_cat["url"])
                                if sub_soup:
                                    sub_categories = self.extract_categories(sub_soup, tv_cat["url"])
                                    for sub_cat in sub_categories:
                                        if sub_cat["url"] == target_url:
                                            path.append({"name": sub_cat["name"], "url": sub_cat["url"]})
                                            return path
                                
                                # å³ä½¿æ²¡æœ‰æ‰¾åˆ°ç¡®åˆ‡åŒ¹é…ï¼Œä¹Ÿè¿”å›å½“å‰è·¯å¾„
                                return path
        
        return None
        
    def _build_tree_from_path(self, path):
        """æ ¹æ®è·¯å¾„æ„å»ºæ ‘ç»“æ„"""
        if not path:
            return None
            
                    # æ ¹èŠ‚ç‚¹
        root_node = {
            "name": path[0]["name"],
            "url": path[0]["url"],
            "children": []
        }
        
        current_node = root_node
        
        # æ„å»ºè·¯å¾„ä¸Šçš„æ¯ä¸ªèŠ‚ç‚¹
        for i in range(1, len(path)):
            new_node = {
                "name": path[i]["name"],
                "url": path[i]["url"],
                "children": []
            }
            
            # æ·»åŠ ä¸ºå½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
            current_node["children"].append(new_node)
            
            # æ›´æ–°å½“å‰èŠ‚ç‚¹
            current_node = new_node
            
        return root_node
        
    def _find_node_by_url(self, node, target_url):
        """åœ¨æ ‘ä¸­æŸ¥æ‰¾æŒ‡å®šURLçš„èŠ‚ç‚¹"""
        if node["url"] == target_url:
            return node
            
        if "children" in node:
            for child in node["children"]:
                result = self._find_node_by_url(child, target_url)
                if result:
                    return result
                    
        return None
        
    def _crawl_recursive_tree(self, url, parent_node):
        """é€’å½’çˆ¬å–æ ‘å½¢ç»“æ„"""
        return self._crawl_recursive_tree_with_resume(url, parent_node)

    def _crawl_recursive_tree_with_resume(self, url, parent_node):
        """é€’å½’çˆ¬å–æ ‘å½¢ç»“æ„ - æ”¯æŒæ–­ç‚¹ç»­çˆ¬"""
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤URLçš„å¤„ç†ï¼Œä½†ä»éœ€è¦éå†å…¶å­èŠ‚ç‚¹
        if self.enable_resume and self.progress_manager:
            if self.progress_manager.should_skip_url_processing_only(url):
                print(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„URLï¼Œä½†ç»§ç»­éå†å­èŠ‚ç‚¹: {url}")
                # ä»å·²ä¿å­˜çš„æ ‘ç»“æ„ä¸­æ¢å¤å­èŠ‚ç‚¹å¹¶ç»§ç»­éå†
                self._continue_from_saved_tree_node(url, parent_node)
                return
            elif self.progress_manager.is_url_failed(url):
                print(f"âš ï¸ è·³è¿‡å¤±è´¥çš„URL: {url}")
                return

        # é˜²æ­¢é‡å¤è®¿é—®åŒä¸€URL
        if url in self.visited_urls:
            return

        # è·³è¿‡ä¸åº”åŒ…å«åœ¨æ ‘ç»“æ„ä¸­çš„é¡µé¢ç±»å‹
        invalid_keywords = ["åˆ›å»ºæŒ‡å—", "Guide/new", "ç¿»è¯‘", "è´¡çŒ®è€…", "è®ºå›é—®é¢˜", "å…¶ä»–è´¡çŒ®"]
        if any(keyword in url for keyword in invalid_keywords):
            print(f"è·³è¿‡æ— æ•ˆé¡µé¢: {url}")
            return

        # æ ‡è®°å¼€å§‹å¤„ç†
        if self.enable_resume and self.progress_manager:
            parent_path = self._get_parent_path_from_tree(parent_node)
            self.progress_manager.mark_url_processing(url, parent_path)

        self.visited_urls.add(url)
        
        # é˜²æ­¢è¿‡å¿«è¯·æ±‚ï¼Œæ·»åŠ å»¶è¿Ÿ
        time.sleep(random.uniform(0.5, 1.0))

        # æ„å»ºå½“å‰ä½ç½®çš„åˆ†ç±»è·¯å¾„æ˜¾ç¤º
        path_str = self._get_current_path(parent_node, self.tree_cache if hasattr(self, 'tree_cache') else None)
        url_segment = url.split('/')[-1]
        print(f"\nğŸŒ³ å¼€å§‹çˆ¬å–èŠ‚ç‚¹:")
        print(f"   è·¯å¾„: {path_str} > {url_segment}")
        print(f"   å®Œæ•´URL: {url}")
        print(f"   çˆ¶èŠ‚ç‚¹: {parent_node.get('name', 'Unknown')}")

        try:
            soup = self.get_soup(url)
            if not soup:
                if self.enable_resume and self.progress_manager:
                    self.progress_manager.mark_url_failed(url, "æ— æ³•è·å–é¡µé¢å†…å®¹")
                return

            # æå–å­ç±»åˆ«
            print(f"   ğŸ” å¼€å§‹æå–å­ç±»åˆ«...")
            categories = self.extract_categories(soup, url)
            print(f"   ğŸ“Š åŸå§‹ç±»åˆ«æ•°é‡: {len(categories)}")

            # è¿‡æ»¤æ‰ä¸åº”åŒ…å«åœ¨æ ‘ç»“æ„ä¸­çš„ç±»åˆ«
            real_categories = []
            filtered_out = []

            for category in categories:
                category_name = category["name"].lower()
                category_url = category["url"].lower()

                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆç±»åˆ«
                is_valid = True
                filter_reason = ""

                # æ£€æŸ¥æ— æ•ˆå…³é”®è¯
                for keyword in invalid_keywords:
                    if keyword.lower() in category_name or keyword.lower() in category_url:
                        is_valid = False
                        filter_reason = f"åŒ…å«æ— æ•ˆå…³é”®è¯: {keyword}"
                        break

                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è®¾å¤‡é“¾æ¥
                if is_valid:
                    if "/Device/" not in category["url"]:
                        is_valid = False
                        filter_reason = "ä¸æ˜¯è®¾å¤‡é“¾æ¥"
                    elif any(x in category["url"] for x in ["/Edit/", "/History/", "?revision", "/Answers/"]):
                        is_valid = False
                        filter_reason = "æ˜¯ç¼–è¾‘/å†å²é¡µé¢"

                if is_valid:
                    real_categories.append(category)
                    print(f"   âœ… æœ‰æ•ˆç±»åˆ«: {category['name']}")
                else:
                    filtered_out.append((category['name'], filter_reason))
                    print(f"   âŒ è¿‡æ»¤ç±»åˆ«: {category['name']} ({filter_reason})")

            print(f"   ğŸ“ˆ è¿‡æ»¤åæœ‰æ•ˆç±»åˆ«æ•°é‡: {len(real_categories)}")
            if filtered_out:
                print(f"   ğŸ—‘ï¸  è¿‡æ»¤æ‰çš„ç±»åˆ«æ•°é‡: {len(filtered_out)}")

            # å¤„ç†å“ç‰Œç”µè§†é¡µé¢ï¼Œå¦‚TCL_Televisionæˆ–LG_Television
            brand_match = re.search(r'([A-Za-z]+)_Television', url)
            if brand_match:
                brand_name = brand_match.group(1)
                print(f"æ£€æµ‹åˆ°{brand_name} Televisioné¡µé¢ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å­ç±»åˆ«")

                # æŸ¥æ‰¾ç±»åˆ«æ•°é‡æç¤ºï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰
                category_count_elements = soup.find_all(string=re.compile(r"\d+\s*(ä¸ªç±»åˆ«|Categories)"))
                for element in category_count_elements:
                    count_text = element.strip()
                    print(f"æ‰¾åˆ°ç±»åˆ«æ•°é‡æç¤º: {count_text}")

                    # æŸ¥æ‰¾åŒ…å«ç±»åˆ«çš„åŒºåŸŸ
                    parent = element.parent
                    if parent:
                        # æŸ¥æ‰¾ä¹‹åçš„divï¼Œé€šå¸¸åŒ…å«ç±»åˆ«åˆ—è¡¨
                        next_section = parent.find_next_sibling()
                        if next_section:
                            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                            links = next_section.find_all("a", href=True)
                            for link in links:
                                href = link.get("href")
                                text = link.text.strip()

                                # æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
                                if (href and text and "/Device/" in href and
                                    not any(invalid in text.lower() or invalid in href.lower()
                                            for invalid in invalid_keywords)):

                                    full_url = self.base_url + href if href.startswith("/") else href

                                    # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç±»åˆ«åˆ—è¡¨ä¸­
                                    if not any(c["url"] == full_url for c in real_categories):
                                        print(f"æ·»åŠ å“ç‰Œç”µè§†å­ç±»åˆ«: {text} - {full_url}")
                                        real_categories.append({
                                            "name": text,
                                            "url": full_url
                                        })

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ç»ˆäº§å“é¡µé¢
            is_final_page = self.is_final_product_page(soup, url)

            # å¦‚æœæ˜¯æœ€ç»ˆäº§å“é¡µé¢ï¼ˆæ²¡æœ‰å­ç±»åˆ«ï¼‰ï¼Œåˆ™æå–äº§å“ä¿¡æ¯
            if is_final_page:
                product_info = self.extract_product_info(soup, url, [])

                # åªæœ‰å½“äº§å“åç§°ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
                if product_info["product_name"]:
                    # è®¾ç½®å½“å‰èŠ‚ç‚¹ä¸ºå¶å­èŠ‚ç‚¹(äº§å“)
                    parent_node["name"] = product_info["product_name"]
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼Œä¸ç»™æ ¹ç›®å½•æ·»åŠ instruction_url
                    is_root_device = url == f"{self.base_url}/Device"
                    if not is_root_device:
                        parent_node["instruction_url"] = product_info["instruction_url"]
                    print(f"âœ… å·²æ‰¾åˆ°å¶å­èŠ‚ç‚¹äº§å“: {path_str} > {product_info['product_name']}")

                # å³ä½¿æ˜¯æœ€ç»ˆäº§å“é¡µé¢ï¼Œä¹Ÿè¦ç¡®ä¿instruction_urlå­—æ®µå­˜åœ¨
                if "instruction_url" not in parent_node:
                    parent_node["instruction_url"] = ""
            else:
                # å¦‚æœæ˜¯æˆ‘ä»¬çš„ç›®æ ‡èŠ‚ç‚¹ï¼ˆå¦‚70UK6570PUBï¼‰ï¼Œå°†å…¶è§†ä¸ºäº§å“å’Œåˆ†ç±»çš„æ··åˆç±»å‹
                if "70UK6570PUB" in url:
                    # å…ˆæå–äº§å“ä¿¡æ¯
                    product_info = self.extract_product_info(soup, url, [])
                    if product_info["product_name"]:
                        # è®¾ç½®å½“å‰èŠ‚ç‚¹ä¸ºäº§å“
                        parent_node["name"] = product_info["product_name"]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼Œä¸ç»™æ ¹ç›®å½•æ·»åŠ instruction_url
                        is_root_device = url == f"{self.base_url}/Device"
                        if not is_root_device:
                            parent_node["instruction_url"] = product_info["instruction_url"]

                        # åŒæ—¶è®©å®ƒä¿æŒcategoryç±»å‹çš„childrenå±æ€§ï¼Œç»§ç»­å‘ä¸‹çˆ¬å–
                        print(f"æ‰¾åˆ°äº§å“ä¸åˆ†ç±»æ··åˆèŠ‚ç‚¹: {path_str} > {product_info['product_name']}")

                # ç¡®ä¿æ‰€æœ‰å¶å­èŠ‚ç‚¹(æ²¡æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹)éƒ½æœ‰instruction_urlå­—æ®µ
                if not is_final_page and "children" in parent_node and (not parent_node["children"] or len(parent_node["children"]) == 0) and "instruction_url" not in parent_node:
                    parent_node["instruction_url"] = ""
                    print(f"æ·»åŠ ç¼ºå¤±çš„instruction_urlå­—æ®µåˆ°å¶å­èŠ‚ç‚¹: {parent_node.get('name', '')}")

                # å¦‚æœæœ‰å­ç±»åˆ«ï¼Œåˆ™ç»§ç»­é€’å½’çˆ¬å–
                if real_categories:
                    # åˆ†ç±»èŠ‚ç‚¹å¤„ç†
                    print(f"ğŸ“‚ ç±»åˆ«é¡µé¢: {path_str}")
                    print(f"ğŸ” æ‰¾åˆ° {len(real_categories)} ä¸ªå­ç±»åˆ«")

                    # è®°å½•æ‰€æœ‰å­ç±»åˆ«åç§°ï¼Œä¾¿äºè°ƒè¯•
                    category_names = [c["name"] for c in real_categories]
                    print(f"   å­ç±»åˆ«åˆ—è¡¨: {', '.join(category_names)}")

                    # æ›´æ–°å‘ç°çš„å­åˆ†ç±»æ•°é‡
                    if self.enable_resume and self.progress_manager:
                        self.progress_manager.update_children_discovered(len(real_categories))

                    # é™åˆ¶å­ç±»åˆ«æ•°é‡ï¼Œé¿å…çˆ¬å–è¿‡å¤šå†…å®¹
                    max_categories = 100  # å¢åŠ é™åˆ¶å€¼ï¼Œç¡®ä¿ä¸ä¼šé—æ¼é‡è¦ç±»åˆ«
                    if len(real_categories) > max_categories:
                        print(f"âš ï¸ å­ç±»åˆ«æ•°é‡è¿‡å¤šï¼Œä»…çˆ¬å–å‰ {max_categories} ä¸ª")
                        real_categories = real_categories[:max_categories]

                    # éå†å¹¶çˆ¬å–å­ç±»åˆ«
                    processed_children = 0
                    total_children = len(real_categories)
                    print(f"   ğŸ”„ å¼€å§‹å¤„ç† {total_children} ä¸ªå­ç±»åˆ«...")

                    for i, category in enumerate(real_categories, 1):
                        # è®°å½•å½“å‰å¤„ç†çš„ç±»åˆ«
                        print(f"\n   ğŸ“‚ [{i}/{total_children}] å¤„ç†å­ç±»åˆ«: {category['name']}")
                        print(f"      URL: {category['url']}")
                        print(f"      è¿›åº¦: {(i/total_children)*100:.1f}%")

                        # è·³è¿‡å·²è®¿é—®çš„é“¾æ¥ï¼Œä½†ä»ç„¶åˆ›å»ºèŠ‚ç‚¹ç»“æ„
                        if category["url"] in self.visited_urls:
                            print(f"   â­ï¸ è·³è¿‡å·²è®¿é—®çš„URL: {category['url']}")
                            # åˆ›å»ºå­èŠ‚ç‚¹ï¼Œå³ä½¿å·²è®¿é—®è¿‡ä¹Ÿä¿æŒç»“æ„å®Œæ•´æ€§
                            clean_name = category["name"]
                            if " Repair" in clean_name:
                                clean_name = clean_name.replace(" Repair", "")

                            child_node = {
                                "name": clean_name,
                                "url": category["url"],
                                "children": [],
                                "instruction_url": ""
                            }
                            parent_node["children"].append(child_node)
                            processed_children += 1
                            continue

                        # æ¸…ç†ç±»åˆ«åç§°ï¼ˆç§»é™¤"Repair"ç­‰åç¼€ï¼‰
                        clean_name = category["name"]
                        if " Repair" in clean_name:
                            clean_name = clean_name.replace(" Repair", "")

                        # æ–­ç‚¹ç»­çˆ¬ï¼šæ£€æŸ¥URLå¤„ç†çŠ¶æ€
                        if self.enable_resume and self.progress_manager:
                            if self.progress_manager.is_url_processed(category["url"]):
                                # URLå·²å¤„ç†ï¼Œä½†éœ€è¦ä»å·²ä¿å­˜çš„æ ‘ä¸­æ¢å¤å­èŠ‚ç‚¹
                                print(f"â­ï¸ URLå·²å¤„ç†ï¼Œä»å·²ä¿å­˜æ ‘ä¸­æ¢å¤å­èŠ‚ç‚¹: {category['url']}")
                                print(f"   ç±»åˆ«åç§°: '{clean_name}'")

                                # åˆ›å»ºå­èŠ‚ç‚¹
                                child_node = {
                                    "name": clean_name,
                                    "url": category["url"],
                                    "children": []
                                }
                                parent_node["children"].append(child_node)

                                # ä»å·²ä¿å­˜çš„æ ‘ä¸­æ¢å¤å¹¶ç»§ç»­éå†å­èŠ‚ç‚¹
                                # ç¡®ä¿å³ä½¿ä»ç¼“å­˜æ¢å¤ï¼Œä¹Ÿè¦å®Œæ•´å¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
                                try:
                                    self._continue_from_saved_tree_node(category["url"], child_node)
                                    print(f"   âœ… æˆåŠŸæ¢å¤å­èŠ‚ç‚¹: '{clean_name}'")
                                except Exception as e:
                                    print(f"   âŒ æ¢å¤å­èŠ‚ç‚¹å¤±è´¥: '{clean_name}' - {str(e)}")
                                    # å¦‚æœæ¢å¤å¤±è´¥ï¼Œå°è¯•é‡æ–°çˆ¬å–
                                    print(f"   ğŸ”„ å°è¯•é‡æ–°çˆ¬å–: {category['url']}")
                                    self._crawl_recursive_tree_with_resume(category["url"], child_node)

                                processed_children += 1
                                continue
                            elif self.progress_manager.is_url_failed(category["url"]):
                                # URLå¤„ç†å¤±è´¥ï¼Œä½†ä»ç„¶åˆ›å»ºèŠ‚ç‚¹ç»“æ„ï¼Œé¿å…é—æ¼
                                print(f"âš ï¸ URLå¤„ç†å¤±è´¥ï¼Œä½†ä»åˆ›å»ºèŠ‚ç‚¹ç»“æ„: {category['url']}")
                                print(f"   ç±»åˆ«åç§°: '{clean_name}'")

                                # åˆ›å»ºå­èŠ‚ç‚¹ï¼Œå³ä½¿å¤„ç†å¤±è´¥ä¹Ÿä¿æŒç»“æ„å®Œæ•´æ€§
                                child_node = {
                                    "name": clean_name,
                                    "url": category["url"],
                                    "children": []
                                }
                                parent_node["children"].append(child_node)
                                processed_children += 1
                                continue

                        # è·³è¿‡ç¼–è¾‘ã€å†å²ã€åˆ›å»ºæŒ‡å—ç­‰éè®¾å¤‡é¡µé¢
                        if any(x in category["url"] or x in category["name"] for x in ["/Edit/", "/History/", "?revision", "/Answers/", "åˆ›å»ºæŒ‡å—", "Guide/new"]):
                            continue

                        # åˆ›å»ºå­èŠ‚ç‚¹
                        child_node = {
                            "name": clean_name,
                            "url": category["url"],
                            "children": [],
                            "instruction_url": ""  # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰instruction_urlå­—æ®µ
                        }
                        parent_node["children"].append(child_node)

                        print(f"ğŸŒ¿ å¼€å§‹é€’å½’çˆ¬å–ç±»åˆ«: {path_str} > {clean_name}")

                        # é€’å½’çˆ¬å–å­ç±»åˆ«ï¼Œç¡®ä¿å®Œæ•´éå†
                        try:
                            self._crawl_recursive_tree_with_resume(category["url"], child_node)
                            print(f"   âœ… å®Œæˆé€’å½’çˆ¬å–: {clean_name}")

                            # éªŒè¯å­èŠ‚ç‚¹æ˜¯å¦æœ‰å†…å®¹
                            child_count = len(child_node.get("children", []))
                            if child_count > 0:
                                print(f"   ğŸ“Š {clean_name} åŒ…å« {child_count} ä¸ªå­èŠ‚ç‚¹")
                            else:
                                print(f"   ğŸ“„ {clean_name} æ˜¯å¶å­èŠ‚ç‚¹")

                        except Exception as e:
                            print(f"   âš ï¸ é€’å½’çˆ¬å–å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹: {clean_name} - {str(e)}")
                            # å³ä½¿é€’å½’å¤±è´¥ï¼Œä¹Ÿä¿æŒèŠ‚ç‚¹ç»“æ„
                            if "instruction_url" not in child_node:
                                child_node["instruction_url"] = ""
                            # ç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹ï¼Œä¸è¦åœæ­¢æ•´ä¸ªæµç¨‹

                        processed_children += 1

                        # æ›´æ–°å·²å¤„ç†çš„å­åˆ†ç±»æ•°é‡
                        if self.enable_resume and self.progress_manager:
                            self.progress_manager.update_children_processed(processed_children)
                elif not is_final_page:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å­ç±»åˆ«ä½†ä¹Ÿä¸ç¬¦åˆæœ€ç»ˆäº§å“é¡µé¢çš„å®šä¹‰
                    # è¿™ç§æƒ…å†µå¯èƒ½æ˜¯é¡µé¢ç»“æ„ç‰¹æ®Šæˆ–è€…ç±»åˆ«æå–å¤±è´¥
                    print(f"âš ï¸ ç‰¹æ®Šæƒ…å†µï¼šé¡µé¢æ—¢ä¸æ˜¯æœ€ç»ˆäº§å“é¡µé¢ï¼Œä¹Ÿæ²¡æœ‰æ‰¾åˆ°å­ç±»åˆ«")
                    print(f"   URL: {url}")
                    print(f"   é¡µé¢æ ‡é¢˜: {soup.title.text if soup.title else 'æ— æ ‡é¢˜'}")

                    # å°è¯•æå–äº§å“ä¿¡æ¯ä½œä¸ºåå¤‡æ–¹æ¡ˆ
                    product_info = self.extract_product_info(soup, url, [])
                    if product_info["product_name"]:
                        parent_node["name"] = product_info["product_name"]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼Œä¸ç»™æ ¹ç›®å½•æ·»åŠ instruction_url
                        is_root_device = url == f"{self.base_url}/Device"
                        if not is_root_device:
                            parent_node["instruction_url"] = product_info["instruction_url"]
                        print(f"   âœ… ä½œä¸ºäº§å“å¤„ç†: {path_str} > {product_info['product_name']}")
                    else:
                        # ç¡®ä¿å³ä½¿æ²¡æœ‰äº§å“ä¿¡æ¯ï¼Œä¹Ÿæœ‰instruction_urlå­—æ®µ
                        if "instruction_url" not in parent_node:
                            parent_node["instruction_url"] = ""
                        print(f"   âš ï¸ æ— æ³•æå–äº§å“ä¿¡æ¯ï¼Œä¿æŒåŸæœ‰èŠ‚ç‚¹ç»“æ„")

            # æ ‡è®°URLå¤„ç†å®Œæˆ
            if self.enable_resume and self.progress_manager:
                children_count = len(real_categories) if 'real_categories' in locals() else 0
                self.progress_manager.mark_url_completed(url, children_count)

            # è¾“å‡ºèŠ‚ç‚¹å¤„ç†å®Œæˆä¿¡æ¯
            node_type = "å¶å­èŠ‚ç‚¹" if is_final_page else f"åˆ†ç±»èŠ‚ç‚¹({len(real_categories) if 'real_categories' in locals() else 0}ä¸ªå­ç±»åˆ«)"
            print(f"   âœ… èŠ‚ç‚¹å¤„ç†å®Œæˆ: {node_type}")
            print(f"   ğŸ“Š å½“å‰èŠ‚ç‚¹ç»Ÿè®¡:")
            print(f"      - èŠ‚ç‚¹åç§°: {parent_node.get('name', 'Unknown')}")
            print(f"      - å­èŠ‚ç‚¹æ•°: {len(parent_node.get('children', []))}")
            print(f"      - æ˜¯å¦æœ‰instruction_url: {'instruction_url' in parent_node}")
            if 'real_categories' in locals() and real_categories:
                print(f"      - å‘ç°çš„å­ç±»åˆ«: {[c['name'] for c in real_categories]}")

        except Exception as e:
            # å¤„ç†é”™è¯¯ï¼Œä½†ä¸è¦åœæ­¢æ•´ä¸ªçˆ¬å–æµç¨‹
            self.logger.warning(f"å¤„ç†URLæ—¶å‡ºé”™ {url}: {e}")
            print(f"   âš ï¸ èŠ‚ç‚¹å¤„ç†å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹: {str(e)[:100]}")
            if self.enable_resume and self.progress_manager:
                self.progress_manager.mark_url_failed(url, str(e))
            # ä¸è¦æŠ›å‡ºå¼‚å¸¸ï¼Œè®©çˆ¬è™«ç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹

    def _get_parent_path_from_tree(self, node):
        """ä»æ ‘èŠ‚ç‚¹è·å–çˆ¶è·¯å¾„"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        return [node.get('name', '')]

    def _get_current_path(self, node, tree=None):
        """è·å–å½“å‰èŠ‚ç‚¹çš„å®Œæ•´è·¯å¾„å­—ç¬¦ä¸²ï¼Œç”¨äºè°ƒè¯•è¾“å‡º"""
        if not tree:
            # å¦‚æœæ²¡æœ‰æä¾›æ ‘ï¼Œä½¿ç”¨ç¼“å­˜çš„æ ‘ç»“æ„
            if hasattr(self, 'tree_cache'):
                tree = self.tree_cache
            else:
                return node.get("name", "")
                
        # åœ¨æ ‘ä¸­æŸ¥æ‰¾ä»æ ¹åˆ°å½“å‰èŠ‚ç‚¹çš„è·¯å¾„
        path = []
        self._find_path_in_tree(tree, node.get("url", ""), path)
        
        # å¦‚æœæ‰¾åˆ°è·¯å¾„ï¼Œè¿”å›å®Œæ•´è·¯å¾„å­—ç¬¦ä¸²
        if path:
            return " > ".join([n.get("name", "") for n in path])
        
        # å¦‚æœæ‰¾ä¸åˆ°è·¯å¾„ï¼Œè¿”å›èŠ‚ç‚¹åç§°
        return node.get("name", "")
        
    def _find_path_in_tree(self, current_node, target_url, path):
        """åœ¨æ ‘ä¸­æŸ¥æ‰¾ä»æ ¹åˆ°ç›®æ ‡URLçš„è·¯å¾„"""
        # æ·»åŠ å½“å‰èŠ‚ç‚¹åˆ°è·¯å¾„
        path.append(current_node)
        
        # å¦‚æœæ˜¯ç›®æ ‡èŠ‚ç‚¹ï¼Œè¿”å›True
        if current_node.get("url", "") == target_url:
            return True
            
        # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
        if "children" in current_node:
            for child in current_node["children"]:
                # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
                if self._find_path_in_tree(child, target_url, path):
                    return True
                    
        # å¦‚æœåœ¨å½“å‰å­æ ‘ä¸­æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡ï¼Œä»è·¯å¾„ä¸­ç§»é™¤å½“å‰èŠ‚ç‚¹
        path.pop()
        return False
            
    def ensure_instruction_url_in_leaf_nodes(self, node):
        """
        ç¡®ä¿æ­£ç¡®çš„å­—æ®µç»“æ„:
        1. æ‰€æœ‰å¶å­èŠ‚ç‚¹(æ²¡æœ‰å­èŠ‚ç‚¹)éƒ½æœ‰instruction_urlå­—æ®µï¼Œä¸”å­—æ®µé¡ºåºä¸ºname-url-instruction_url-children
        2. æ‰€æœ‰åˆ†ç±»èŠ‚ç‚¹(æœ‰å­èŠ‚ç‚¹)éƒ½æ²¡æœ‰instruction_urlå­—æ®µï¼Œå­—æ®µé¡ºåºä¸ºname-url-children
        3. æ ¹ç›®å½•ï¼ˆDeviceé¡µé¢ï¼‰ä¸æ·»åŠ instruction_urlå­—æ®µ
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•ï¼ˆDeviceé¡µé¢ï¼‰
        url = node.get('url', '')
        is_root_device = url == f"{self.base_url}/Device"

        if "children" in node:
            if not node["children"] or len(node["children"]) == 0:
                # è¿™æ˜¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œä½†æ’é™¤æ ¹ç›®å½•
                if not is_root_device:
                    # ç¡®ä¿instruction_urlå­˜åœ¨
                    if "instruction_url" not in node:
                        node["instruction_url"] = ""
                        print(f"åå¤„ç†: æ·»åŠ ç¼ºå¤±çš„instruction_urlå­—æ®µåˆ°å¶å­èŠ‚ç‚¹: {node.get('name', '')}")

                    # é‡æ–°æ’åºå­—æ®µï¼šname-url-instruction_url-children
                    name = node.get("name", "")
                    url = node.get("url", "")
                    instruction_url = node.get("instruction_url", "")
                    children = node.get("children", [])

                    # æ¸…é™¤æ‰€æœ‰é”®
                    node.clear()

                    # æŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ·»åŠ å›å»
                    node["name"] = name
                    node["url"] = url
                    node["instruction_url"] = instruction_url
                    node["children"] = children
                else:
                    # æ ¹ç›®å½•èŠ‚ç‚¹ï¼Œç¡®ä¿æ²¡æœ‰instruction_urlå­—æ®µ
                    if "instruction_url" in node:
                        print(f"åå¤„ç†: åˆ é™¤æ ¹ç›®å½•èŠ‚ç‚¹ä¸­çš„instruction_urlå­—æ®µ: {node.get('name', '')}")
                        del node["instruction_url"]
            else:
                # è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»èŠ‚ç‚¹(æœ‰å­èŠ‚ç‚¹)ï¼Œç¡®ä¿æ²¡æœ‰instruction_urlå­—æ®µ
                if "instruction_url" in node:
                    # å‘ç°åˆ†ç±»èŠ‚ç‚¹æœ‰instruction_urlå­—æ®µï¼Œéœ€è¦åˆ é™¤
                    print(f"åå¤„ç†: åˆ é™¤åˆ†ç±»èŠ‚ç‚¹ä¸­çš„instruction_urlå­—æ®µ: {node.get('name', '')}")
                    
                    # é‡æ–°æ’åºå­—æ®µï¼šname-url-children
                    name = node.get("name", "")
                    url = node.get("url", "")
                    children = node.get("children", [])
                    
                    # æ¸…é™¤æ‰€æœ‰é”®
                    node.clear()
                    
                    # æŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ·»åŠ å›å»(ä¸åŒ…æ‹¬instruction_url)
                    node["name"] = name
                    node["url"] = url
                    node["children"] = children
                
                # é€’å½’å¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
                for child in node["children"]:
                    self.ensure_instruction_url_in_leaf_nodes(child)
    
    def save_tree_result(self, tree_data, filename=None, target_name=None):
        """ä¿å­˜æ ‘å½¢ç»“æ„åˆ°JSONæ–‡ä»¶"""
        # ç¡®ä¿æ‰€æœ‰å¶å­èŠ‚ç‚¹éƒ½æœ‰instruction_urlå­—æ®µ
        self.ensure_instruction_url_in_leaf_nodes(tree_data)
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)
        
        if not filename:
            # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œä¸­æŒ‡å®šçš„çˆ¬å–ç›®æ ‡åç§°
            if target_name:
                # ç§»é™¤æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                safe_name = target_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                filename = f"results/tree_{safe_name}.json"
            else:
                # ä»æ ¹èŠ‚ç‚¹åç§°æ„å»ºæ–‡ä»¶å
                root_name = tree_data.get("name", "tree")
                # å¦‚æœæ˜¯å®Œæ•´è·¯å¾„çš„æœ€ç»ˆèŠ‚ç‚¹åç§°ï¼Œåªå–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ–‡ä»¶å
                if " > " in root_name:
                    root_name = root_name.split(" > ")[-1]
                # ç§»é™¤æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦    
                root_name = root_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                filename = f"results/tree_{root_name}.json"
            
        # å°†æ ‘å½¢ç»“æ„ä¿å­˜ä¸ºJSON
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(tree_data, f, ensure_ascii=False, indent=2)
            
        print(f"\nå·²ä¿å­˜æ ‘å½¢ç»“æ„åˆ° {filename}")
        return filename
        
    def print_tree_structure(self, node, level=0):
        """æ‰“å°æ ‘å½¢ç»“æ„ï¼Œæ–¹ä¾¿åœ¨æ§åˆ¶å°æŸ¥çœ‹"""
        indent = "  " * level
        # æ²¡æœ‰childrenæˆ–childrenä¸ºç©ºçš„èŠ‚ç‚¹è§†ä¸ºäº§å“
        if "children" not in node or not node["children"]:
            print(f"{indent}- {node.get('name', 'Unknown')} [äº§å“]")
        else:
            print(f"{indent}+ {node.get('name', 'Unknown')} [ç±»åˆ«]")
            
        # é€’å½’æ‰“å°å­èŠ‚ç‚¹
        if "children" in node:
            for child in node["children"]:
                self.print_tree_structure(child, level + 1)

def process_input(input_text):
    """å¤„ç†è¾“å…¥ï¼Œæ”¯æŒURLæˆ–è®¾å¤‡å"""
    if not input_text:
        return None, "è®¾å¤‡"
        
    # å¦‚æœæ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
    if input_text.startswith("http"):
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        return input_text, name
    
    # å¦‚æœåŒ…å«/Device/ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†URL
    if "/Device/" in input_text:
        parts = input_text.split("/")
        name = parts[-1].replace("_", " ")
        if not input_text.startswith("https://"):
            return "https://www.ifixit.com" + input_text, name
        return input_text, name
    
    # å¦åˆ™è§†ä¸ºè®¾å¤‡å/äº§å“åï¼Œæ„å»ºURL
    # å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«ç©ºæ ¼ï¼Œå¦‚æœæœ‰ï¼Œæ›¿æ¢ä¸ºä¸‹åˆ’çº¿ç”¨äºURL
    processed_input = input_text.replace(" ", "_")
    return f"https://www.ifixit.com/Device/{processed_input}", input_text

def print_usage():
    print("ä½¿ç”¨æ–¹æ³•:")
    print("python tree_crawler.py [URLæˆ–è®¾å¤‡å]")
    print("ä¾‹å¦‚: python tree_crawler.py https://www.ifixit.com/Device/Television")
    print("      python tree_crawler.py Television")

def main():
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†å‚æ•°
    if len(sys.argv) > 1:
        input_text = sys.argv[1]
        # æ£€æŸ¥æ˜¯å¦æœ‰è°ƒè¯•å‚æ•°
        debug_mode = len(sys.argv) > 2 and sys.argv[2].lower() in ('debug', 'true', '1')
    else:
        print("=" * 60)
        print("iFixitæ ‘å½¢çˆ¬è™«å·¥å…· - ä»¥å±‚çº§æ ‘å½¢ç»“æ„å±•ç¤ºè®¾å¤‡åˆ†ç±»")
        print("=" * 60)
        print("\nè¯·è¾“å…¥è¦çˆ¬å–çš„iFixitäº§å“åç§°æˆ–URL:")
        print("ä¾‹å¦‚: 70UK6570PUB            - æŒ‡å®šäº§å“å‹å·")
        print("      https://www.ifixit.com/Device/70UK6570PUB - æŒ‡å®šäº§å“URL")
        print("      Television             - ç”µè§†ç±»åˆ«")
        print("      Electronics            - ç”µå­äº§å“ç±»åˆ«")
        
        input_text = input("\n> ").strip()
        debug_mode = input("\næ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼? (y/n): ").strip().lower() == 'y'
    
    if not input_text:
        print_usage()
        return
    
    # å¤„ç†è¾“å…¥
    url, name = process_input(input_text)
    
    if url:
        print("\n" + "=" * 60)
        print(f"å¼€å§‹çˆ¬å–ä»¥ {name} ä¸ºç›®æ ‡çš„å®Œæ•´æ ‘å½¢ç»“æ„...")
        print("=" * 60)
        
        # åˆ›å»ºæ ‘å½¢çˆ¬è™«
        crawler = TreeCrawler(verbose=debug_mode)
        # è®¾ç½®è°ƒè¯•æ¨¡å¼
        crawler.debug = debug_mode
        
        # çˆ¬å–æ ‘å½¢ç»“æ„
        print("\n1. æŸ¥æ‰¾ä»æ ¹ç›®å½•åˆ°ç›®æ ‡äº§å“çš„ç²¾ç¡®è·¯å¾„...")
        tree_data = crawler.crawl_tree(url, name)
        
        # ç¼“å­˜æ ‘ç»“æ„ç”¨äºè·¯å¾„æŸ¥æ‰¾
        crawler.tree_cache = tree_data
        
        # ä¿å­˜ç»“æœ
        if tree_data:
            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            filename = crawler.save_tree_result(tree_data, target_name=input_text)

            # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
            product_count = count_products_in_tree(tree_data)
            category_count = count_categories_in_tree(tree_data)

            print("ğŸ‰ çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š åˆ†ç±»: {category_count} ä¸ª | äº§å“: {product_count} ä¸ª")
            print(f"ğŸ“ å·²ä¿å­˜åˆ°: {filename}")

            # åœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if hasattr(crawler, 'verbose') and crawler.verbose:
                print("\næ ‘å½¢ç»“æ„æ‘˜è¦ (+ è¡¨ç¤ºç±»åˆ«ï¼Œ- è¡¨ç¤ºäº§å“):")
                print("=" * 60)
                crawler.print_tree_structure(tree_data)
                print("=" * 60)
                print("\nç¤ºä¾‹è·¯å¾„å±•ç¤º:")
            
            # æ˜¾ç¤ºç¤ºä¾‹è·¯å¾„
            if product_count > 0:
                # æ‰¾åˆ°ä¸€ä¸ªäº§å“èŠ‚ç‚¹è¿›è¡Œæ¼”ç¤º
                sample_product = find_sample_product(tree_data)
                if sample_product:
                    product_path = crawler._get_current_path(sample_product, tree_data)
                    print(f"äº§å“è·¯å¾„: {product_path}")
            
            print("=" * 60)
        else:
            print("\nçˆ¬å–å¤±è´¥ï¼Œæœªè·å–åˆ°æ ‘å½¢ç»“æ„æ•°æ®")
    else:
        print_usage()

def find_sample_product(tree):
    """åœ¨æ ‘ä¸­æŸ¥æ‰¾ä¸€ä¸ªç¤ºä¾‹äº§å“èŠ‚ç‚¹"""
    # æ²¡æœ‰å­èŠ‚ç‚¹çš„è§†ä¸ºäº§å“èŠ‚ç‚¹
    if "children" not in tree or not tree["children"]:
        return tree
        
    if "children" in tree:
        for child in tree["children"]:
            result = find_sample_product(child)
            if result:
                return result
                
    return None
        
def count_products_in_tree(node):
    """è®¡ç®—æ ‘ä¸­äº§å“èŠ‚ç‚¹çš„æ•°é‡"""
    count = 0
    # æ²¡æœ‰å­èŠ‚ç‚¹çš„è§†ä¸ºäº§å“èŠ‚ç‚¹
    if "children" not in node or not node["children"]:
        count = 1
    elif "children" in node:
        for child in node["children"]:
            count += count_products_in_tree(child)
    return count
    
def count_categories_in_tree(node):
    """è®¡ç®—æ ‘ä¸­åˆ†ç±»èŠ‚ç‚¹çš„æ•°é‡"""
    # æœ‰å­èŠ‚ç‚¹çš„è§†ä¸ºåˆ†ç±»èŠ‚ç‚¹
    if "children" not in node or not node["children"]:
        return 0
        
    count = 1  # å½“å‰èŠ‚ç‚¹æ˜¯ä¸€ä¸ªåˆ†ç±»
    if "children" in node:
        for child in node["children"]:
            count += count_categories_in_tree(child)
    return count

if __name__ == "__main__":
    main()
